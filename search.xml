<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>3D结构重建</title>
      <link href="/3d/3dreconstruction/"/>
      <url>/3d/3dreconstruction/</url>
      
        <content type="html"><![CDATA[<h1 id="3D重建"><a href="#3D重建" class="headerlink" title="3D重建"></a>3D重建</h1><p>介绍3D重建的各种模型框架方法等。</p><p>3D数据常用的表示方法：</p><ul><li>Rasterized forms例如voxels and multi-view RGB(D) images.</li><li>Geometric forms例如point clouds, polygon meshes, and sets of primitives</li><li>其他：geometry images、depth images、classification boundaries、signed distance function</li></ul><h2 id="3D-R2N2"><a href="#3D-R2N2" class="headerlink" title="3D-R2N2"></a>3D-R2N2</h2><blockquote><p>ECCV2016《3D-R2N2: A Unified Approach for Single and Multi-view 3D Object Reconstruction》</p><p>paper下载：<a href="https://arxiv.org/abs/1604.00449" target="_blank" rel="noopener">https://arxiv.org/abs/1604.00449</a></p></blockquote><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>提出3D-R2N2网络，能够输入一张/多张图片并重建出3D occupancy grid，该方法在训练/测试阶段不需要任何的图像标注和物体类别标签（但需要bounding box）。在某些情形中使用传统SFM/SLAM方法失效而本方法却能够重建出来（因为这些情形缺少纹理，或者摄像机的拍摄视角变化大）。3D-R2N2是自动学习的、end-to-end的。性能超越了SOTA。</p><p>创新点：使用RNN来实现允许单图片/多图片的输入</p><p>注：</p><ul><li>3D-R2N2 == 3D Recurrent Reconstruction Neural Network</li><li><p>先前方法的要求条件（因为SFM/SLAM这类方法假设了features can be matched across views）：</p><ul><li><p>观察的视角密集，即相机的位置变化比较小</p></li><li><p>表面是Lambertian reflectance，即均匀的漫反射</p></li><li><p>表面的纹理丰富，不单一</p></li></ul></li><li>所使用的数据集：ShapeNet、PASCAL 3D、Online Products、MVS CAD Models</li><li>所对比的模型：Kar et al.（《Category-Specific Object Reconstruction from a Single Image》）、Multi View Stereo method（MVS）</li></ul><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p><img src="1_1.png" alt></p><p>利用RNN的“可接受任意长的序列输入”的性质，来达到单视图/多视图输入的统一。</p><p>首先将图片利用2D conv编码成1024维的特征向量。然后丢到3D Convolutional LSTM中，输出是$4\times 4\times 4$大小的voxels（4D tensor），每个voxel位置上都是一个vector（其实是排列成$4\times 4\times 4$的64个LSTM单元，然后取hidden state作为每个voxel的特征向量）。然后将4D tensor丢到3D conv中上采样，得到$32\times 32\times 32$的3D occupancy grid（每个voxel位置上是这个voxel取到的概率，所以需要一个threshold来确定最终的输出）。</p><p>随着输入图片变多，得到的结果也逐渐变得精细。</p><p>视觉上解释LSTM带来的优点：LSTM的遗忘门/输入门机制对应于自动纠错错误重建的部分/自动重建未看见的部分（有效的处理物体遮挡问题）</p><h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><p>分为三部分：2D Convolutional Neural Network (2D-CNN), a novel architecture named 3D Convolutional LSTM (3D-LSTM), and a 3D Deconvolutional Neural Network (3D-DCNN)</p><ol><li><p>Encoder: 2D-CNN</p><p>可以使用普通的浅CNN（上面那个），或者是使用残差结构的深CNN（下面那个）。实验表明残差结构更好。</p><p><img src="1_2.png" alt></p></li><li><p>Recurrence: 3D Convolutional LSTM</p><p><img src="1_3.png" alt></p><p>在实施例中，有64个不同的LSTM单元，排列成$4\times 4\times 4$的形状。经过某种计算之后，我们取每个LSTM单元的$N_h$维的hidden state作为输出，于是得到$4\times 4\times 4 \times N_h$的4D tensor输出。计算过程如下：</p><p><img src="1_4.png" alt></p><p>其中$f_t$是遗忘门，$i_t$是输入门，$s_t$是memory cell，$h_t$是hidden state。</p><p>先考虑在一个位置上的计算：$U*h_{t-1}$表示周围邻居（红色）的$t-1$时刻的hidden state的线性加权和$\sum_{k \in \mathcal{N}_{index}} u_k h_{t-1}[k]$，$\mathcal{T(x_t)}$表示$t$时刻的1024维向量输入。那么输出$h_t$就是一个向量。</p><p>如果考虑全部64个LSTM单元，那么输出$h_t$就是一个4D tensor。</p><p>作者在这里没有使用outputs gate，节省了参数量。这里的kernel size是3。</p><p>当然也有基于GRU（Gated Recurrent Unit）的实现方法，实验表明GRU效果好于LSTM，kernel size为3时好于为1时。</p></li><li><p>Decoder: 3D Deconvolutional Neural Network</p><p>可以用普通的3D卷积，也可以用残差结构的3D卷积。</p><p>最后将$32\times 32\times\times 32 \times 2$的输出套用softmax，得到voxel-wise的概率值（尺寸$32\times 32\times \times 32$）。然后用threshold截断概率便得到最后的结果。</p><p>损失函数是the sum of voxel-wise cross-entropy</p></li><li><p>训练和效果</p><ul><li>Data augmentation： augmented the input images with random crops、tinted the color、randomly translated the images、all viewpoints were sampled randomly</li><li>Training： variable length inputs ranging from one image to an arbitrary number of images</li><li>Metrics： voxel Intersection-over-Union (IoU)、cross-entropy loss</li></ul></li></ol><h2 id="PointOutNet"><a href="#PointOutNet" class="headerlink" title="PointOutNet"></a>PointOutNet</h2><blockquote><p>CVPR2017《A Point Set Generation Network for 3D Object Reconstruction from a Single Image》</p><p>paper下载：<a href="http://openaccess.thecvf.com/content_cvpr_2017/html/Fan_A_Point_Set_CVPR_2017_paper.html" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2017/html/Fan_A_Point_Set_CVPR_2017_paper.html</a></p></blockquote><h3 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h3><p>作者是第一个用深度学习重建点云的人。输入单张图片，输出点云的3D坐标。他们构造的模型是“conditional shape sampler”（即有个输入变量$r$可以控制生成的多样性，用于描述重建的不确定性），可以输出多种可能的点云（取决于$r$）。单图输入的情况下性能超越SOTA（与3D-R2N2对比）。也可以用于3D补全，以及多可能的输出。</p><p>似乎没深究如何生成点云以达到无序性输出，文中好像是在fc/conv这样的结构化排列中输出的？</p><p>难点：</p><ul><li>如何输出点云</li><li>仅靠输入的单张图片难以直接确定3D形状（即inherent ambiguity in groundtruth）</li></ul><p>特点：</p><ul><li>引入随机变量$r$可以从输入操控生成点云的多样性</li></ul><p>作者的Discussion：</p><ul><li>First, how to generate an orderless set of entities. Towards building generative models for more sophisticated combinatorial data structures such as graphs, knowing how to generate a set may be a good starting point.</li><li>Second, how to capture the ambiguity of the groundtruth in a regression problem. Other than 3D reconstruction, many regression problems may have such inherent ambiguity. Our construction of the MoN loss by wrapping existing loss functions may be generalizable to these problems.</li></ul><p>TODO IDEA：能否不使用“Distance Metric between Point Sets”，转而使用能直接处理点云的网络？能否定向操控$r$各个分量所对应的3D属性？</p><h3 id="实现-1"><a href="#实现-1" class="headerlink" title="实现"></a>实现</h3><p>主要由三部分组成：architecture, loss function and learning paradigm</p><p>注：architecture的作用是根据图片和随机向量$r$来生成点云；loss function的作用是度量点云相似度；learning paradigm作用是使得随机变量$r$能在网络中起作用，相当于在loss function外部包上了一层“MoN函数”。</p><ol><li><p>Architecture - Point Set Prediction Network（此网络简记作$\mathbb{G}$）</p><p><img src="2_1.png" alt></p><p>作者设计了3种网络的结构：vanilla version、two prediction branch version和hourglass version</p><p>每种网络都有encoder stage和predictor stage。encoder stage用于将图片$I$和输入的随机向量$r$映射到embedding space；而predictor则输出一个$N \times 3$大小的矩阵$M$（$N$是重建出的点云的点数）。A random vector $r$ is subsumed so that it perturbs the prediction from the image $I$（随机向量$r$的用法在“Generation of Multiple Plausible Shapes”会讲到）</p><ul><li>vanilla version：最原始的版本，细节见图（r.v.就是随机向量$r$）。Though simple, this version works reasonably well in practice.</li><li>two prediction branch version：目的是better accommodate large and smooth surfaces which are common in natural objects。它的predictor有两个并行的分支：a fully-connected (fc) branch and a deconvolution (deconv) branch。fc branch预测$N_1=256$个点（fc输出256*3个nodes），deconv branch预测输出$H \times W=24\times 32$个点（排列成$24\times 32$然后一个像素对应一个3D的点，是3通道的）。那么总共输出$N = 24 \times 32 + 256=1024$个点（不知道如何处理无序性呢？？）。Their predictions are later merged together to form the whole set of points in $M$. Multiple skip links are added to boost information flow across encoder and predictor。fc branch功能：high flexibility、可describe intricate structures。deconvolution branch功能：节省参数、more friendly to large smooth surfaces, due to the spatial continuity induced by deconv and conv。</li><li>hourglass version：This deep network conducts the encoding-decoding operations recurrently, thus has stronger representation power and can mix global and local information better.</li></ul></li><li><p>Loss function - Distance Metric between Point Sets</p><p>对Loss的要求：可微、高效计算、对异常点鲁棒</p><p>那么需要找到一种衡量两个点云$S_i^{pred}$和$S_i^{gt}$相似度的距离函数$d(S_i^{pred}，S_i^{gt})$，那么损失函数就是$L = \sum_i d(S_i^{pred}，S_i^{gt})$（其中$i$是training samples的index。这个损失函数还不是最终的损失函数，不能实际应用）</p><p>作者建议使用Chamfer distance（CD）或Earth Mover’s distance（EMD）</p><p><img src="2_2.png" alt></p><p>Facing the inherent inability to resolve the shape precisely, neural networks tend to predict a “mean” shape averaging out the space of uncertainty. The mean shape carries the characteristics of the distance itself.</p></li><li><p>Learning paradigm - Generation of Multiple Plausible Shapes</p><p>The ambiguity of groundtruth shape may significantly affect the trained predictor, as the loss function induces our model to predict the mean of possible shapes. So We expect that the random<br>variable $r$ passed to “Point Set Prediction Network” would help it explore the groundtruth distribution. 如果跳过这步不做的话，就会导致the loss minimization will nullify the randomness.</p><p>我们可以使用最小化下面的损失函数（称作Min-of-N loss （MoN））的方法来解决：</p><p><img src="2_3.png" alt></p><p>意思是尝试$n $个random variable的取值，然后取最小距离作为距离值。（$n=2$足矣）</p><p>另一种方法是使用Conditional VAE（这样好像$\mathbb{G}$里就不用$r$了），详情略。</p><p>似乎也能用Conditional GAN，但是作者没深究。</p></li></ol><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>用ShapeNet dataset，从CAD模型中渲染出图片来</p><ol><li><p>3D Shape Reconstruction from RGB Images</p><p>与3D-R2N2作比较。将3D-R2N2输出的体数据用最远点采样获得点数据，然后计算CD、EMD值；将本模型输出的点数据经过后期处理得到体数据，然后计算IoU值。</p><p>实验表明本方法在单张图片输入时，在所有种类上精度远超3D-R2N2。同时大部分种类的精度远超5张图片输入的3D-R2N2。而且本方法不存在“薄弱细小结构”不能重建的缺点。</p></li><li><p>3D Shape Completion from RGBD Images</p><p>当是RGBD输入时，还可以起到3D补全的作用。</p><p><img src="2_4.png" alt></p></li><li><p>Predicting Multiple Plausible Shapes</p><p>改变$r$值可以产生多种输出。</p><p><img src="2_5.png" alt></p></li><li><p>Network Design Analysis</p><ul><li><p>Effect of combining deconv and fc branches for reconstruction</p><ul><li><p>deconv的引入提升了精度；Stacking another hourglass level也提升了精度</p></li><li><p>In the deconv branch the network learns to use the convolution structure to constructs a 2D surface that warps around the object. In the fully connected branch the output is less organized as the channels are not ordered.</p></li><li><p>The deconv branch is in general good at capturing the “main body” of the object, while the fully connected branch complements the shape with more detailed components</p><p><img src="2_6.png" alt></p></li></ul></li><li><p>Analysis of distance metrics</p><p>The network trained by CD tends to scatter a few points in its uncertain area (e.g. behind the door) but is able to better preserve the detailed shape of the grip. In contrast, the network trained by EMD produces more compact results but sometimes overly shrinks local structures. This is in line with experiment on synthetic data.</p></li></ul></li><li><p>More results and application to real world data</p><p>也能用于现实生活中照片的3D重建，但需要从背景分割出物体来</p></li><li><p>Analysis of human ability for single view 3D reconstruction</p><p>某些图片上超越人类</p></li><li><p>Analysis of failure cases</p><p>仅仅是主观分析，略</p></li></ol><h2 id="OGN-👍"><a href="#OGN-👍" class="headerlink" title="OGN 👍"></a>OGN 👍</h2><blockquote><p>ICCV2017《Octree Generating Networks: Efficient Convolutional Architectures for High-resolution 3D Outputs》</p><p>paper下载：<a href="http://openaccess.thecvf.com/content_iccv_2017/html/Tatarchenko_Octree_Generating_Networks_ICCV_2017_paper.html" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_iccv_2017/html/Tatarchenko_Octree_Generating_Networks_ICCV_2017_paper.html</a></p></blockquote><h3 id="概述-2"><a href="#概述-2" class="headerlink" title="概述"></a>概述</h3><p>使用八叉树（octree）的表示法，生成高分辨率的3D体素数据。本文中的OGN是deep convolutional decoder（即3D体素生成器）。近似平方的复杂度（而不是立方复杂度），大大节省了内存和计算量消耗。低分辨率时与普通的voxel方法精度差不多，而且它还能生成高分辨率体素，甚至可以生成$512^3$这么大的分辨率。可以用在多种场合，如3D convolutional autoencoders、reconstruction from high-level representations or a single image</p><p><img src="3_3.png" alt></p><p>注意：</p><ul><li>使用octree意味着可以使用不同大小的cell size，不断细分大的cell来达到高分辨率。即不断精细化体素来达到高分辨率</li><li>OGN是在八叉树上操作。用binary occupancy maps来表示生成的形状</li><li>同时OGN也是灵活的，即可以任意指定层数和层的配置</li><li>OGN是end-to-end的，可用反向传播计算</li></ul><h3 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h3><p><img src="3_1.png" alt></p><ul><li><p>最左边的“dense block”由普通的3D卷积层组成，输出$d_1 \times d_2 \times d_3 \times c$大小的4D tensor。</p></li><li><p>然后这个4D tensor被转换为键值对（index-value pairs）存储在hash table中，value就是一个voxel内的特征向量。</p></li><li><p>然后接下来的octree block就会预测octree新生成的结构、和相应生成的内容（特征向量）。</p></li><li><p>多次通过octree block处理就会生成高精度的体素表示。</p></li></ul><h3 id="实现-2"><a href="#实现-2" class="headerlink" title="实现"></a>实现</h3><ol><li><p>Octree编码法</p><p>作用：将体素空间用octree表示，那么对voxel grid的操作就可以转换为对octree的操作</p><p>基于哈希表的Octree编码（可以实现常数时间的元素获取）：</p><ul><li><p>Octree cell的空间坐标$\mathtt{x}=(x,y,z)$，其在树中的层级（分辨率等级）为$l$，其内容为$v$。</p></li><li><p>将其转换为索引$m=\mathcal{Z}(\mathtt{x},l)$，其中$\mathcal{Z}(\cdot)$是Z-order curve。</p></li><li><p>那么就形成了键值对$(m,v)$。于是八叉树就是$O=\{(m,v)\}$</p></li></ul><p>那么数据操作和更新都在hash table中进行。</p><p>查值函数（从八叉树$O$中获取值）：</p><p><img src="3_2.png" alt></p></li><li><p>Octree Generating Networks</p><p>binary occupancy values $v=\{0,1\}$</p><p>由普通3D卷积生成$d_1 \times d_2 \times d_3 \times c$大小的4D tensor，然后被转换成octree，并将数据存储在hash table中。</p><p><img src="3_4.png" alt></p><p>图中为了示意的简便起见，用2D voxel替代3D voxel来展示。</p><p>propagated features是指需要处理或细分的网格特征</p><p>empty是指不是实体的网格</p><p>filled是指被占据、是实体的网格</p><p>mixed是指需要细分的网格（与GT相比既有empty又有filled）</p><ul><li><p>OGN-Conv</p><p>OGN-Conv需要处理由哈希表表示的特征图。OGN-Conv支持步长卷积和上采样卷积。</p><p>基本原理（类似于“im2col”和“col2im”函数）：将hash table转变为feature matrix，然后与权重矩阵相乘，再复原回hash table</p></li><li><p>OGN-Loss</p><p>predictions就是判断voxel是empty/filled/mixed中的哪种，是3分类问题。用$1^3$卷积和softmax实现即可。</p><p>最小化predictions与the cell state of the ground truth的交叉熵：</p><p><img src="3_5.png" alt></p><p>$p_m^i$是输出概率值，$M_l$是第$l$层的叶子集合。那么总目标函数就是全部层的$\mathcal{L_l}$的求和。</p></li><li><p>OGN-Prop</p><p>将预测出来是“mixed”的取出来作为输出。可能还要取出一些其他的邻居，以便用于接下来的卷积计算。</p><p>根据在测试阶段是否知道Octree的GT，传播方式分为：Prop-Known方法（例如语义分割，结构不变，只需要与GT做对比即可）和Prop-Pred方法（例如三维重建，结构需要你预测，需要训练分类模型来判断每个voxel是empty/filled/mixed中的哪种）。不详述。</p></li></ul></li></ol><h2 id="MarrNet"><a href="#MarrNet" class="headerlink" title="MarrNet"></a>MarrNet</h2><blockquote><p>NIPS2017《MarrNet: 3D Shape Reconstruction via 2.5D Sketches》</p><p>paper下载：<a href="http://papers.nips.cc/paper/6657-marrnet-3d-shape-reconstruction-via-25d-sketches" target="_blank" rel="noopener">http://papers.nips.cc/paper/6657-marrnet-3d-shape-reconstruction-via-25d-sketches</a></p></blockquote><h3 id="概述-3"><a href="#概述-3" class="headerlink" title="概述"></a>概述</h3><p>从单张图片重构出3D voxel-based reconstruction。从RGB图得到2.5D表示，再转换成3D形状（two-step）。MarrNet可以在synthetic data数据上训练然后在real data上进行self-supervised的fine-tune（一定程度上解决了domain adaptation问题）。MarrNet是end-to-end的可训练模型，达到了SOTA。</p><p>2D → 2.5D → 3D的优点：</p><ul><li>First, compared to full 3D shape, 2.5D sketches are much easier to be recovered from a 2D image; models that recover 2.5D sketches are also more likely to transfer from synthetic to real data.</li><li>Second, for 3D reconstruction from 2.5D sketches, systems can learn purely from synthetic data.</li><li>Third, we derive differentiable projective functions from 3D shape to 2.5D sketches; the framework is therefore end-to-end trainable on real images, requiring no human annotations</li></ul><p>注：</p><ul><li>取名MarrNet是因为与David Marr’s theory of perception相近</li><li>intrinsic images（描述物体内在固有属性的可分离的本质图像）：例如depth、surface normals、silhouette等等</li><li>Reprojection Consistency只在fine-tune中使用；预训练使用与GT voxel gird比较的交叉熵。</li><li>本文定性分析（图片展示）比较多，定量分析比较少（只有人类肉眼比较结果和一个IoU结论）</li><li>我感觉本文的ablation study说服力不足，即不能说明fine-tune是有效的，并不能说明Reprojection Consistency的设计是起到实际作用的。详见实验的第二部分。</li></ul><h3 id="原理-2"><a href="#原理-2" class="headerlink" title="原理"></a>原理</h3><p><img src="4_1.png" alt></p><p>用2D CNN的方法生成2.5D图，然后将2.5D图用encoder-decoder方法生成体素表示。用reprojection consistency loss确保3D与2.5D匹配。</p><h3 id="实现-3"><a href="#实现-3" class="headerlink" title="实现"></a>实现</h3><ol><li><p>2.5D Sketch Estimation</p><p>用encoder-decoder结构生成2.5D表示。用ResNet-18编码，将256x256的RGB转变为8x8x256的特征图。然后用decoder恢复成256x256的depth, surface normal, and silhouette images</p></li><li><p>3D Shape Estimation</p><p>用encoder-decoder结构生成3D体素。输入depth和normal图（施加以silhouette images掩膜），然后用卷积转变为200维向量，然后用3D卷积生成128x128x128的体素表示。</p></li><li><p>Reprojection Consistency</p><p>使得3D表示与2.5D表示相一致，即将2.5D投射到3D空间中，体素也需要具有与2.5D的描述相一致的性质。reprojection consistency loss分为depth reprojection loss和surface normal reprojection loss。并且reprojection consistency loss对voxel可求导，使得可以用反向传播计算和优化。</p><p>记3D voxel grid在$(x,y,z)$处存在该voxel的概率为$v_{x,y,z}\in[0,1]$，深度图在$(x,y)$处的值为$d_{x,y}$，法向图在$(x,y)$处的法向为$n_{x,y}=(n_a,n_b,n_c)$。并且假设是<strong>正交投影</strong>（注意这里不是相机成像的投影模型）</p><p><img src="4_3.png" alt></p><ul><li><p>depth reprojection loss的目标是：将depth image的point投影到3D中，使得这个3D point恰好出现在3D voxel reconstruction中，并且该点的视线前方无遮挡。</p><p><img src="4_2.png" alt></p></li><li><p>surface normal reprojection的目标是：使得该点在3D切平面上的邻居存在</p><p>已知法向$n_{x,y}=(n_a,b_b,n_c)$必定与$n_x’=(0,-1,n_b/n_c)$、$n_y’=(-1,0,n_a/n_c)$垂直。那么voxel在$(x,y,z)\pm n_x’$和$(x,y,z)\pm n_y’$的值必定为1（当这些点在silhouette image以内时才算）。</p><p><img src="4_4.png" alt></p></li></ul></li><li><p>Training paradigm</p><p>将“2.5D sketch estimation”和“3D shape estimation”分别在synthetic images上预训练：The 3D interpreter is trained using ground truth voxels and a <strong>cross-entropy loss</strong>（注意预训练是用与GT比较的交叉熵！）</p><p>然后再在真实图片上fine-tune：The <strong>reprojection consistency loss</strong> is used to fine-tune the 3D estimation component of our model on real images, using the predicted normal, depth, and silhouette。固定decoder of the 3D estimator（因为它包含了3D形状的先验，不希望改变），只fine-tune the encoder（学习映射到decoder合理的特征空间中）。这种fine-tune类似自监督，可以在无任何标注的单张测试图片上进行fine-tune。</p></li></ol><h3 id="实验-1"><a href="#实验-1" class="headerlink" title="实验"></a>实验</h3><ol><li><p>3D Reconstruction on ShapeNet</p><p>生成的形状具有smoother surfaces and finer details</p><p>Our full model achieves a higher IoU (0.57) than the direct prediction baseline (0.52).</p><p>direct prediction baseline：不经过2.5D表示，直接基于3D卷积的体素生成</p></li><li><p>3D Reconstruction on Pascal 3D+</p><p>现在ShapeNet上预训练，然后fine-tune them on the PASCAL 3D+ dataset</p><p><img src="4_5.png" alt></p><p>上图的ablation study表明，需要固定decoder of the 3D estimator进行fine-tune才能取得更好的fine-tune效果。</p><p>但感觉本文的ablation study说服力不足，文中说The model trained on synthetic data provides a reasonable shape estimate和Our final model, fine-tuned with the decoder fixed, keeps the shape prior and provides more details of the shape，这只是作者自己的观点，并没有数据支撑，并且“human studies”中并没有fine-tune前后的“preferences”对比，并不能说明fine-tune是有效的，即并不能说明Reprojection Consistency的设计是有作用的。</p><p>作者认为IoU指标不好，不能描述精细结构等，提出“human studies”，即人类肉眼比较MarrNet和竞争对手DRC哪个效果比较好。效果当然是说MarrNet比较好啦。</p><p><img src="4_6.png" alt></p><p>MarrNet不能复原复杂的、细弱的结构，而且silhouette mask不能精确估计。</p></li><li><p>3D Reconstruction on IKEA</p><p>IKEA furniture数据集，our model can deal with mild occlusions in real life scenarios</p></li><li><p>Extensions</p><p>We further train MarrNet jointly on all three object categories, and our model successfully recovers shapes of different categories</p></li></ol><h2 id="LSM"><a href="#LSM" class="headerlink" title="LSM"></a>LSM</h2><blockquote><p>NIPS2017《Learning a Multi-View Stereo Machine》</p><p>paper下载：<a href="http://papers.nips.cc/paper/6640-learning-a-multi-view-stereo-machine" target="_blank" rel="noopener">http://papers.nips.cc/paper/6640-learning-a-multi-view-stereo-machine</a></p></blockquote><h3 id="概述-4"><a href="#概述-4" class="headerlink" title="概述"></a>概述</h3><p>利用n幅图像（n≥1）以及这些摄像机的内外参（camera poses），求出物体的体素重建（Voxel LSM）或相应的n幅深度图（Depth LSM）。LSM端到端可微分。核心思想：利用投射关系将2D排列的特征与3D排列的特征相互转换，且能此过程可微分，使其利用了空间投影关系的几何先验。</p><p>注：</p><ul><li>多视角立体视觉（Multi-view stereopsis，简称MVS）：给定物体的多幅图像、这些图像所分别拍摄的摄像机的姿态（内参和外参），试求物体的几何表示（3D结构/深度图等）</li><li>LSM == Learnt Stereo Machine</li><li>our system is able to better use camera pose information leading to significantly large improvements while adding more views（然而对比的网路（3D-R2N2）是add pose information in a fully connected manner，并没有专门对此设计过）</li><li>可能作者调参和优化了很久。。。</li></ul><p>TODO IDEA：将几何投影等关系编码且使得可微分，使得可以反向传播。</p><h3 id="实现-4"><a href="#实现-4" class="headerlink" title="实现"></a>实现</h3><p><img src="5_1.png" alt></p><p>简述：输入n幅图像，然后经过2D CNN特征提取出2D的特征图。再分别使用“unprojection”操作得到3D排列的体特征$\mathcal{G}^f_i$。然后使用RNN将其全部整合成一个体特征$\mathcal{G}^p$。再将其用3D卷积得到$\mathcal{G}^o$。如果想得到体素表示，则直接使用3D卷积生成即可；如果想得到相应的深度图，则使用“projection”操作转换为2D特征图之后再使用2D卷积上采样即可。</p><ol><li><p>2D Image Encoder</p><p>用UNet生成特征图。从图像$\{I_i\}^n_{i=1}$转变为特征图$\{\mathcal{F}_i\}^n_{i=1}$</p></li><li><p>Differentiable Unprojection</p><p>输入特征图和相机姿态，输出3D排列的特征。</p><p>即从特征图$\{\mathcal{F}_i\}^n_{i=1}$转变为3D gird$\{\mathcal{G}_i^f\}^n_{i=1}$</p><p><img src="5_2.png" alt></p><p>上图只是方便图示而已，实际上是3D gird和2D feature map。</p><p>实际实现过程：将3D gird center投影到image plane得到连续的坐标，然后根据discrete grid的feature value用bilinear sampling插值得到该3D gird的feature</p><p>为了可以支持单张图片输入，我们还往每个3D gird中添加几何特征（例如depth value和ray direction）</p><p>这个过程是可微分的，使得可以端到端训练。</p></li><li><p>Recurrent Grid Fusion</p><p>从多个3D gird$\{\mathcal{G}_i^f\}^n_{i=1}$到单个$\mathcal{G^p} $</p><p>使用3D convolutional variant of the Gated Recurrent Unit (GRU)，类似于3D-R2N2中的做法。</p><p>训练时随机打乱输入的顺序，减小输入顺序带来的影响。</p></li><li><p>3D Grid Reasoning</p><p>将$\mathcal{G^p} $转变为$\mathcal{G^o} $</p><p>使用3D UNet。目的（感觉乱说。。）：use shape cues present in $\mathcal{G^p} $ such as feature matches and silhouettes as well as build in shape priors like smoothness and symmetries and knowledge about object classes enabling it to produce complete shapes even when only partial information is visible</p></li><li><p>Differentiable Projection</p><p>从$\mathcal{G^o}$转变为多个2D feature map$\{\mathcal{O}_i\}^n_{i=1}$</p><p>输入：$\mathcal{G^o}$和相机姿态。输出：2D feature map$\mathcal{O}$</p><p><img src="5_3.png" alt></p><p>上图只是方便图示而已，实际上是3D gird和2D feature map。</p><p>实际实现过程：有多个平行于相机平面的等间距平面（图中的z=1,2,3平面），视线ray是穿过2D feature map的离散格点中心的。由此横穿经过的3D gird的特征值就作为特征值。在3D gird中是nearest neighbor interpolation而不是trilinear interpolation。详情见上图。</p><p>最后使用1x1的卷积可以减少channels的数量。</p></li><li><p>Architecture Details and Experiments</p><p>卷积均使用instance normalization和layer normalization</p><ul><li><p>Voxel LSM (VLSM)：最后使用3D卷积上采样。softmax + binary cross entropy loss</p></li><li><p>Depth LSM (D-LSM)：使用Projection将$\mathcal{G^o}$转变为多个2D feature map$\{\mathcal{O}_i\}^n_{i=1}$，然后分别进行1x1卷积和deconvolution上采样，其中deconvolution有skip connections来自之前的image encoder。</p></li></ul><p>Experiments性能：</p><p><img src="5_4.png" alt></p></li></ol><h2 id="IM-NET-👍"><a href="#IM-NET-👍" class="headerlink" title="IM-NET 👍"></a>IM-NET 👍</h2><blockquote><p>CVPR2019《Learning Implicit Fields for Generative Shape Modeling》</p><p>paper下载：<a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Learning_Implicit_Fields_for_Generative_Shape_Modeling_CVPR_2019_paper.html" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Learning_Implicit_Fields_for_Generative_Shape_Modeling_CVPR_2019_paper.html</a></p></blockquote><h3 id="概述-5"><a href="#概述-5" class="headerlink" title="概述"></a>概述</h3><p>使用隐式场的方法来生成3D点云，后处理后的形成的mesh具有superior visual quality（higher surface quality）。并在插值时表现出良好的过度性质。所使用的implicit field decoder称作IM-NET，可以用于3D形状自编码（即shape representation learning）（IM-AE）、形状生成（IM-GAN）、形状插值、单视图3D重建等。其中encoder不属于本文设计范畴，只要能得到shape feature的都可以。</p><p>注：</p><ul><li>Our work is the first to introduce a deep network for learning implicit fields for generative shape modeling</li><li>缺点：训练时间长（因为the decoder needs to be applied on each point in the training set），或许可以考虑只生成物体表面的点来提高速度</li></ul><p>TODO IDEA：后期可以考虑用decoder来生成其他的属性，例如颜色、纹理等。或许还可以用来做part segmentation</p><h3 id="原理-3"><a href="#原理-3" class="headerlink" title="原理"></a>原理</h3><p>decoder的关键是怎么根据shape feature得到出3D数据（点云）。这通过判断给定点$p$是处于shape的内部还是外部来实现的，那么中间的分界等值面就是shape的边界。</p><p><img src="6_1.png" alt></p><p>构造一个MLP，输入是shape feature和point coordinate，输出是判断内外部的二分类（sigmoid函数）。</p><p><img src="6_2.png" alt></p><p>其中输入的point coordinate对空间的均匀采样得到的，因而输出形状的分辨率可以任意高。</p><p>The skip connections（copy and concatenate）可以使训练稳定、加快训练。</p><h3 id="实现-5"><a href="#实现-5" class="headerlink" title="实现"></a>实现</h3><ol><li><p>用decoder $f_\theta(p)$为每个点$p$计算“是内部点的概率”（$f_\theta(p):[0,1]^3 \rightarrow [0,1]$），然后计算等值面（如3D形状则使用Marching Cubes方法，2D图像则使用applying thresholding方法），从而得到mesh表示。对于闭合形状，GT为$\mathcal{F}(p)$内部取1外部取0。</p></li><li><p>decoder训练方法（可采取的方法有两种）（作者的实现方式是一个decoder只对应一个物体种类）：</p><ul><li>A naive sampling：将训练形状的空间离散化（voxelize or rasterize），然后均匀采样。多分辨率采样依次得到$16^3,32^3,64^3,128^3$个点。先用低分辨率训练再逐渐用高分辨率训练（train the model progressively）。大致是立方复杂度。</li><li>A more efficient approach：采样更多的靠近表面的点，对离表面比较远的点不采样。然后对采样的每个点都有个权重$w_p$，是该点采样密度的倒数，用以补偿采样密度的变化。大致是平方复杂度。</li></ul></li><li><p>损失函数（$S$是采样得到的点集）：</p></li></ol><script type="math/tex; mode=display">\mathcal{L}(\theta)=\frac{\sum_{p \in S}|f_\theta(p)-\mathcal{F}(p)|^2 \cdot w_p}{\sum_{p \in S} w_p}</script><ol><li><p>IM-NET的应用举例</p><ul><li><p>Auto-Encoding：使用3D卷积从$64^3$的voxels中提取128维特征，然后decoder使用IM-NET。使用progressive training。</p></li><li><p>3D shape generation：使用autoencoder中训练好的encoder的输出作为latent-GAN的输入</p></li><li><p>single-view 3D reconstruction（SVR）：用ResNet将$128\times128$的图像编码成128维的特征向量。使用autoencoder中训练好的decoder，固定其参数不变，只训练ResNet的参数（encoder）去最小化mean squared loss</p></li></ul></li></ol><h3 id="实验-2"><a href="#实验-2" class="headerlink" title="实验"></a>实验</h3><ol><li><p>获取point cloud之后，转换为mesh（如Marching Cubes方法），最后用Poisson-disk Sampling获取10000个表面的点。</p></li><li><p>作者觉得chamfer distance（CD）、mean squared error（MSE）、IoU并不能很好的描述物体表面的视觉性质（例如桌子面发生一点点上下的移动将剧烈地改变IoU的值，但是视觉上的效果却差不多；并且IoU指标并不抵制表面的凹凸起伏），提出使用计算机图形领域的light field descriptor（LFD）指标。以及coverage score（COV-LFD）、Minimum Matching Distance（MMD-LFD）指标。</p></li><li><p>插值效果</p><p><img src="6_3.png" alt></p><p>优点：cleaner surface boundaries、smooth part movements、handles topology changes</p><p>但是不知道如何控制这种变化的过程</p></li></ol><h2 id="DISN-👍"><a href="#DISN-👍" class="headerlink" title="DISN 👍"></a>DISN 👍</h2><blockquote><p>NIPS2019《DISN: Deep Implicit Surface Network for High-quality Single-view 3D Reconstruction》</p><p>paper下载：<a href="https://arxiv.org/abs/1905.10711" target="_blank" rel="noopener">https://arxiv.org/abs/1905.10711</a></p></blockquote><h3 id="概述-6"><a href="#概述-6" class="headerlink" title="概述"></a>概述</h3><p>提出了一种单视图重建的网络，能生成高质量的3D形状。核心是预测“signed distance field”。不仅结合了“global image features”，还结合了“local features from the patch”，因而能够生成更多细节的高质量3D形状（例如细小结构、孔洞）。在单视图重建方面达到了SOTA。但是只能用于图片背景干净的输入。</p><p>注：</p><ul><li>Signed Distance Functions（SDF）是属于隐式的surface表示；传统的方法是显式的surface表示（例如mesh）</li><li>DISN == Deep Implicit Surface Network</li><li>原始生成的3D形状是点云，然后用Marching Cubes方法来确定iso-surface（以获得3D mesh）</li><li>缺点：要生成很多3D points才能确定等值面（最后需要从dense 3D grid中采样格点作为输入）。</li><li>优点：生成的分辨率可以任意高</li><li>还可以扩展到multi-view reconstruction和shape interpolation应用中</li><li>实现细粒度可能的原因是Local Feature是从images中的对应区域抠特征子图实现的。Local Feature预测出来的SDF值可以认为是总的SDF值的残差，残差对应“细粒度/细节”。</li></ul><p>TODO IDEA：</p><ul><li>IDEA1：生成空间分布，然后在分布中采样得到物体表面的点。</li><li>IDEA2：使用无符号的Distance Functions（记作$f(p)$），然后随机初始化一个3维空间点$p\in \mathbb{R}^3$，通过求解优化问题$min_{p} \frac{\partial f(p)}{\partial p}$，满足此问题的解$p$组成的一系列的点集就是物体的表面。相当于用导数来引导point在surface上的移动。这样理想情况下只需要初始化一个随机点就能使得该点遍历整个surface了（👍）</li><li>IDEA3：能否借鉴U-Net的思想生成高分辨率的细粒度SDF？</li></ul><h3 id="原理-4"><a href="#原理-4" class="headerlink" title="原理"></a>原理</h3><p>Signed Distance Functions（SDF）描述了空间某点到物体表面的有符号距离。</p><p>在物体外部则为正号，内部则为符号，在物体表面则为零。绝对值是到表面的距离。</p><p><img src="7_1.png" alt></p><p>然后采样很多个点，通过预测SDF的值，再用寻找等值面的方法来求得表面。</p><h3 id="实现-6"><a href="#实现-6" class="headerlink" title="实现"></a>实现</h3><p>首先估计相机姿态，目的是把3D点投射到2D plane上，用以确定2D feature map上local patch的位置。然后将point-wise feature、local feature、global feature联合起来推断SDF的值。</p><p>最后采样dense 3D grid，为每个格点预测SDF值，然后用Marching Cubes获取3D mesh。</p><p>Local Feature是从images中的对应区域提取的，point-wise feature直接就是MLP处理点坐标，global feature直接就是CNN处理整张图像。</p><p>整体结构（图中的“+”号是加法，作者实验表明两路的推断（两个decoder）比一路的推断（只使用一个decoder）效果要好）：</p><p><img src="7_2.png" alt></p><ol><li><p>Camera Pose Estimation</p><p><img src="7_3.png" alt></p><p>将输入图片用CNN推断出位移量$t$（Translation）和6D旋转表示$b$（Rotation）。由$b$可以用公式计算出旋转矩阵$R$，那么点$p$的空间变换就是$Rp+t$。对aligned model space（即world space）施加空间变换得到“预测的变换点云”，再与GT比较就可以计算出Loss。</p><p>Loss的计算方法如下（本质是mean squared error，$PC_w$代表world space中的点云，$p_G$代表camera space中的GT）：</p><p><img src="7_4.png" alt></p></li><li><p>SDF Prediction</p><ul><li><p>Point-wise feature：目的是将位置向量映射到更高维的空间中去（即图中的“Point feature”）</p></li><li><p>Global feature：利用CNN推断全局的特征向量</p></li><li><p>Local Feature：利用估计出来的相机参数将3D点$p$投射到2D image plane的点$q$去，然后将相应位置上的特征子图抠出来再concat，由此得到local feature。（因为特征图尺寸不一样，所以先用双线性插值再抠图）</p><p><img src="7_5.png" alt></p><p>那么SDF的结果就是两个decoder输出结果的和。可以认为是下面这个decoder分支是一种“residual SDF”，即一种残差预测结构。</p></li><li><p>Loss Functions：使用加权的损失函数，含义是使得更看重GT表面附近的SDF估计误差（其中$m_1&gt;m_2$）</p><p><img src="7_6.png" alt></p></li></ul></li></ol><h3 id="实验-3"><a href="#实验-3" class="headerlink" title="实验"></a>实验</h3><p>使用ShapeNet，并且train a single network on all categories，使用VGG-16作为CNN的提取特征。以CD、EMD、IoU作为指标。与多种模型（如AtlasNet, Pixel2Mesh, 3DN, OccNet and IMNET）进行比较，取得较好的性能。详情略。</p><p><img src="7_7.png" alt></p><h2 id="Neural-Renderer"><a href="#Neural-Renderer" class="headerlink" title="Neural Renderer"></a>Neural Renderer</h2><blockquote><p>CVPR2018《Neural 3D Mesh Renderer》</p><p>paper下载：<a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Kato_Neural_3D_Mesh_CVPR_2018_paper.html" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2018/html/Kato_Neural_3D_Mesh_CVPR_2018_paper.html</a></p></blockquote><h3 id="概述-7"><a href="#概述-7" class="headerlink" title="概述"></a>概述</h3><p>将Renderer的渲染过程近似可微化，使得渲染器（Renderer）可以嵌入到神经网络结构中来。渲染的对象是3D mesh，生成2D图像。由此作者根据Neural Renderer提出了两种应用：single-image 3D mesh reconstruction with silhouette image supervision和gradient-based 3D mesh editing with 2D supervision（such as 2D-to-3D style transfer and 3D DeepDream），展现了将Renderer集成到神经网络中的巨大威力</p><p><img src="8_1.png" alt></p><p>难点：如何将rasterization的过程可微分</p><p>注：</p><ul><li><p>polygon mesh的优点：compactness（容易表示、参数少）、geometric properties（suitability for geometric transformations. The rotation, translation, and scaling of objects are represented by simple operations on the vertices.）</p></li><li><p>渲染过程：Rendering consists of <strong>projecting</strong> the vertices of a mesh onto the screen coordinate system, and generating an image through regular grid <strong>sampling</strong>。后者的过程（rasterization）是离散的、不可微分的，需要我们近似梯度，从而实现反向传播算法</p></li><li><p>Our proposed renderer can flow gradients into texture, lighting, and cameras as well as object shapes.（mesh + texture + lighting + …  = 2D rendered image）</p></li><li><p>Polygon mesh表示法：3D顶点$\{v_i^o\}$和面${f_j}$（其中$f_j$是3维向量，内容是三角形面的顶点的索引）</p></li><li><p>2D-to-3D style transfer：</p><script type="math/tex; mode=display">\begin{align*}\frac{\partial Loss}{\partial Mesh}&=\frac{\partial Loss}{\partial Image}\times\frac{\partial Image}{\partial Mesh}\\&=(the\; gradient\; of\; loss) \times (the\; gradient\; of\;renderer)\end{align*}</script><p>风格迁移：将content mesh的“content”和style image的“style”融合到输出的output mesh上。需要用content loss衡量content的迁移损失、style loss衡量style的迁移损失<br>$Mesh \rightarrow Image$提供了对顶点和面纹理的梯度流，因此可以修改面纹理，将风格迁移到面纹理上。</p></li><li><p>Renderer使得一些对图像地操作（例如风格迁移）迁移到3D数据结构中成为可能。</p></li></ul><h3 id="实现-7"><a href="#实现-7" class="headerlink" title="实现"></a>实现</h3><ol><li><p>Rendering pipeline and its derivative</p><p>将3D顶点$\{v_i^o\}$投射到2D平面上得到$\{v_i^s\} $，此过程是可微分的</p><p>然后将$\{v_i^s\} $和$\{f_j\}$通过rasterization得到2D rendered image：考虑2D rendered image上的一个pixel点$P_j$（其颜色值$I_j$），如果这个pixel的center位于face $f_i$的内部，则此pixel的颜色值$I_j $被赋值（染色）成$I_{ij}$</p><p><img src="8_2.png" alt></p><p>图中只考虑$x_i$是可变的，其他暂时冻结住不考虑其变化</p><p>我们将突变的过程连续化（线性插值），得到“（d）”所示的效果。我们前向传播使用图“（b）”所示的曲线，反向传播的导数使用“（e）”所示的曲线。</p><p>其中颜色突变量$\delta_j^I=I(x_1)-I(x_0)$，位移变化量$\delta_j^x=x_1-x_0$，那么斜线的斜率就是$\frac{\delta_j^I}{\delta_j^x}$</p><p>考虑反向传播的误差信号$\delta_j^p = \frac{\partial Loss}{\partial I_j}$，当$\delta_j^p&gt;0$时$I_j$会减少，当$\delta_j^I&gt;0$时$I_j$会增大。为了不使得做无用功，我们要求$\delta_j^I \delta_j^p &lt; 0$才可以更新参数，所以：</p><p><img src="8_3.png" alt></p><p>类似地，当像素点本身就在face的内部时，则如下图所示：</p><p><img src="8_4.png" alt></p><p>并且其导数如下所示：</p><p><img src="8_5.png" alt></p><p>当多个faces时，our rasterizer draws only the frontmost face at each pixel, and do not flow gradients if they are occluded by surfaces not including $v_i$</p><p>Texture：位于face$\{v_1,v_2,v_3\}$上的点$p$可以被分解为$p=w_1v_1+w_2v_2+w_3v_3$，那么用$(w_1,w_2,w_3)$就可以表示出face$\{v_1,v_2,v_3\}$上的点。其纹理texture就可以利用$(w_1,w_2,w_3)$从 texture image$s_t\times s_t \times s_t$（each face has its own texture image）中查值获得。不难验证必定满足$w_1 + w_2 + w_3 = 1$</p><p>Lighting：考虑环境光$l^a$和定向光源$l^d$，那么pixel color$ I_j $经过光源渲染后的颜色就是$I^l_j = (l^a+(n^d\cdot n^j)l^d)I_j$（$n^j$是face上pixel的单位法向，$n^d$是定向光源的单位方向）</p></li><li><p>Single image 3D reconstruction</p><p><img src="8_6.png" alt></p><p>思想：match the ground truth silhouettes。有点像encoder-decoder结构</p><p>3D generator idea：deform an isotropic sphere with 642 vertices to generate a new mesh, therefore the mesh we use is specified by 642*3 parameters。然后使用silhouette loss（鼓励IoU越大越好）和smoothness loss（鼓励面面夹角越接近180°越好）训练此generation function</p><p>将mask作为附加通道加入到RGB图中</p></li><li><p>Gradient-based 3D mesh editing</p><p><img src="8_7.png" alt></p><p>We optimize a 3D mesh consisting of vertices, faces, and textures based on its rendered image</p><p>In this section, we propose a method to transfer the style of an image onto a mesh</p><p>其中content loss确保3D mseh的shape相同；style loss确保风格相同（图中的Loss指的是style loss）；同时使用了regularizer for noise reduction。总Loss就是所有loss的加权和，对顶点和纹理最小化loss的值</p><p>类似地，还可以用于3D DeepDream</p></li></ol><h2 id="Pixel2Mesh"><a href="#Pixel2Mesh" class="headerlink" title="Pixel2Mesh"></a>Pixel2Mesh</h2><blockquote><p>ECCV2018《Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images》</p><p>paper下载：<a href="http://openaccess.thecvf.com/content_ECCV_2018/html/Nanyang_Wang_Pixel2Mesh_Generating_3D_ECCV_2018_paper.html" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_ECCV_2018/html/Nanyang_Wang_Pixel2Mesh_Generating_3D_ECCV_2018_paper.html</a></p></blockquote><h3 id="概括"><a href="#概括" class="headerlink" title="概括"></a>概括</h3><p>提出了单视图输入的end-to-end框架，能生成三角形网格（triangular mesh）。使用GCN来处理mesh的顶点特征，将椭球形变来拟合表面，同时利用图像的特征。mesh从粗到精细化，顶点数变多。定义了4种loss来优化结果。结果具有更好的精细度，超越了SOTA。</p><p>注：</p><ul><li>形变法的优点：deep network is better at predicting residual。a series of deformations can be added up together, which allows shape to be gradually refined in detail。it provides the chance to encode any prior knowledge to the initial mesh。</li><li>限制条件：只能用于亏格（genus）为零的shape，因为椭球没有“孔”</li><li>具有上采样层，点数逐渐增多。Deformation block则负责精细化mesh</li></ul><h3 id="原理-5"><a href="#原理-5" class="headerlink" title="原理"></a>原理</h3><p><img src="9_2.png" alt></p><p>将mesh初始化为椭球，然后用GCN对vertex feature施加变换，从而实现对3D vertex point的位置空间变换。然后上采样graph，以增加点数。不断重复此过程，实现从粗到精的过渡。image feature融合进入deformation blocks中来实现全局shape的把握。</p><h3 id="实现-8"><a href="#实现-8" class="headerlink" title="实现"></a>实现</h3><p>整体由image feature network和cascaded mesh deformation network组成。</p><p>deformation blocks：将图像的2D特征提升到3D vertex的特征中来（1280维），然后cancat（1280+128=1408维），再GCN（输出空间坐标3维和特征128维）。</p><p>graph unpooling layers：上采样图，增加顶点数，从而可以实现更精细的结构</p><ol><li><p>Initial ellipsoid</p><ul><li><p>ellipsoid with average size placed at the common location</p></li><li><p>initial feature contains only the 3D coordinate of each vertex（3维。特征提取之后才是128维的feature）</p></li></ul></li><li><p>GCN</p><p>将mesh用vertex和edge表示，那么就是一张graph。每个vertex上都有个feature。通过一次GCN层，得到变换后的特征：</p><p><img src="9_1.png" alt></p><p>其中$w_0$和$w_1$都是$d_{l+1}\times d_{l} $尺寸的变换矩阵，即经过一层GCN层其特征向量的维度可能发生变化。我们的特征向量$f_p$是“3D空间坐标”、“局部形状特征”和“输入图像特征”的cancat。使用GCN将变换特征，等效于空间点位置的变换以及特征提取。</p></li><li><p>Mesh deformation block</p><p><img src="9_3.png" alt></p><p>将“input image feature”提升到3D空间中的点特征，得到1280维的特征。与128维的形状特征concat得到1408维的特征，再进行GCN，其中一个分支预测点的3维坐标，另一个分支预测128维特征。</p><p>提升到3D空间：将3D点投射到2D平面上，然后根据图像中的gird feature进行双线性插值得到该点的特征。其中feature maps取自VGG16，一共有1280个通道，所以输出的特征是1280维。</p><p>残差结构的graph based ResNet（G-ResNet）有助于efficient exchange of the information between vertices、提升感受野。</p><p>所有的deformation blocks似乎是共用同一个VGG16的feature maps</p></li><li><p>Graph unpooling layer</p><p>目的：增加点的数量。使用图中Edge-based的方法。</p><p><img src="9_4.png" alt></p><p>This edge-based unpooling uniformly upsamples the vertices, and doesn’t causes the imbalanced vertex degrees.</p></li><li><p>Losses and Regularizations</p><ul><li>Chamfer loss：It is reasonably good to regress the vertices close to its correct position, however is not sufficient to produce nice 3D mesh</li><li>Normal loss：this loss requires the edge between a vertex with its neighbors to perpendicular to the observation from the ground truth. enforce the consistency of surface normal</li><li>Laplacian regularization：prevent the vertices from moving too freely, which potentially avoids mesh self-intersection. encourages neighboring vertices to have the same movement. laplacian regularization to maintain relative location between neighboring vertices during deformation,</li><li>Edge length regularization：edge length regularization to prevent outliers. 鼓励edge越短越好</li><li>总：就是上面4种的线性加权和</li></ul></li></ol><h2 id="Pixel2Mesh-1"><a href="#Pixel2Mesh-1" class="headerlink" title="Pixel2Mesh++"></a>Pixel2Mesh++</h2><blockquote><p>ICCV2019《Pixel2Mesh++: Multi-View 3D Mesh Generation via Deformation》</p><p>paper下载：<a href="https://arxiv.org/abs/1908.01491" target="_blank" rel="noopener">https://arxiv.org/abs/1908.01491</a></p></blockquote><h3 id="概括-1"><a href="#概括-1" class="headerlink" title="概括"></a>概括</h3><p>从多视图输入、已知相机姿态的条件下重建出3D Mesh，是Pixel2Mesh的升级版。同样是利用GCN抽取顶点的特征、对初始mesh进行粗到细的形变。不同的是形变的过程为：对每个顶点分别提出形变的候选点，然后利用projection、GCN操作、结合图像的feature maps变换特征得到每个候选点的重要性系数，进行坐标的线性加权得到形变后的坐标。能够生成视觉效果好的Mesh，泛化能力强，达到SOTA。</p><p>注：</p><ul><li>Pixel2Mesh生成的mesh往往只在single-image的视角方向效果好，再其他视角下效果差</li><li>多视图输入的关键原理：使用诸如mean、std、max的统计特征，实现可以接受任意多视图的输入</li><li>Pixel2Mesh++由perceptual network（VGG16）、coarse shape generation（Pixel2Mesh）和Multi-View Deformation Network（MDN）组成。</li><li>MDN是end-to-end trainable的。可接受任意数量的视图输入</li><li>注意在MDN中没有上采样层，不能增加点的数量。</li><li>作者主要的贡献是设计了MDN，而MDN的作用更像是微调已生成的mesh</li><li>候选点（Hypothesis）本质上是对vertex周围环境的感知，GCN起到类似于local graph的neighborhood之间的特征交互融合、特征提取的作用。最后根据GCN得到的score来make movement decision。而Pixel2Mesh没有感知vertex周围环境的能力。</li></ul><h3 id="原理-6"><a href="#原理-6" class="headerlink" title="原理"></a>原理</h3><p><img src="10_1.png" alt></p><p>基于Pixel2Mesh做初始mesh的生成。从多视图中用VGG16抽取特征，底层级称作geometry feature，高层级称作semantic feature。然后为每个vertex生成形变候选点，利用图像抽取来的特征为每个候选点计算score，最后根据score和3D坐标作线性加权得到最终形变后的位置。多次形变即可得到输出的mesh。</p><h3 id="实现-9"><a href="#实现-9" class="headerlink" title="实现"></a>实现</h3><ol><li><p>Deformation Hypothesis Sampling</p><p>目的：propose deformation hypotheses（形变候选点）for each vertex</p><p><img src="10_2.png" alt></p><p>实现：在每个vertex的位置上放置一个level-1 icosahedron（具有42个顶点、80个面、120条边），然后为每个vertex构造一个local graph（local graph的边由“level-1 icosahedron”的120条边、vertex到“level-1 icosahedron”的顶点的连线组成。所以一个local graph一共有120+42=162条边，42+1=43个点）。每个顶点都要构造一个local graph，然后喂给接下来的“Cross-View Perceptual Feature Pooling”步骤</p></li><li><p>Cross-View Perceptual Feature Pooling</p><p>目的：assign each node（in the local graph）features from the multiple input color images</p><p><img src="10_3.png" alt></p><p>大体类似于Pixel2Mesh中的“perceptual feature pooling”操作。feature maps来自VGG16的conv1_2、conv2_2、conv3_3（它们是底层级的geometry feature），一个视图concat起来输出64+128+256=448长度的特征。</p><p>考虑到对不同数量的视图输入如果concat的话会得到不同长度的feature。我们改而使用统计特征，实现固定长度的输出。使用mean、max、std这三种统计特征。而且使用统计特征对视图的输入的顺序具有不变性。所以特征长度变为448*3=1344维。再把空间坐标concat进去得到1344+3=1347维度的特征。</p></li><li><p>Deformation Reasoning</p><p>目的：reason an optimal deformation for each vertex from the hypotheses using pooled cross-view perceptual features</p><p><img src="10_4.png" alt></p><p>实现：将local graph和local graph nodes feature（即pooled cross-view perceptual features）丢到scoring network（本质是GCN）中得到每个nodes的重要性得分$s_i$（softmax输出后自动满足$\sum_{i=1}^{43} s_i=1$）。若候选点的空间位置记作$h_i$（包括vertex，$i=1…43$），那么形变后的位置就是位置的线性加权：$\sum_{i=1}^{43}s_i h_i$。对每个顶点都运行一遍，就得到了最终形变后的结果。</p></li><li><p>Losses</p><p>损失函数照抄Pixel2Mesh的，但是唯一的区别是作者还extends the Chamfer distance loss to a resampled version，即：从mesh的face上均匀采样点，采样的点数正比于face的面积，由此可以额外采样出4000个点，加上2466个原本的mesh的顶点，总共6466个点，根据6466个点计算Chamfer distance loss</p><p>re-sample Chamfer loss的功能：helps to remove artifacts in the results，解决了points are not uniformly distributed on the surface的问题</p></li><li><p>Implementation Details</p><p>we use Pixel2Mesh to generate a coarse shape with 2466 vertices → 最终生成的点的数量就是2466个</p></li></ol><h3 id="实验-4"><a href="#实验-4" class="headerlink" title="实验"></a>实验</h3><ul><li><p>对比的网络：</p><ul><li>P2M-M：we directly run single-view Pixel2Mesh on each of the input image and fuse multiple results</li><li>MVP2M：we replace the perceptual feature pooling to our cross-view version to enable Pixel2Mesh for the multi-view scenario</li><li>LSM：Learnt Stereo Machine</li><li>3DR2N2：3D Recurrent Reconstruction Neural Network</li></ul></li><li><p>性能：Our model significantly outperforms previous methods（based on F-score）</p></li></ul><h2 id="AtlasNet-👍"><a href="#AtlasNet-👍" class="headerlink" title="AtlasNet 👍"></a>AtlasNet 👍</h2><blockquote><p>CVPR2018《A Papier-Mâché Approach to Learning 3D Surface Generation》</p><p>paper下载：<a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Groueix_A_Papier-Mache_Approach_CVPR_2018_paper.html" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2018/html/Groueix_A_Papier-Mache_Approach_CVPR_2018_paper.html</a></p></blockquote><h3 id="概述-8"><a href="#概述-8" class="headerlink" title="概述"></a>概述</h3><p>提出使用基于<a href="https://zh.wikipedia.org/wiki/流形" target="_blank" rel="noopener">流形</a>中Charts和Atlases的概念的3Dshapes的surface生成方法。将surface表示成多个局部小表面的集合（类似于Charts的概念）。提高了精度、泛化强、能生成任意精度的分辨率。AtlasNet的本质是一个3D shape decoder（输入形状描述子输出mesh/pointcloud）。AtlasNet可以应用于形状自编码、3D重建等多个领域。</p><p>注：</p><ul><li>pointcloud的缺点：不能直接表示与邻居的关系（no surface connectivity），使得很难得到光滑、高保真的流形</li><li>polygonal mesh的特点：对光滑流形的分块平面近似</li><li>Surface parameterization： Establishing a connection between the surface of the 3D shape and a 2D domain</li><li>3D重建时输入的对象可以是pointcloud、images等，因为AtlasNet只要求特征向量作为输入，可以使用其他网络提取输入对象的特征向量之后作为AtlasNet的输入。</li><li>chart：3D→2D（1个MLP函数）；atlas：a set of charts（多个MLPs）</li><li>AtlasNet的功能：learn an atlas for 2-manifold $\mathcal{S}$</li><li>缺点：生成的mesh不闭合，有小孔，不同的曲面块之间有overlap</li></ul><h3 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h3><p><img src="11_1.png" alt></p><p>将2D单位正方形平面的点集（$[0,1]\times [0,1]$）使用MLP映射到3D曲面上。有多个MLP，于是就能得到多个小的3D曲面。多个小3D曲面就能组成物体。即，we seek to approximate the target surface locally by<br>mapping a set of squares to the surface of the 3D shape</p><script type="math/tex; mode=display">[0,1]\times[0,1] \leftrightarrow 3D \; surface</script><p>这样1个MLP就能局部生成一个2-manifold（3D曲面）了</p><ul><li><p>2D→3D：通过MLP得到</p></li><li><p>3D→2D：UV parameterization of the surface / 2D （UV） embedding to a plane</p></li></ul><p>于是在2D平面上的性质就能迁移到3D曲面中：例如2D平面有纹理，则在3D曲面的对应点上也有相同的纹理</p><p>生成mesh的方法：将方形的2Dmesh映射到3D空间中，并保持点与点之间的连接关系</p><h3 id="实现-10"><a href="#实现-10" class="headerlink" title="实现"></a>实现</h3><ol><li><p>Learning to decode a surface</p><p>目标：有一个形状描述子$\mathtt{x}$（形状特征向量），请据此生成the surface of the shape</p><p>实现：</p><ul><li>有N个learnable parameterizations（MLP）$\phi_{\theta_i}$，$i\in\{1,…,N\}$，其中$\theta_{i}$是MLP参数</li><li><p>将$\mathtt{x}$和$(0,1)\times(0,1)$采样的点$(x,y)$进行cancat，再送到MLP中</p></li><li><p>损失函数：Chamfer loss。通过计算两个点集之间的Chamfer Distance来比较。</p></li></ul><p>注意到MLP并没有明确禁止所编码的区域重叠（符合atlas的要求之一），并且最好覆盖整个shape（全覆盖才能使得Chamfer loss变小）</p></li><li><p>Implementation details</p><ul><li>auto-encoder中的encoder是PointNet；形状描述子的维度：1024；输入点云的点数：250~2500</li><li>reconstruction中的encoder是ResNet-18；只训练encoder，而decoder则是autoencoder中的decoder（固定参数，不训练）</li><li>decoder是4层的MLP：1024（ReLU）、512（ReLU）、256（ReLU）、128（tanh）。使用tanh的原因可能是点云经过单位球化了</li><li>输出点云的点数：2500</li><li>2D采样方法：网格规则采样</li></ul></li><li><p>Mesh generation</p><p>有三种方法：</p><ul><li>Propagate the patch-grid edges to the 3D points：即transfer a regular mesh on the unit square to 3D，使得可以生成高分辨率的mesh，是一种自然的生成mesh的好方法（因为mesh的拓扑连接关系源自于2D grid的edge的连接方式）【AtlasNet专用，效果好】</li><li>Generate a highly dense point cloud and use Poisson surface reconstruction（PSR）：生成很多点以及他们的normals，然后用PSR方法来得到mesh【Baseline专用，AtlasNet可用】</li><li>Sample points on a closed surface rather than patches：从3D球面采样输入的点，而不是从多个2D单位正方形中采样。可以生成闭合图形，但是似乎只能用一个MLP。【AtlasNet专用，记作“1 sphere”】</li></ul></li></ol><h3 id="实验-5"><a href="#实验-5" class="headerlink" title="实验"></a>实验</h3><ol><li><p>数据集：standard ShapeNet Core dataset</p></li><li><p>评价指标：Chamfer distance（CD，用于衡量pointcloud）和Metro（average Euclidean distance between the two meshes，用于衡量mesh）</p></li><li><p>Baseline/Oracle</p><ul><li>“Points baseline”：如上一幅图的（a）所示。直接输入1024维度的形状描述子，然后输出2500*3=7500维度，即2500个点（MLP层数为1024（ReLU）、512（ReLU）、256（ReLU）、7500（tanh））。最终输出的是pointcloud，所以没有“Metro”指标。</li><li>“Pointsbaseline + normals”：输出空间指标和法向量，一个点是$\mathbb{R}^6$。然后根据切平面来增加点的数量（Augmentation）。然后使用PSR算法得到mesh。最终输出的是mesh。</li><li>Oracle：从GT中随机采样点，然后计算CD和Metro的值。代表了模型所能达到的性能的上界（因为是由GT生成的）</li></ul></li><li><p>3D reconstruction结果</p><p><img src="11_2.png" alt></p></li><li><p>其他实验</p><ul><li><p>Generalization across object categories：略</p></li><li><p>Singleview reconstruction：略</p></li><li><p>Shape interpolation</p><p><img src="11_3.png" alt></p></li><li><p>Finding shape correspondences：Notice that we get semantically meaningful correspondences, such as the chair back, seat, and legs without any supervision from the dataset on semantic information.</p><p><img src="11_4.png" alt></p></li><li><p>Mesh parameterization：our inferred atlas usually has relatively high texture distortion. But we can minimize distortion with off-theshelf geometric optimization, yielding small distortion.</p><p><img src="11_8.png" alt></p></li><li><p>Excess of distortion</p><p><img src="11_5.png" alt></p></li><li><p>Topological issues</p><p><img src="11_6.png" alt></p></li><li><p>Deformable shapes</p><p><img src="11_7.png" alt></p></li></ul></li></ol><h2 id="Deep-Marching-Cubes"><a href="#Deep-Marching-Cubes" class="headerlink" title="Deep Marching Cubes"></a>Deep Marching Cubes</h2><blockquote><p>CVPR2018《Deep Marching Cubes: Learning Explicit Surface Representations》</p><p>paper下载：<a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Liao_Deep_Marching_Cubes_CVPR_2018_paper.html" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2018/html/Liao_Deep_Marching_Cubes_CVPR_2018_paper.html</a></p></blockquote><h3 id="概述-9"><a href="#概述-9" class="headerlink" title="概述"></a>概述</h3><p>提出了一种end-to-end的surface prediction方法。即“Differentiable Marching Cubes Layer”，使得可作为神经网络的最后一层来输出mesh。并且设计了loss函数使得可以在sparse point supervision上训练。并且能补全形状、分离物体的内外表面，即使在GT是稀疏和不完全的情况下。可以用于从点云中推断形状。可以与各种encoder、shape inference techniques结合使用。</p><p>特点：</p><ul><li>differentiable, end-to-end</li><li>predicts explicit surface representations of arbitrary topology</li><li>avoids the need for defining auxiliary losses or converting target meshes to implicit distance fields</li><li>more accurate reconstructions while also handling noise and missing observations</li><li>separating inside from outside even if the ground truth is sparse or not watertight</li><li>easily integrating additional priors about the surface（e.g., smoothness）</li><li>does not require explicit surface ground truth</li></ul><p>注：</p><ul><li>普通的marching cubes的输入是SDF，而deep marching cubes输入是$O$和$X$。所以网络的输出类型也要要求不同。</li><li>现有的3D surface prediction不能end-to-end训练，因为他们需要中间表示（例如TSDF），因而需要后处理（例如marching cubes算法）</li><li>普通的marching cubes算法不可微分</li><li>传统的表面重建方法：<ul><li>Voxel based method <ul><li>output: voxel occupancy / TSDF</li><li>post-processing: Marching cubes</li></ul></li><li>Point based method<ul><li>output: points</li><li>post-processing: Poisson surface reconstruction / SSD</li></ul></li></ul></li><li>传统方法的缺点：<ul><li>the ground truth of the implicit representation is often hard to obtain, e.g., in the presence of a noisy and incomplete point cloud or when the inside and outside of the object is unknown.</li><li>these methods only optimize an auxiliary loss defined on an intermediate representation and require an additional postprocessing step for surface extraction. Thus they are unable to directly constrain the properties of the predicted surface.</li></ul></li></ul><h3 id="实现-11"><a href="#实现-11" class="headerlink" title="实现"></a>实现</h3><ol><li><p>传统的Marching Cubes（非本文的方法）</p><p>分为两步：</p><ul><li>estimation of the topology：确定cell的拓扑类型（面的顶点位置不确定，只是知道拓扑概念上的位置）</li><li>prediction of the vertex locations of the triangles：确定面的顶点的具体位置</li></ul><p>记号：</p><ul><li>$D \in \mathbb{R}^{N\times N\times N}$为signed distance field。表示到surface的有向距离。内部为正，外部为负，表面为零。其元素记作$d_n$，其中$n=(i,j,k)\in \mathbb{N}^3$是索引</li></ul><p>实现：</p><ul><li><p>首先确定cell’s surface topology T。假设$D\in \mathbb{R}^{3\times 3\times 3}$，那么相当于这27个点分布在二阶魔方的各个角块的顶角上，它只是点。cell就是指二阶魔方的8个角块，它具有体积。然后根据d是否符号改变（穿过零）确定这个cell的拓扑类型。（只是确定了拓扑类型，图中蓝色面的顶点的具体位置并不知道，需要下一步来计算）</p><p><img src="12_1.png" alt></p></li><li><p>然后如果这个cell内部存在蓝色平面，就需要计算蓝色平面顶点的具体位置。使用线性插值得到。（如果是上图中的左上角的第一种类型，那就不用计算了，这种类型往往出现在物体的内部、或者是远离物体的外部）</p><p><img src="12_2.png" alt></p></li></ul></li><li><p>Differentiable Marching Cubes</p><p>记号：</p><ul><li><p>每个格点的伯努利分布参数：$O\in [0,1]^{N\times N\times N}$，$o_n\in[0,1]$</p></li><li><p>每个格点的面顶点的偏移量：$X\in [0,1]^{N\times N\times N\times3}$，$x_n\in [0,1]^3$</p></li></ul><p>计算：</p><ul><li>那么格点$n$处于状态$t\in \{0,1\}$（1代表被占据（红色表示），0代表不被占据（绿色表示））时的概率为$p_n(t)=(o_n)^t(1-o_n)^{1-t}$</li><li>那么grid cell n处于状态$T$时的概率为$p_n(T)=\prod_{m\in \{0,1\}^3}(o_{n+m})^{t_m}(1-o_{n+m})^{1-t_m}$。其中$2^8$种顶点的排列方式中只有140种是有效的拓扑形式。</li><li>那么entire grid的概率分布就是$p(\{T_n\})=\prod_{n\in\mathcal{T}}p_n(T_n)$。其中$\mathcal{T}=\{1,…,N-1\}^{3}$有$(N-1)^3$个cell</li></ul><p>网络结构图：（输入点云，然后预测出$O$和$X$，从而预测出mesh）</p><p><img src="12_3.png" alt></p><p>Loss：（包含4项）</p><ul><li>Point to Mesh Loss：最小化observed 3D points到mesh平面的几何距离之和。作用：directly<br>measures the geometric error of the inferred mesh</li><li>Occupancy Loss：鼓励边界上不被占据，鼓励内部被占据</li><li>Smoothness Loss：鼓励相邻格点的占据状态不变</li><li>Curvature Loss：鼓励相邻cell的normal orientation的smooth transitions</li></ul></li></ol><h2 id="DeepSDF-👍"><a href="#DeepSDF-👍" class="headerlink" title="DeepSDF 👍"></a>DeepSDF 👍</h2><blockquote><p>CVPR2019《DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation》</p><p>paper下载：<a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Park_DeepSDF_Learning_Continuous_Signed_Distance_Functions_for_Shape_Representation_CVPR_2019_paper.html" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_CVPR_2019/html/Park_DeepSDF_Learning_Continuous_Signed_Distance_Functions_for_Shape_Representation_CVPR_2019_paper.html</a></p></blockquote><h3 id="概述-10"><a href="#概述-10" class="headerlink" title="概述"></a>概述</h3><p>DeepSDF能生成具有复杂拓扑结构的、高质量的、闭合的连续表面、可用于形状插值、形状补全。一个DeepSDF能够表示一个类别的形状（一类物体训练一个模型，而传统的SDF方法一个模型只能表示一个物体的形状）。取得了SOTA。而且模型的大小缩小了。</p><p><img src="13_3.png" alt></p><p>本质：使用“coordinate + shape latent vector”作为输入，通过MLP后输出SDF值。训练时同时优化“shape vector”和“MLP parameters”（梯度下降优化，最后得到的shape vector就是训练集的shape vector）。推断“latent vector”时使用梯度下降不断迭代优化“latent vector”的初始值，从而得到符合observation的latent vector的值。</p><p>注：</p><ul><li>DeepSDF最有用的用途是Shape Completion（inference + feed-forward + marchingcubes）、Shape Interpolation（inference + interpolate + feed-forward + marchingcubes）</li><li>SDF：有向距离函数：内部负数、外部正数、表面为零。</li><li>“auto-decoder”可用于降噪、补全、错误检测</li><li>补全：只输入部分数据，然后生成latent vector（形状信息），再生成完整的shape prediction</li><li>最后还是要用marching cubes来生成等值面</li><li>feed-forward耗时短；inference耗时长（对latent vector的优化速度慢，或许可以考虑替换掉ADAM优化器）</li></ul><p>TODO IDEA：</p><ul><li>或许可以用于生成动态的3D物体、或者是带有纹理材质的物体。</li><li>不同物体形状在latent vector分布中的位置不能指定。或许latent vector可以是类似one-hot的，一个变量代表一个物体种类，每个变量都服从高斯，然后只需要从高斯采样就能得到物体形状了。可能类似于Variational Auto-Decoder？</li></ul><h3 id="实现-12"><a href="#实现-12" class="headerlink" title="实现"></a>实现</h3><p>通过学习一个“shape-conditioned classifier”，决策边界（最后一层是tanh）就是物体表面。</p><p>使用“auto-decoder”，接受latent vector和point coordinate作为输入，输出SDF值。</p><ul><li>training：已知observation（点坐标&amp;SDF）来优化latent vector $z$和model parameters $\theta$</li><li>inference：已知observation（点坐标&amp;SDF）求latent vector $z$</li><li>feed-forward：已知latent vector $z$和点坐标，求SDF</li></ul><p><img src="13_1.png" alt></p><p>记号：</p><ul><li>coordinate $x$丢到模型里得到的SDF估计（latent vector为$z$）：$f_\theta(z,x)$</li><li>第i个形状的coordinate $x$处真实的SDF值：${SDF^i}(x)$</li><li>第i个形状的数据集（包含点坐标、真实的SDF值）：$X_i := \{(x_j,s_j):s_j=SDF^i(x_j)\}$</li></ul><p>第一种方式是使用上图（a）所示的方法。但是一个模型只能对应一个物体。故我们不采用。</p><p>第二种方式是使用上图（b）所示的方法。一个模型可以对应一整类物体（例如各种小车）。我们采样这种方法。但是code未知，我们使用“auto-decoder”的学习方式来自动学习code的分布（注意这里的code不是人为指定的分布，例如不是one-hot）</p><ul><li><p>训练（training）：我们首先初始化code为一个很小的值（例如$\mathcal{N}(0,0.01^2)$），然后根据已知的coordinate和SDF值来训练网络，反向传播时根据$\frac{\partial Loss}{\partial z_i}$和$\frac{\partial Loss}{\partial \theta}$分别更新Code $z_i$和Parameter $\theta$。</p><p><img src="13_5.png" alt></p></li><li><p>推断（inference）：根据已知的coordinate和SDF值，利用反向传播的$\frac{\partial Loss}{\partial z}$，更新优化Code $z$</p><p><img src="13_4.png" alt></p></li><li><p>前向传播（暂且称作feed-forward吧）：根据latent vector $z$和点坐标$x$得到预测的SDF值</p></li></ul><p>注：</p><ul><li><p>还可以计算得到surface normals：方法一和二（采用SDF的方法）都能通过计算$\frac{\partial f_\theta(x)}{\partial x}$来计算估计得到的表面法向（因为沿着法线方向SDF变化最快）</p></li><li><p>对于一个点的损失函数：$\mathcal{L}(f_\theta(x),s)=|clamp(f_\theta(x),\delta)-clamp(s,\delta)|$，以$f_\theta(x)$、$s$为x、y轴画图得（$\delta$越小越能描述表面的细节）：</p><p><img src="13_2.png" alt></p></li><li><p>“auto-decoder”是对“coded shape deepsdf”的训练方式。</p></li><li><p>为什么不用“encoder-decoder”结构：</p><p>传统的“encoder-decoder”结构的使用方法：</p><ul><li>训练：“encoder-decoder”两部分一起训练</li><li>测试：丢掉训练好的encoder。在形状补全任务中，首先准备待补全的训练数据，然后重新训练另一个encoder。</li></ul><p>作者认为训练“encoder-decoder”结构时，最后使用时只是用到了decoder，而没有用到训练完成的encoder，极大浪费了计算力。并且使用“encoder-decoder”进行形状补全时，训练新的encoder时的训练数据也要是“待补全”的数据集（这样才能从输入推断出latent vector）。</p></li><li><p>DeepSDF可以只输入部分的observation（例如depth map中点，只是部分的表面的点），然后推断出latent vector $z$的值，所以可以用于形状补全。</p></li><li><p>整个实现都是基于在物体对齐的条件下进行的（in a canonical pose）。否则同一个coordinate会因为不对齐（例如旋转）而会有不同的SDF值</p></li></ul><h3 id="实现-13"><a href="#实现-13" class="headerlink" title="实现"></a>实现</h3><p>实施细节见前面的部分。下面是实验部分（使用ShapeNet）：</p><ol><li><p>Representing Known 3D Shapes（represent training data）</p><p>用于测试latent vector和MLP是否能充分表示精细的结构。所使用的物体是在训练集内的。</p><p>实现过程：利用GT（在训练集中）的$X$（包含point coordinate和SDF）来inference得到这个物体所对应的latent vector的值；然后输入latent vector和密集网格采样的point coordinate，通过正向传播得到预测的SDF值，再用marching cubes来重建表面。</p></li><li><p>Representing Test 3D Shapes（use learned feature representation to reconstruct unseen shapes）</p><p>用于测试“得到latent vector的方法”是否能用于全新未见过的物体</p><p>实现过程：利用GT（不在训练集中）通过inference的方法得到latent vector；再正向传播、marching cubes得到表面。</p></li><li><p>Shape Completion（apply shape priors to complete partial shapes）</p><p>已知物体表面的一些部分（例如depth map只涉及一些表面），求整个物体的形状</p><p>实现过程：根据depth map可以知道位于surface上的点，然后估计surface normal。沿着surface normal的方向偏移$\pm \eta$的距离得到两个点（位于surface两侧且距离surface$\eta$的两个点），这样就得到SDF的样本数据。利用SDF样本数据先inference得到latent vector，再正向传播、marching cube就能得到mesh。</p><p>注：depth map的视角需要与物体对齐。作者还额外从“介于surface和camera之间的free-space”中随机采样一些点作为empty space points（或者称作free-space observations，且SDF值大于零），使得样本的点数增加，且不会在free-space中生成奇怪的形状。而且添加了“freespace loss”。</p></li><li><p>Latent Space Shape Interpolation（learn smooth and complete shape embedding space from which we can sample new shapes）</p><p>过程：利用training最后时得到的latent vector（或inference得到的都可以），将两个物体的latent vector线性插值再feed-forword得到中间状态的物体</p></li></ol><h2 id="GEOMetrics"><a href="#GEOMetrics" class="headerlink" title="GEOMetrics"></a>GEOMetrics</h2><blockquote><p>PMLR《GEOMetrics: Exploiting Geometric Structure for Graph-Encoded Objects》</p><p>paper下载：<a href="http://proceedings.mlr.press/v97/smith19a/smith19a.pdf" target="_blank" rel="noopener">http://proceedings.mlr.press/v97/smith19a/smith19a.pdf</a></p></blockquote><h3 id="概述-11"><a href="#概述-11" class="headerlink" title="概述"></a>概述</h3><p>能够生成Adaptive mesh（顶点密度不均匀的mesh），作者是第一个。提出了0N-GCN layers，避免了顶点信息的平滑；提出了Adaptive Face Splitting，在高曲率的地方自动分割三角面片（改变拓扑），使得可以生成细节；改进了Chamfer Loss得到Point-to-point loss和Point-to-surface loss。提出了global mesh loss用于比对形状的全局信息。并最后用单图重建mesh的任务来说明其性能。达到了SOTA，视觉效果好，所需的顶点数较少。</p><p>基本流程（主要基于deformation）：对初始mesh（球）附带特征（例如通过project投射到CNN的feature map上），然后用0N-GCN进行mesh的形变和特征变换，最后用face splitting策略将曲率大的三角面片添加顶点继续细分。不断重复得到最终输出mesh。</p><p><img src="14_3.png" alt></p><p>注：</p><ul><li><p>GEOMetrics == Geometrically Exploited Object Metrics。利用了graph-encoded objects的优势和几何结构</p></li><li><p>将triangle mesh视作graph便可以使用图卷积的方法</p></li><li><p>传统mesh重建的效果：顶点和面的分布比较均匀。没有充分利用mesh表示的独有优势。应当在细节处放置更多顶点、在平坦处放置顶点较少。这样才能with smaller space requirements and higher precision</p></li><li><p>普通的GCN可能会使得顶点的信息平滑（是由于取neighborhood mean造成的），作者提出对于一些特征维度不聚合邻居信息而是保持原样，以避免特征的平滑（over-smoothing）。</p></li><li><p>普通的Chamfer Distance只是约束vertex position，并没有约束三角面片。</p></li><li><p>face splitting技术似乎生成了许多不想要的细节，例如：</p><p><img src="14_2.png" alt></p></li></ul><h3 id="原理-7"><a href="#原理-7" class="headerlink" title="原理"></a>原理</h3><p><img src="14_1.png" alt></p><p>上图是mesh reconstruction module的构成（主要功能：细化输入的mesh）。整个系统由m=3个这样的module串联组成。系统的输入是球状mesh和一张图片。不同module的cnn feature map其实都是同一个。</p><p>首先将mesh投射到feature map上使得vertices带上feature map上的feature vector。然后使用作者发明的GCN（Zero-Neighbor Graph Convolutional Networks）进行graph的特征提取和deformation。最后使用face splitting技术分割曲率大的面（以描述细节部分）。最后得到输出的mesh。再重复m-1次module的流程，最后输出的mesh就是最终结果。</p><h3 id="实现-14"><a href="#实现-14" class="headerlink" title="实现"></a>实现</h3><ol><li><p>feature extraction</p><p>得到的feature vector应该是以下三者/两者的cancat：</p><ul><li>feature map对应位置上的feature vector</li><li>3D point的空间坐标</li><li>前一个module输出的mesh的feature vector（如果当前module不是第一个的话）</li></ul></li><li><p>mesh deformation</p><p>使用多个“0N-GCN layers”进行特征变换，输出residual prediction（位置的偏移量），那么3D点的最终位置就是original position + residual prediction</p><p>“0N-GCN layers”的目的：为了解决顶点的特征被平滑掉的问题</p><p>传统GCN：$H’=\sigma(AHW+b)$，其中A是NxN的邻接矩阵，H是NxF的输入特征向量矩阵，H’是NxF’的输出特征向量矩阵，W是权重矩阵，b是偏置向量，$\sigma(\cdot)$是激活函数</p><p>方法：一部分特征维度取邻接矩阵的零次幂（identity matrix）以不聚合特征，其余维度正常聚合特征</p><p>即现如今变成：$H’=HW,\;H’’=\sigma([AH’_{0:i}||A^0 H’_{i:}]+b)$</p><p>其中$[\cdot||\cdot]$是concat算符（cancat到矩阵的右边），且$i\in[0,F’-1]$</p><p>但是其中$i$是怎么选取的不知道。</p></li><li><p>face splitting</p><p>对曲率大的曲面，在center位置添加顶点，并添加3条连向它的边。</p><p><img src="14_4.png" alt></p><p>曲率的计算方法（似乎是定性表示）：</p><ul><li>首先根据三角面片的3个顶点，可以计算得到其法向$N_f$</li><li>然后对于三角面片$f$，计算其邻居面$i\in \mathcal{H}_f$的法向量$N_i$，并计算$N_i $与$N_f$的夹角$\theta_{f,i}$</li><li>然后计算$\theta_{f,i}$的平均值$\frac{1}{|\mathcal{H}_f|}\sum_{i\in \mathcal{H}_f} \theta_{f,i}$便得到在面$f$处的曲率$C_f$</li></ul><p>根据设定的曲率阈值$\alpha$来判断曲率大不大，大的话就进行face splitting</p></li><li><p>Differentiable Surface Sampling Losses（Chamfer Loss的改版）</p><p>用于描述local topology，即三角面片的约束</p><p><img src="14_7.png" alt></p><p>过程：</p><ul><li><p>首先需要从GT mesh和pred mesh的surface中采样，分别得到点集$S$和$\hat{S} $</p></li><li><p>如果是Point-to-point的形式：（点到点的距离）</p><p><img src="14_5.png" alt></p></li><li><p>如果是Point-to-surface的形式：（点到面的距离）</p><p><img src="14_6.png" alt></p></li></ul></li><li><p>Global Encoding of Graphs（用于描述全局信息）</p><p>global mesh loss，或称作latent loss，见下图：</p><p><img src="14_8.png" alt></p><p>是两个latent vector之差的二范数的平方</p><p>encoder：stacking 0N-GCN layers，followed by a max pooling</p><p>decoder：3D CNN</p><p>做成encoder-decoder的结构，其中后面voxel的形式用于监督学习，目标仅仅是得到为了encoder</p></li><li><p>Optimization Details</p><p>还有an edge length minimizing regularizer、Laplacian-maintaining regularizer，最后的Loss是上述4者的加权和。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 3d </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 3d </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>英语语法基础</title>
      <link href="/english/englishgrammar/"/>
      <url>/english/englishgrammar/</url>
      
        <content type="html"><![CDATA[<h1 id="英语语法"><a href="#英语语法" class="headerlink" title="英语语法"></a>英语语法</h1><p>本文课程：<a href="https://www.bilibili.com/video/av8200354" target="_blank" rel="noopener">【英语学习】重头学英语 零基础语法入门50课 教程自学教学 持续更新</a></p><p>更多学习资源：</p><ul><li><a href="https://www.rong-chang.com/" target="_blank" rel="noopener">ESL: English as a Second Language</a></li><li><a href="https://www.grammarly.com" target="_blank" rel="noopener">Grammarly: Free Writing Assistant</a></li><li><a href="https://www.zhihu.com/question/28157101" target="_blank" rel="noopener">其他语法纠错工具</a></li></ul><h2 id="五种基本句式"><a href="#五种基本句式" class="headerlink" title="五种基本句式"></a>五种基本句式</h2><ol><li><p>主+谓</p><p><code>The universe remains.</code></p></li><li><p>主+系+表</p><p><code>The food is delicious.</code></p></li><li><p>主+谓+宾</p><p><code>He took his bag and left.</code></p></li><li><p>主+谓+间宾+直宾</p><p><code>His father bought her a dictionary.</code></p></li><li><p>主+谓+宾+宾补</p><p><code>We made him our monitor.</code></p></li></ol><h2 id="Be动词"><a href="#Be动词" class="headerlink" title="Be动词"></a>Be动词</h2><ol><li><p>Be动词的形式（8种）</p><ul><li><code>be</code>：原型</li><li><code>is, am, are</code>：一般现在时</li><li><code>was, were</code>：过去时</li><li><code>being</code>：现在分词</li><li><code>been</code>：过去分词</li></ul></li><li><p>Be动词的用法</p><ul><li><code>~ is a teacher</code>：接名词</li><li><code>~ are colorful</code>：接形容词</li><li><code>~ was in the kitchen</code>：接地点副词</li></ul></li><li><p>Be动词的否定</p><ul><li>在<code>am, is, are, was, were</code>后加<code>not</code></li><li>缩写（+<code>n&#39;t</code>）<code>am not, isn&#39;t, aren&#39;t, wasn&#39;t, weren&#39;t</code></li></ul></li><li><p>Be动词的问句和回答（仅用于得到Yes/No的回答）</p><ul><li>问句例句（将Be动词放到句首即可）：<ul><li>他是老师吗？→ <code>Is he a teacher?</code></li><li>你是老师吗？→ <code>Are you a teacher?</code></li><li>他们之前是老师吗？→ <code>Were they teachers?</code></li><li>这个是这样子的吗？→ <code>Is this like this?</code></li><li>这个是这样子做的吗？→ <code>Is this done like this?</code></li><li>我正在看电视吗？→ <code>Am I watching TV?</code></li></ul></li><li>回答例句：<ul><li><code>Yes, He is.</code></li><li><code>No, he isn&#39;t.</code></li></ul></li><li>练习：<ul><li>他们昨天在教室吗？→ <code>Were they in the classroom yesterday?</code></li><li>他上学经常迟到吗？→ <code>Is he often late for school?</code></li></ul></li></ul></li></ol><h2 id="代词"><a href="#代词" class="headerlink" title="代词"></a>代词</h2><ol><li><p>主格代词（作主语）</p><ul><li><code>I, you, he/she/it, we, they</code></li></ul></li><li><p>宾格代词（作宾语）</p><ul><li><code>me, you, him/her/it, us, them</code></li></ul></li><li><p>形容词性物主代词（表示“xx的”）</p><ul><li>单数：<code>my, your, his/her/its, one&#39;s</code></li><li>复数：<code>our, your, their</code></li><li>例子：<ul><li><code>my book</code></li><li><code>our motherland</code></li></ul></li></ul></li><li><p>名词性物主代词（表示“xx的东西”）</p><ul><li>单数：<code>mine, yours, his/hers/its, one&#39;s</code></li><li>复数：<code>ours, yours, theirs</code></li><li>例子：<ul><li><code>The book is ours</code></li><li><code>The apple is hers</code></li></ul></li></ul></li><li><p>反身代词（自己）</p><ul><li>单数：<code>myself, yourself, himself/herself/itself</code></li><li>复数：<code>yourselves, ourselves, themselves</code></li><li>例子：<ul><li>作宾语：<ul><li><code>Please help yourself to some fish</code></li><li><code>Take good care of yourself</code></li></ul></li><li>作同位语：<code>The thing itself is not important</code></li></ul></li></ul></li><li><p>指示代词</p><ul><li>用来替代前面出现过的</li><li><code>this/these, that/those</code></li><li>例句：<ul><li><code>This is his book</code></li><li><code>Those apples were his. / Those were his.</code></li></ul></li></ul></li><li><p>不定代词</p><ul><li>指代不确定的</li><li><code>one/the other/some/any/something/nothing</code></li><li>例句：<ul><li><code>No one knows where he is</code></li><li><code>Some of the boys want to go to Shanghai, but the others want to go to Xi&#39;an</code></li><li><code>Each of the students has got a book</code></li><li><code>I know nothing about this person</code></li><li><code>Is there anyone here who can help me?</code></li></ul></li></ul></li></ol><h2 id="实意动词"><a href="#实意动词" class="headerlink" title="实意动词"></a>实意动词</h2><p>即具有实际意义的动词（Be动词没有实际意义）。有时态变化、单复数变化</p><ol><li><p>时态</p><ul><li>一般现在时</li><li>一般过去时</li></ul></li><li><p>举例</p><ul><li><code>come, read, go, watch...</code></li></ul></li><li><p>例句</p><ul><li><code>He comes from Shenyang</code></li><li><code>She is reading story book</code></li><li><code>We have watched the game for three times</code></li></ul></li><li><p>否定</p><ul><li>在助动词<code>do, does, did</code>后面加<code>not</code>（其缩写分别为<code>don&#39;t, doesn&#39;t, didn&#39;t</code>）</li><li>使用动词原形</li><li>例句：<ul><li><code>I don&#39;t go to school by bus</code></li><li><code>She doesn&#39;t watch TV everyday</code></li><li><code>They didn&#39;t swim last night</code></li><li><code>Tom didn&#39;t have breakfast yesterday.</code></li></ul></li></ul></li><li><p>问句（一般用于得到Yes/No的回答）</p><ul><li>将助动词提前到句首，动词用动词原形（可以理解为时态转移）</li><li>例句：<ul><li><code>He often plays golf</code> → <code>Does he often play golf?</code> → <code>Yes, he does/No, he doesn&#39;t</code></li><li><code>They go to school by bus</code> → <code>Do they go to school by bus?</code> → <code>Yes, they do/No, they don&#39;t</code></li><li><code>Sam had breakfast yesterday</code> → <code>Did Sam have breakfast yesterdaty?</code> → <code>Yes, he did/No, he didn&#39;t</code></li></ul></li><li>练习：<ul><li>他每天都学英语吗？→ <code>Does he learn English everyday?</code></li><li>他经常这样做吗？→ <code>Does he often do this?</code></li><li>你觉得这样对吗？→ <code>Do you think that is right?</code></li><li>你认为有什么运气因素可以帮助你成功吗？→ <code>Do you think there is any element of luck that can help you succeed?</code></li><li>你们中间有没有人知道他去了哪儿？→ <code>Does any of you know where he has gone?</code></li></ul></li></ul></li></ol><h2 id="疑问词提问"><a href="#疑问词提问" class="headerlink" title="疑问词提问"></a>疑问词提问</h2><ol><li><p>疑问词</p><ul><li><code>when, where, who, what, how</code></li><li><code>how long, how far, how often, why</code></li></ul></li><li><p>提问例句（回答的内容更宽泛）</p><ol><li><code>He bought three books yesterday</code><ul><li>提问：<ul><li>Who：<code>Who bought three books yesterday?</code>（对主语提问，则语序不变，因为没有助动词所以可以理解为没有时态转移）</li><li>What：<code>What did he buy yesterday?</code>（对非主语进行提问时，在疑问词的后面用助动词/be动词，动词恢复成原形（因为时态转移到助动词上了））</li><li>When：<code>When did he buy three books?</code>（对非主语进行提问时，在疑问词的后面用助动词/be动词，动词恢复成原形（因为时态转移到助动词上了））</li></ul></li></ul></li><li><code>They have been in China for three years</code><ul><li>提问：<ul><li>How long：<code>How long have they been in China?</code>（此处have是助动词）</li></ul></li></ul></li></ol><ul><li>练习：<ul><li>谁想坐飞机去上海？→ <code>Who wanted to go to Shanghai by air?</code></li><li>他们想坐飞机去哪里？→ <code>Where did they want to go by air?</code></li><li>他们是怎么想去上海的？→ <code>How did they want to go to Shanghai?</code></li><li>从北京到西安多远？→ <code>How far is it from Beijing to Xi&#39;an?</code></li><li>他们多久来看望你一次？→ <code>How often do they come to visit you?</code></li><li>她怎么过来得这么晚？→ <code>Why did she come so late?</code></li><li>他为什么迟到了？→ <code>Why is he late?</code></li><li>是谁杀了他？→ <code>Who killed him?</code></li><li>他杀了谁？→ <code>Who did he kill?</code></li><li>你是怎么做的？→ <code>How did you do it?</code></li><li>她正在读什么书？→ <code>What book is she reading?</code></li><li>那么这个部分怎么做呢？→ <code>So how do you do this part?</code></li><li><code>How do you do?</code> ← 你好 </li><li>他们学了多长时间的英语？→ <code>How long have they learned English?</code></li><li>你多长时间看一次电影？→ <code>How often do you watch movies?</code></li><li>你的家离学校多远？→ <code>How far is it from your house to your school?</code>（陈述句为：<code>it is xxx kilometers from my house to my school</code>）</li><li>在过去的半年里他在实验室做了些什么？→ <code>What has he done in the lab in the past six months?</code></li><li>你和谁去的？→ <code>Who did you go with?</code></li><li>你觉得呢？→ <code>What do you think?</code></li><li>什么幸运因素能帮助你成功？→ <code>What luck factor can help you succeed?</code></li><li>你觉得是谁造就了如今的我们？→ <code>Who do you think made us who we are?</code></li><li>你认为在你的朋友中谁是最好的人？→ <code>Who do you think is the best among your friends?</code></li></ul></li></ul></li></ol><h2 id="名词"><a href="#名词" class="headerlink" title="名词"></a>名词</h2><ol><li><p>可数名词、不可数名词</p><ul><li>可数名词：<code>apple(+s), pencil(+s), tomato(+es), ...</code></li><li>不可数名词：<code>salt, coffee, water, history, love, ...</code>（没有复数形式）</li><li>可数名词变复数的规则：<ul><li><strong>一般</strong>末尾<code>+s</code>（如<code>friends</code>）</li><li><strong>以<code>s/z/x/ch/sh</code>结尾</strong>的，末尾<code>+es</code>（如<code>buses</code>）</li><li><strong>以<code>辅音字母 + y</code>结尾</strong>的（除<code>a/e/i/o/u</code>之外的字母叫辅音字母），将<code>y</code>改成<code>i</code>再<code>+es</code>（如<code>candies</code>）</li><li><strong>以<code>o</code>结尾</strong>的：是英语词汇（非其他语言引进的词汇）则<code>+es</code>；不是缩写则<code>+es</code>，缩写则<code>+s</code>（如<code>tomatoes, hippos</code>）</li></ul></li></ul></li><li><p>a/an/量词</p><ul><li>可数名词前加<code>a/an/量词</code>；不可数名词前不加<code>a/an</code>，但可以加量词</li><li><code>a</code>：辅音开头</li><li><code>an</code>：元音开头</li><li><code>量词</code>：如<code>a box of, a cup of</code></li><li>例子：<ul><li><code>an apple, apples, a box of apples</code></li><li><code>a tomato, tomatoes, a bag of tomatoes</code></li><li><code>a cup of coffee</code></li></ul></li></ul></li><li><p>不定量表达法</p><ul><li><code>some</code>：一些（后面加可数名词复数或者是不可数名词），主要用于肯定句，希望得到肯定回答时也可用于疑问句</li><li><code>any</code>：任意（后面加可数名词复数或者是不可数名词），主要用于否定句和疑问句</li><li><code>most</code>：大部分（后面接复数名词）</li><li><code>every</code>：每一个（后面通常接单数名词）</li><li><code>all</code>：所有（后面接可数名词复数，或不可数名词）</li><li><code>both</code>：两者都（复数），可作形容词/代词/副词</li><li><code>either</code>：两者之一（单数）</li><li><code>neither</code>：两者都不（单数）</li><li><code>many</code>：修饰可数</li><li><code>much</code>：修饰不可数</li><li><code>a lot of / lots of / plenty of</code>：均可修饰可数/不可数</li><li><code>a few</code>：几个（即还有一些，肯定含义），接可数名词的复数形式</li><li><code>few</code>：没几个（即几乎没有，否定含义），接可数名词的复数形式</li><li><code>a little</code>：一点儿（即还有一些，肯定含义），接不可数名词</li><li><code>little</code>：没多少（即几乎没有，否定含义），接不可数名词（带<code>a</code>的表示还有一些，不带<code>a</code>的表示快没了；<code>few</code>可数，<code>little</code>不可数）</li><li><code>none / no one</code>：区别：<code>none</code>可以接<code>of</code>短语，动词可单数可复数；而<code>no one</code>不可接<code>of</code>短语，动词只能用单数。</li><li>例句：<ul><li>我整个上午都在等着一些信，但没有一封是给我的 → <code>I&#39;d been expecting some letters the whole morning, but there weren&#39;t any for me.</code></li><li>这里大多数人都是来自中国的 → <code>Most people here are from China</code></li><li>大多老师都很想在这里工作 → <code>Most teachers want to work here</code></li><li>每个人都很喜欢那部影片 → <code>Every one likes the film</code>（<code>every one</code>还可以指物，后面可以跟介词<code>of</code>，如<code>Every one of us is getting ready for the exam</code>；而<code>everyone</code>则只能指人，不可以加<code>of</code>）</li><li>所有的车都停在停车场 → <code>All the cars are parked in the parting lot</code></li><li>所有咖啡都准时供应 → <code>All the coffee is served on time</code></li><li>两只眼睛都被严重烧伤 → <code>Both of eyes were severely burned</code></li><li>街道的两边都站满了人 → <code>Both sides of the street were full of people.</code></li><li>街道两边都有树 → <code>There are trees on both sides of the street.</code>（<code>both</code>强调两边都是，为复数形式）</li><li>街道两边都有树 → <code>There are trees on either side of the street</code>（<code>either</code>强调两边中的一边，是单数形式）</li><li>不管是A还是B方式，你都会很性感 → <code>Either way, you’ll look sexy.</code></li><li>你拿这个或那个都可以 → <code>You can have either this one or that one</code></li><li>要么他对，要么你对 → <code>Either he or you are right</code>（就近原则）</li><li>两个答案都不对 → <code>Neither answer is correct</code></li><li>你和他的答案都不对 → <code>Neither you nor he has the right answer.</code>（就近原则）</li><li>她既不喜欢黄油也不喜欢乳酪 → <code>She likes neither butter nor cheese</code></li><li><code>many books, much water, a lot of water, a lot of books</code></li><li>没几本书放在盒子里 → <code>Few books are put into the box</code></li><li>瓶子里面有一点水 → <code>There is a little water in the bottle</code></li><li>我们当中没有一个人到达 → <code>None of us have/has arrived</code>（因为<code>None of us</code>可以看作单数也可以看作复数）</li><li>没有人知道答案 → <code>No one knows the answer</code></li></ul></li></ul></li></ol><h2 id="形容词"><a href="#形容词" class="headerlink" title="形容词"></a>形容词</h2><ol><li><p>修饰作用</p><ul><li>例子：<ul><li><code>the beautiful girl</code></li><li><code>the girl is beautiful</code></li></ul></li></ul></li><li><p>表示一类</p><ul><li>例子（<code>The + adj. == 复数名词</code>）：<ul><li><code>The old need more care than the young</code></li><li><code>The rich sometimes complain their empty life</code></li></ul></li></ul></li></ol><h2 id="副词"><a href="#副词" class="headerlink" title="副词"></a>副词</h2><ol><li><p>功能</p><ul><li>副词可以修饰<code>v./adj./adv./其他结构</code></li><li>举例：<ul><li>修饰动词：<code>He runs fast</code></li><li>修饰形容词：<code>She is very beautiful</code></li><li>修饰副词：<code>They work very hard</code></li></ul></li></ul></li><li><p>副词的位置</p><ul><li>放在助动词之后（一般放在第一个助动词之后，如<code>have already been</code>）</li><li>形容词之前（如<code>very beautiful</code>）</li><li>频度副词（如<code>always, usually, often, sometimes, never</code>等）的位置通常是<code>(助动词/be动词) + 频度副词 + 实意动词</code>（如<code>He has always run away</code>）</li><li>举例：<ul><li><code>He speak very fast</code></li><li><code>They have already left</code>（<code>have</code>是助动词）</li><li><code>They have already been repaired</code>（<code>have</code>和<code>been</code>都是助动词）</li><li><code>They always come early</code></li><li>这是一篇写得非常好的文章 → <code>It is an extremely well written essay</code></li><li>她的成绩在过去3年里几乎总是年级第一 → <code>Her grades have almost always been the first in the grade in the past three years</code></li></ul></li></ul></li></ol><h2 id="There-be-Here-be句型"><a href="#There-be-Here-be句型" class="headerlink" title="There be / Here be句型"></a>There be / Here be句型</h2><ol><li><p>There be（倒装句，表示存在）</p><ul><li>Be动词要根据后面的名词作单复数的变化</li><li>例句：<ul><li>书架上有一些书 → <code>There are some books on the bookshelf</code>（原句：<code>Some books are on the bookshelf</code>）</li><li>瓶子里有一些水 → <code>There is a lot of water in the bottle</code></li><li>书桌上有什么？→ <code>What is there on the desk?</code>（等效于：<code>What is on the desk?</code>）</li></ul></li></ul></li><li><p>Here be（倒装句，表示“这”）</p><ul><li>Be动词要根据后面的名词作单复数的变化</li><li>例句：<ul><li>这是巴士站 → <code>Here is the bus stop</code></li><li><code>Here comes the bus</code>（谓语还可以是<code>come, go</code>等表示移动或动态的不及物动词）</li></ul></li></ul></li></ol><h2 id="一般现在时-现在进行时"><a href="#一般现在时-现在进行时" class="headerlink" title="一般现在时 / 现在进行时"></a>一般现在时 / 现在进行时</h2><ol><li><p>一般现在时（<code>do/does</code>）</p><ul><li>表示通常性、规律性、习惯性的状态或动作</li><li>动词<strong>第三人称单数</strong>变化规则：<ul><li>一般直接<code>+s</code>（如<code>plays</code>）</li><li>以<code>s/x/ch/o</code>结尾的，则<code>+es</code>（如<code>guesses</code>）</li><li>以<code>辅音字母 + y</code>结尾的，变<code>y</code>为<code>i</code>再<code>+es</code>（如<code>studies</code>）</li></ul></li><li>例句：<ul><li><code>They often get up at 7:00</code></li><li><code>He often gets up at 7:00</code></li></ul></li></ul></li><li><p>现在进行时（<code>is/am/are + doing</code>）</p><ul><li>表示正在进行或发生的动作</li><li>动词的<strong>现在分词</strong>的变化规则：<ul><li>一般直接<code>+ing</code>（如<code>working</code>）</li><li>若以不发音的<code>e</code>结尾，则<code>-e</code>并<code>+ing</code>（如<code>taking</code>）</li><li>重读闭音节（以辅音音素结尾的而且是重读音节的音节）的动词，要双写词尾字母再<code>+ing</code>（如<code>cutting</code>）</li><li>以<code>ie</code>结尾的，通常变<code>ie</code>为<code>y</code>再<code>+ing</code>（如<code>lying</code>）</li></ul></li><li>例句：<ul><li><code>They are watching TV</code></li></ul></li></ul></li></ol><h2 id="一般过去时-过去进行时"><a href="#一般过去时-过去进行时" class="headerlink" title="一般过去时 / 过去进行时"></a>一般过去时 / 过去进行时</h2><ol><li><p>一般过去时（<code>did</code>）</p><ul><li>表示过去某个时间里发生的动作或状态，过去习惯性、经常性的行为</li><li>动词<strong>一般过去时</strong>的规则变化的规律：<ul><li>一般<code>+ed</code>（如<code>played</code>）</li><li>在<code>e</code>结尾的词后面只<code>+d</code>（如<code>liked</code>）</li><li>在<code>辅音字母 + y</code>结尾的词，改<code>y</code>为<code>i</code>再<code>+ed</code>（如<code>supplied</code>）</li><li>在以重读闭音节结尾且末尾只有一个辅音字母的动词后，双写最后一个辅音字母，再<code>+ed</code>（如<code>planned</code>）</li></ul></li><li>例句：<ul><li><code>He worked very hard last night</code></li><li><code>They came here by car</code></li></ul></li></ul></li><li><p>过去进行时（<code>was/were + doing</code>）</p><ul><li>表示过去某一时刻或时间段内进行或发生的动作</li><li>例句：<ul><li><code>They were waiting for you</code></li><li><code>He was talking with his friends just now</code></li><li><code>Sam was watching TV at 7:00 last night</code></li></ul></li></ul></li></ol><h2 id="将来时"><a href="#将来时" class="headerlink" title="将来时"></a>将来时</h2><ol><li>一般将来时（<code>will / shall / be going to + do</code> 或 <code>be doing</code>表示位置转移的动词）<ul><li>表示将来某一时刻的动作或状态，或将来某一时间内经常发生的动作或状态。常常与表示将来的时间状语连用</li><li>例句：<ul><li><code>They will go to Shanghai by ship next month</code></li><li><code>We shall leave for Shanghai next month</code>（<code>shall</code>一般用于<code>I/We</code>）</li><li><code>They are going to play football this afternoon</code>（<code>be going to</code>意为打算、就要）</li><li>他们将要离开去日本 → <code>They are leaving for Japan</code>（表示位置转移的动词如<code>go, come, leave, start, arrive</code>，可以用现在进行时表示将来时）</li></ul></li></ul></li></ol><h2 id="完成时"><a href="#完成时" class="headerlink" title="完成时"></a>完成时</h2><ol><li><p>现在完成时（<code>have done</code>）</p><ul><li>表示过去发生，已经完成，对现在造成影响或后果，动作可能还会持续（翻译成“已经”）</li><li>可以使用的时间状语：<code>already, yet, for 3 hours</code></li><li>例句：<ul><li><code>They have already arrived in Shanghai</code></li><li><code>She hasn&#39;t finished the homework yet</code></li></ul></li></ul></li><li><p>过去完成时（<code>had done</code>）</p><ul><li>表示动作发生在过去之前，即过去的过去，已经完成，对过去造成了一定的影响和后果（翻译成“已经”）</li><li>例句：<ul><li><code>They had arrived in Shanghai</code></li><li><code>She had played soccer for 3 hours</code></li></ul></li></ul></li></ol><h2 id="动词的用法"><a href="#动词的用法" class="headerlink" title="动词的用法"></a>动词的用法</h2><ol><li><p>时态和状态</p><ul><li>时态：现在、过去、将来、（过去将来）</li><li>状态：一般、进行、完成、（完成进行）</li><li>两两组合可以产生9种（16种）时态</li><li>例句：<ul><li><code>He goes to school every day</code></li><li><code>He went to hosptial last night</code></li></ul></li></ul></li><li><p>动词形态的变化</p><ul><li>原形：<code>go</code></li><li>第三人称单数：<code>goes</code></li><li>现在分词：<code>going</code></li><li>过去式：<code>went</code></li><li>过去分词：<code>gone</code></li></ul></li><li><p>动词的分类</p><ul><li>实意动词</li><li>系动词</li><li>助动词</li><li>情态动词</li></ul></li></ol><h2 id="情态动词"><a href="#情态动词" class="headerlink" title="情态动词"></a>情态动词</h2><ol><li><p>情态动词</p><ul><li>带有情态含义的动词（后面要接动词原形）</li><li><code>can</code>是现在的能力，<code>could</code>是过去的能力。可以用<code>be able to</code>替代</li><li><code>may/might</code>表示可能性、请求、允许，<code>may</code>可能性大，<code>might</code>更委婉</li><li><code>must</code>主观多一点，<code>have to</code>客观多一点</li><li><code>should</code>强调主观看法，<code>ought to</code>（应该）更强调客观要求</li><li><code>need</code>（需要）既可做情态动词，也可以做实意动词</li><li><code>had better</code>（最好做某事），虽然<code>had</code>是过去式，但是不表征过去，<code>better</code>后面接动词原形；否定形式直接在后面加上<code>not</code></li><li><code>would rather</code>（宁愿、宁可、最好、还是…为好），语感比<code>had better</code>要轻；否定形式直接在后面加上<code>not</code></li><li><code>used to</code>（过去经常，过去习惯性的状态），可指过去的状态或情况（<code>would</code>则不能），现在已经结束了</li><li><code>would</code>（过去经常会），表示反复发生的动作（如果没有反复性，就用<code>used to</code>），现在有可能再发生</li><li>例句：<ul><li><code>Can/Could I stay here?</code></li><li><code>He can/could come tomorrow</code></li><li><code>May/Might I join you? No, you can&#39;t. Yes, please. No, you mustn&#39;t</code></li><li><code>He may/might come here by bus</code></li><li><code>You must get up early</code></li><li><code>You don&#39;t have to go</code></li><li><code>You should / ought to do the job right now</code></li><li><code>Should they stay here now?</code></li><li><code>He need come here early</code>（<code>need</code>是情态动词）</li><li><code>He needn&#39;t come here early</code>（<code>needn&#39;t == need not</code>是情态动词的否定）</li><li>他需要早点来这里吗？→ <code>Need he come here early? Yes, he need. No, he needn&#39;t</code>（<code>need</code>做情态动词）</li><li>他需要早点来这里 → <code>He needs to come here early</code>（<code>need</code>用作实意动词）</li><li>他需要早点来这里吗？→ <code>Does he need to come here early?</code>（<code>need</code>用作实意动词）</li><li>明天我必须早点来这里吗？→ <code>Must I come here early tomorrow? No, you needn&#39;t. No, you don&#39;t have to.</code>（用<code>needn&#39;t / don&#39;t have to</code>来否定地回答<code>must / have to</code>的提问）</li><li>你最好多吃一点 → <code>You had better eat more</code></li><li>你最好现在完成他 → <code>You&#39;d better finish it right now</code>（<code>&#39;d better == had better</code>）</li><li><code>You would rather deal with it now</code></li><li>他最好别再吃了 → <code>He had better not eat more</code></li><li>你最好现在别处理那件事 → <code>You would rather not deal with it now</code></li><li><code>The noval used to be popular</code></li><li>他过去每一周都练习英语 → <code>He would practice English every week</code>（反复发生）</li><li>我之前住在北京 → <code>I used to live in Beijing</code>（不具备反复性）</li><li>人们曾经相信地球是平的 → <code>People used to believe that the earth is flat</code>（只能是过去发生，现在不再发生）</li><li>他一有空就去公园 → <code>He would go to the park as soon as he was free</code>（现在还可能发生）</li></ul></li></ul></li><li><p>情态动词的否定</p><ul><li>一般加<code>not</code>（有些可以缩写为<code>n&#39;t</code>）</li><li>例句：<ul><li>他可能不知道她 → <code>He may not know her</code></li><li><code>He mustn&#39;t go there</code></li><li><code>He doesn&#39;t have to go there</code></li></ul></li></ul></li><li><p>情态动词的疑问</p><ul><li>一般是情态动词放在句子的最前面</li><li>例句：<ul><li><code>Can he sing a song? Yes, he can. No, he can&#39;t</code></li><li><code>Must he go there? Yes, he must. No, he needn&#39;t</code></li><li><code>Does he have to go there? Yes, he does. No, he doesn&#39;t</code>（<code>have to</code>则是助动词放在最前面）</li></ul></li></ul></li><li><p>情态动词 + 完成时</p><ul><li>表达过去的事实、推测的含义（可能已经xxx了）</li><li><code>should have done</code>：本应该</li><li><code>needn&#39;t have done</code>：本不需要</li><li><code>must have done</code>：准是已经</li><li><code>can&#39;t have done</code>：不可能已经</li><li>例句：<ul><li>他可能已经到了 → <code>He can/could have arrived</code></li><li>他可能已经到了 → <code>He may/might have arrived</code></li><li>他准是已经到了 → <code>He must have arrived</code></li><li>他本应该到了 → <code>He should have arrived</code>（但实际没到）</li><li>你本来不需要这样做的 → <code>You needn&#39;t have done so</code>（但实际上做了）</li><li>你一定已经到了 → <code>You must have arrived</code></li><li>他不可能已经到了 → <code>He can&#39;t have arrived</code></li></ul></li></ul></li></ol><h2 id="被动语态"><a href="#被动语态" class="headerlink" title="被动语态"></a>被动语态</h2><ol><li><p>被动语态（<code>be done (by sb.)</code>）</p><ul><li>主语是动作的承受者</li><li>例句：<ul><li><code>He is taken to America by his mother</code></li><li><code>The information is needed by us</code></li><li><code>The book was being read by him</code>（过去进行时<code>was doing</code>的被动语态）</li><li><code>The book was read by him</code>（一般过去时<code>was</code>的被动语态）</li><li><code>The computer has been used by him</code></li><li><code>The room will be cleaned</code></li><li>这台电脑可能是他用过的 → <code>The computer could have been used by him</code></li></ul></li></ul></li><li><p>被动语态 + 情态动词</p><ul><li>一般是<code>情态动词 be done</code></li><li><code>need doing</code>：需要被做</li><li><code>need to be done</code>：需要被做</li><li>例句：<ul><li><code>The food could be taken away</code></li><li><code>The food should be taken away</code></li><li><code>The food had better be taken away</code></li><li><code>Books used to be returned in two days</code></li><li><code>The food needs taking away</code></li></ul></li></ul></li><li><p>行为者被省略</p><ul><li><code>by sb.</code>可以省略</li></ul></li><li><p>被动语态的一般疑问句</p><ul><li>构成：<code>助动词 + 主语 +（其他助动词）+ 动词的过去分词</code></li><li>例句：<ul><li><code>Is the information needed by him? Yes, it is. No, it isn&#39;t.</code></li><li><code>Has the computer been used by her? Yes, it has. No, it hasn&#39;t.</code></li><li><code>Will the room be cleaned? Yes, it will. No, it won&#39;t.</code></li></ul></li></ul></li><li><p>被动语态的特殊疑问句</p><ul><li>构成：<code>疑问词 + 助动词 + 主语 +（其他助动词）+ 动词的过去分词</code></li><li>例句：<ul><li><code>What is needed by him?</code>（原句：<code>The imformation is needed by him</code>）</li><li><code>Where is the girl taken?</code>（原句：<code>The girl is taken to Shanghai</code>）</li><li><code>How many times has the book been read?</code>（原句：<code>The book has been read three times</code>）</li></ul></li></ul></li></ol><h2 id="非谓语动词"><a href="#非谓语动词" class="headerlink" title="非谓语动词"></a>非谓语动词</h2><ol><li><p>动词不定式 <code>to do</code></p><ul><li>可以做主语、宾语、宾补、定语、表语、状语</li><li>例句：<ul><li>骑自行车到那儿要花费一个小时 → <code>To get there by bike will take us an hour</code>（作主语。因为未完成所以用<code>To do</code>）</li><li>司机没能及时看到车 → <code>The driver failed to see the car in time</code>（作宾语）</li><li>我们相信他是有罪的 → <code>We believe him to be guilty</code>（作宾补）</li><li>下一班到达的火车是从首尔开来的 → <code>The next train to arrive is from Seoul</code>（作定语：修饰名词）</li><li>我的建议是推迟会议 → <code>My suggestion is to put off the meeting</code>（作表语：放在Be动词后面的结构）</li><li>我来这儿只为了和你说再见 → <code>I come here only to say goodbye to you</code>（作状语，此处是目的状语）</li><li>你的任务是迅速清洁窗户 → <code>Your task is to clean the Windows quickly</code></li></ul></li></ul></li><li><p>动名词 <code>doing</code></p><ul><li>具有动词的特征以及变化形式，但在句子中的用法和功能类似于名词</li><li>也可以被副词修饰或支配宾语</li><li>可以用作：主语、宾语、表语、定语</li><li>例句：<ul><li>阅读是门艺术 → <code>Reading is an art</code>（作主语）</li><li>他们继续走，从未停止谈话 → <code>They went on walking and never stopped talking</code>（作宾语）</li><li>你的任务是快速清洁窗户 → <code>Your task is quickly cleaning the windows</code>（作表语，并被副词修饰）</li><li>这是一间阅读室 → <code>This is a reading room</code>（作定语）</li></ul></li></ul></li><li><p>形式主语</p><ul><li><code>to do</code>不定式或动名词<code>doing</code>可以在主语的位置上，但一般用<code>it</code>替代它做形式主语（避免头重脚轻）</li><li>例句：<ul><li>很荣幸被邀请 → <code>It&#39;s a great honor to be invited</code>（原句：<code>To be invited is a great honor</code>）</li><li>覆水难收 → <code>It&#39;s no use crying over split milk</code>（原句：<code>Crying over split milk is no use</code>，<code>over</code>翻译作“因为”/“对”）</li></ul></li></ul></li><li><p>形式宾语</p><ul><li>在宾语的位置上，用<code>it</code>替代它作形式宾语</li><li>例句：<ul><li>我们认为学习英语很重要 → <code>We think it important to learn English</code>（<code>it</code>实际上是<code>to learn English</code>）</li><li>我觉得在公园散步很愉快 → <code>I found it pleasant walking in the park</code>（<code>it</code>实际上是<code>walking in the park</code>）</li></ul></li></ul></li><li><p>否定</p><ul><li>在不定式或动名词前面加<code>not</code>就可以了（<code>not to do, not doing</code>）</li><li>例句：<ul><li>他假装没看见她 → <code>He pretended not to see her</code></li><li>他后悔没加入他们 → <code>He regrets not joining them</code></li></ul></li></ul></li><li><p><code>to do</code>不定式表示目的（目的状语）</p><ul><li><code>in order to do</code>：既能放在句首，也能放在句尾（否定形式为<code>in order not to do</code>，翻译作“以免，免得”）</li><li><code>so as to do</code>：只能放在句尾（否定形式为<code>so as not to do</code>，翻译作“以免，免得”）</li><li><code>to do</code>：也能表示目的，但是不常用</li><li>例句：<ul><li><code>I&#39;ve written it down in order to remember it</code></li><li><code>He shouted and waved so as to be noticed</code></li><li><code>I&#39;ve written it down in order not to forget it</code></li><li><code>He said nothing so as not to be noticed</code></li></ul></li></ul></li><li><p>常见的不定式和动名词句型</p><ul><li><code>too ... to ...</code>：太。。以致不能。。<ul><li>例句：<code>The room is too small to live</code></li></ul></li><li><code>enough n. to do / adj. enough to do</code>：足够的。。来。。<ul><li>例句：<ul><li><code>There is enough food to eat</code></li><li><code>The box is big enough to contain six apples</code></li></ul></li></ul></li><li><code>on doing</code>：一。。就。。<ul><li>例句：<code>on seeing the snake, the girl was very frightened</code></li></ul></li><li><code>There is no hope of doing</code>：没希望。。<ul><li>例句：<code>There is no hope of seeing him</code></li></ul></li><li><code>feel like doing</code>：想要<ul><li>例句：<code>I feel like eating ice cream now</code></li></ul></li><li><code>have a hard time doing</code>：做。。艰难<ul><li>例句：<code>They have a hard time solving the problem themselves</code></li></ul></li></ul></li><li><p>分词句</p><ul><li>包含现在分词（表达进行）或过去分词（表达完成和被动）的分句</li><li>例句：<ul><li><code>The students went out of the classroom, laughing and talking</code>（<code>laughing and talking</code>是现在分词，作伴随状语）</li><li><code>Accompanied by his friend, he went to the railway station</code>(<code>Accompanied</code>是过去分词，作伴随状语)</li></ul></li></ul></li></ol><h2 id="原形不定式"><a href="#原形不定式" class="headerlink" title="原形不定式"></a>原形不定式</h2><ol><li><p>使役动词</p><ul><li>使役动词表示“使得/让。。”</li><li><code>have/make/let 宾语 do</code>，没有不定式符号<code>to</code>，直接使用动词原形即可</li><li><code>get 宾语 to do</code></li><li><code>help 宾语 (to) do</code></li><li><code>have/make/let/get/help 宾语 done</code>：让。。被。。（被动）</li><li>例句：<ul><li><code>He made me laugh</code></li><li><code>I let him go</code></li><li><code>Please have him come here</code></li><li>我找不到人来把工作做好 → <code>I can&#39;t get anyone to do the work properly</code></li><li><code>I helped him (to) repair the car</code></li><li><code>I must get my hair cut</code>（此处的<code>cut</code>是过去分词）</li><li>他不能让自己被听见 → <code>He could&#39;t make himself heard</code></li><li><code>Can you get the work finished in time?</code></li></ul></li></ul></li><li><p>感官动词</p><ul><li>能够表达“感觉/感知”含义的动词</li><li>例如：<code>see, watch, observe, notice, hear, smell, taste, feel</code></li><li>用法：<ul><li><code>感官动词 + 宾语 + 动词原形（动词不定式的原形）/现在分词</code><ul><li>接动词原形时，表示动作的真实性</li><li>接现在分词时，表示动作的连续性、进行性</li></ul></li><li><code>感官动词 + 宾语 + 过去分词</code>（过去分词表达宾语的被动含义）</li></ul></li><li>例句：<ul><li><code>I saw him work in the garden yesterday</code>（强调观测的真实性）</li><li><code>I saw him working in the garden yesterday</code>（强调昨天他正在工作，一直工作）</li><li><code>John saw the man knocked down by the car</code></li><li><code>I heard Sam sent to the hosptial</code></li></ul></li></ul></li></ol><h2 id="假设"><a href="#假设" class="headerlink" title="假设"></a>假设</h2><ol><li><p>单纯的条件/假设</p><ul><li>用<code>if</code>引导条件状语从句，可以放在句首/句尾</li><li>从句用一般现在时，主句一般用一般将来时</li><li>例句：<ul><li><code>If you get up early, you will catch up with the train</code></li><li><code>She will be upset if you fail the exam</code></li><li><code>The cat scratches you if you pull her tail</code>（条件句谈论的是重复发生和预示要发生的情景和事件，则主从句大多用一般现在时态）</li></ul></li></ul></li><li><p>与现在事实相反的虚拟语气</p><ul><li><code>if</code>从句是一种虚拟的条件或假设，和现在的事实相反</li><li>从句：动词过去式（<code>be</code>动词只能用<code>were</code>不用<code>was</code>）</li><li>主句：<code>would/could/should/might + do</code></li><li>例句：<ul><li><code>If I were you, I would join them</code></li><li><code>She would come with you if you invited her</code>（事实上之前并没有邀请）</li></ul></li></ul></li><li><p>与过去事实相反的虚拟语气</p><ul><li><code>if</code>从句是一种虚拟的条件或假设，和过去的事实相反</li><li>从句：<code>had done</code>（<code>be</code>动词用<code>had been</code>）</li><li>主句：<code>would/could/should/might + have done</code></li><li>例句：<ul><li>如果过去我早一点到那的话，我就能够遇到她了 → <code>If I had got there earlier, I should have met her</code></li><li>如果他接受我的建议的话，他就不会犯下这样的错误了 → <code>If he had taken my advice, he would not have made such a mistake</code></li></ul></li></ul></li><li><p><code>wish</code> 希望，但愿</p><ul><li>译为“希望。。就好了”，是不可能实现的假设<ul><li>与现在事实相反的愿望：从句用一般过去时（<code>did</code>）<ul><li>我希望我和你一样高 → <code>I wish I were as tall as you</code>（实际上没有那么高）</li></ul></li><li>与过去事实相反的愿望：从句用过去完成时（<code>had done</code>）<ul><li>他希望他从未说过那番话 → <code>He wished he hadn&#39;t said that</code>（他说的那番话是过去说的）</li></ul></li><li>与将来事实相反的愿望：<code>would/should/could</code><ul><li>我希望明天下雨 → <code>I wish it would rain tomorrow</code>（但实际上明天并不会下雨）</li></ul></li></ul></li></ul></li><li><p><code>as if</code> 犹如，好似</p><ul><li>译为“看起来好像。。”</li><li>与现在事实相反：从句用一般过去式（<code>did</code>）<ul><li>你看起来好像并不在乎 → <code>You look as if you didn&#39;t care</code></li></ul></li><li>与过去事实相反：从句用过去完成时（<code>had done</code>）<ul><li>他谈及罗马就好像他去过那一样 → <code>He talks about the Rome as if he had been there before</code>（去罗马必定是之前去的，但实际上他之前没去）</li></ul></li><li>与将来事实相反：<code>would/should/could</code><ul><li>他张开嘴犹如要说话 → <code>He opened his mouth as if he would say something</code>（要说话必定是未来的，但他实际上不说话）</li></ul></li></ul></li></ol><h2 id="定语从句"><a href="#定语从句" class="headerlink" title="定语从句"></a>定语从句</h2><ol><li><p>定语从句中的关系代词</p><ul><li>在句子中作定语，修饰名词/代词。</li><li>被修饰的词叫“先行词”，由“关系词”引出定语从句。</li><li>常用的关系代词：<code>that, who, which, whom, whose</code></li><li>先行词是人的话，用<code>that/who/whom/whose</code>引导。<ul><li><code>whom</code>在从句中作宾语</li><li><code>who/that</code>在从句中既可以作主语又可以作宾语</li><li><code>whose</code>：谁谁的（人）</li></ul></li><li>先行词是动物/事物的话，用<code>which/that/whose</code>引导。<ul><li><code>which/that</code>在从句中可作主语和宾语，<strong>作宾语时可以省略</strong></li><li><code>whose</code>：xx的（物）</li></ul></li><li>例句：<ul><li><code>The girl whom/that I spoke to is my cousin</code></li><li><code>They are the people whom/that/who I saw yesterday</code></li><li><code>They are the people whose wallets were lost yesterday</code></li><li><code>He came back for the book (which/that) he had forgotten</code></li><li><code>He came back for the book which/that was on the desk</code></li><li><code>This is the chair whose legs were broken</code></li></ul></li></ul></li><li><p>定语从句中的关系副词</p><ul><li>常用的关系副词有：<code>when, where, why</code></li><li><code>why</code>用于修饰表示原因的名词，例如<code>reason</code></li><li><code>when</code>用于修饰表示时间的名词，例如<code>next week</code></li><li><code>where</code>用于修饰表示地点的名词，例如<code>the place</code></li><li>例句：<ul><li><code>We don&#39;t know the reason why he didn&#39;t show up</code></li><li><code>We&#39;ll put off the picnic until next week, when the weather may be better</code>（从句：<code>the weather may be better next week</code>）</li><li><code>We don&#39;t know the place where he lives</code>（从句：<code>he lives where</code>）</li></ul></li></ul></li></ol><h2 id="宾语从句"><a href="#宾语从句" class="headerlink" title="宾语从句"></a>宾语从句</h2><ol><li>宾语从句<ul><li>在句子中起宾语作用的从句</li><li>从属连词：<code>that, if, whether</code></li><li>连接代词：<code>who, whom, whose, what</code></li><li>连接副词：<code>when, where, why, how</code></li><li>例句：<ul><li><code>He told me (that) he would go to college the next year</code></li><li><code>He said (that) he was there yesterday</code>（从属连词，<strong>可省略</strong>，不做任何成分）</li><li>他不知道他是否在那 → <code>He doesn&#39;t know if/whether he was there</code>（从属连词）</li><li><code>Do you know who has won the game?</code>（连接代词，从句语序不变）</li><li><code>Do you know whom he likes?</code>（连接代词，从句语序不变）</li><li><code>Do you know whose book it is?</code>（连接代词，从句语序不变）</li><li><code>Do you know what he is looking at?</code>（连接代词，从句语序不变）</li><li><code>He want to know where the party is</code>（连接副词）</li><li><code>He want to know when the party is</code>（连接副词）</li><li><code>He want to know why they have a party</code>（连接副词）</li><li><code>He want to know how they come</code>（连接副词）</li></ul></li></ul></li></ol><h2 id="比较级最高级"><a href="#比较级最高级" class="headerlink" title="比较级最高级"></a>比较级最高级</h2><ol><li><p>形容词/副词的变化规则</p><ul><li>通常加<code>er/est</code>（如：<code>hard → harder/the hardest</code>）</li><li>词尾是不发音的单音节<code>e</code>时，加<code>r/st</code>（如<code>nice → nicer/the nicest</code>）</li><li>词尾是<code>辅音 + y</code>的双音节时，改<code>y</code>为<code>i</code>再加<code>er/est</code>（如<code>dry → drier/the driest</code>）</li><li>以辅音字母结尾的重读闭音节，双写最后一个字母，加<code>er/est</code>（如<code>hot → hotter/hottest</code>）</li><li>多音节和双音节，程度加强加<code>more/the most</code>，程度减弱加<code>less/the least</code>（如<code>the most interesting, less interesting</code>）</li><li>不规则变化（如<code>good → better/the best</code>）</li></ul></li><li><p>比较级</p><ul><li>例句：<ul><li><code>He is taller than his brother</code></li><li><code>The book is more expensive than the pen</code></li><li><code>Her English is better than his</code></li></ul></li></ul></li><li><p>最高级</p><ul><li>例句：<ul><li><code>He is the tallest in his class</code></li><li><code>The book is the most expensive of the three</code></li><li><code>Her English is the best among the three</code></li></ul></li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> english </category>
          
      </categories>
      
      
        <tags>
            
            <tag> english </tag>
            
            <tag> grammar </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>3D目标检测</title>
      <link href="/3d/3ddetection/"/>
      <url>/3d/3ddetection/</url>
      
        <content type="html"><![CDATA[<h1 id="3D目标检测"><a href="#3D目标检测" class="headerlink" title="3D目标检测"></a>3D目标检测</h1><p>介绍3D detection的各种模型框架方法等。</p><p>背景知识：</p><ul><li>RGB image == 2D；RGB-D image == 2.5D；point cloud == 3D</li><li>Amodal 3D Object Detection：即使部分被遮挡，也要能正确估计出尺寸，而不受遮挡的影响。</li><li>objectness：指代非背景的任何物体，不讨论特定的物体类别。</li><li>LiDAR：激光雷达（更精细）；radar：无线电波雷达（更远）</li></ul><h2 id="2D-Driven-3D-Object-Detection-in-RGB-D-Images"><a href="#2D-Driven-3D-Object-Detection-in-RGB-D-Images" class="headerlink" title="2D-Driven 3D Object Detection in RGB-D Images"></a>2D-Driven 3D Object Detection in RGB-D Images</h2><blockquote><p>ICCV2017《2D-Driven 3D Object Detection in RGB-D Images》</p><p>paper下载：<a href="http://openaccess.thecvf.com/content_iccv_2017/html/Lahoud_2D-Driven_3D_Object_ICCV_2017_paper.html" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_iccv_2017/html/Lahoud_2D-Driven_3D_Object_ICCV_2017_paper.html</a></p></blockquote><h3 id="概括"><a href="#概括" class="headerlink" title="概括"></a>概括</h3><p>使用RGB-D图像，做detection。借助2D图像中成熟的detection技术，来减少3D中的搜索空间，加速运行速度，同时精度又不会损失很多。</p><p>TODO IDEA：将2D detection network换成2D segmentation network是否会更好？</p><h3 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h3><p>在RGB通道（普通彩图）使用传统的2D detection方法，得到2D bounding box。那么物体所在的位置就被限制在2D bounding box所在的视野范围内（即frustum，截头锥体）。使得搜索空间减少。</p><h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><p><img src="1_1.png" alt></p><p>主要流程：</p><ul><li><p>2D object detection：得到frustum，减少搜索空间</p></li><li><p>object orientation：得到方向</p></li><li><p>bounding box regression：根据方向，回归出框框的位置和大小</p></li><li><p>refinement：根据环境信息，优化方框得分</p></li></ul><ol><li><p>Slit Detection</p><p>利用2D目标检测的技术，得到2D bounding box，从而得到frustum</p><p>使用Faster R-CNN网络，以VGG-16作为backbone</p><p><img src="1_2.png" alt></p></li><li><p>Estimating 3D Object Orientation</p><p>出发点：大多数物体都具有“Manhattan frame”，即方方正正的物体结构（若是“non-Manhattan structure”，例如球体，其任意的orientations输出均认为是正确的）。需要使得normals与3个主要的正交方向（三个坐标轴）对齐。并且假设一个frustum内只有一个object、物体放置在水平面上（orientation其中一个轴与floor法向平行）</p><p>the Manhattan Frame Estimation (MFE) technique：</p><p>   旋转矩阵$R$能通过求解下面的优化问题得到：</p><script type="math/tex; mode=display">\min_{R,X} \frac{1}{2}||X-RN||^2_F + \lambda ||X||_{1,1}</script><p>   其中$N$是由每个3D点的normals组成的矩阵，$X$是使得$RN$稀疏的松弛变量。</p><p>   将normals旋转后，对齐坐标轴，即使得$RN$稀疏。</p></li><li><p>Bounding Box Regression</p><p>以3D points的重心作为原点，3个orientations作为坐标轴方向，建立局部的正交坐标系统。</p><p>然后沿着坐标轴（3个都要），然后根据3D points的坐标计算直方图，则直方图能表示点的密度（物体的surface处密度比较大）。然后将直方图输入MLP中进行学习，输出bounding box的位置和大小。</p><p>最后粗略计算score，它是score1和score2的线性组合：</p><ul><li>score1：2D detection时的score</li><li>score2：3D point density score（The 3D point density score is computed by training a linear SVM classifier on the 3D point cloud density of the 3D cuboids for all classes）</li></ul></li><li><p>Refinement Based on Context Information</p><p>利用环境信息优化结果，例如一般情况下chairs与table靠近的可能性比较大。</p><p>假设labels的分布是Gibbs distribution，然后构造factor graph，使用unary and binary log potential functions得到probability，然后Our goal is to find the labelling that maximizes the aposteriori (MAP)，然后转化为linear program求解。</p></li></ol><h2 id="Amodal-Detection-of-3D-Objects-Inferring-3D-Bounding-Boxes-from-2D-Ones-in-RGB-Depth-Images-👍"><a href="#Amodal-Detection-of-3D-Objects-Inferring-3D-Bounding-Boxes-from-2D-Ones-in-RGB-Depth-Images-👍" class="headerlink" title="Amodal Detection of 3D Objects: Inferring 3D Bounding Boxes from 2D Ones in RGB-Depth Images 👍"></a>Amodal Detection of 3D Objects: Inferring 3D Bounding Boxes from 2D Ones in RGB-Depth Images 👍</h2><blockquote><p>CVPR2017《Amodal Detection of 3D Objects: Inferring 3D Bounding Boxes from 2D Ones in RGB-Depth Images》</p><p>paper下载：<a href="http://openaccess.thecvf.com/content_cvpr_2017/html/Deng_Amodal_Detection_of_CVPR_2017_paper.html" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2017/html/Deng_Amodal_Detection_of_CVPR_2017_paper.html</a></p></blockquote><h3 id="概括-1"><a href="#概括-1" class="headerlink" title="概括"></a>概括</h3><p>本质：将RGB-D图当作普通的图像来处理，使用Fast R-CNN网络（网络中使用的proposal是2D的），只不过网络输出的2D box regression变成了3D box regression。然后使用外部方法将2D proposal转换为3D proposal。最后：final box == 3D proposal + 3D box regression</p><p>创新点：</p><ul><li><p>将2D图像的feature应用于3D bounding box的regression上，即不需要将2.5D RGB-D图转化为3D的数据来处理。说明2.5D能编码足够的3D amodal object detection特征。</p></li><li><p>能直接得到bounding box的位置、大小、朝向（因为将其表示为7维的向量了）</p></li></ul><p>注：</p><ul><li>2.5D方法：直接在2.5D的RGB-D上学习，将2D的结果转化为3D的结果</li><li>3D的方法：直接在3D点云上进行学习，从3D window中提取点云特征</li><li>真实采集RGB-D图像并不是完美的，在Depth上noisy，而且在Depth图可能有“black hole”（或许是红外线反光造成的）。而复原出的point cloud误差大、稀疏、且物体只有一部分的表面有点集。</li><li>作者还稍微改进/修正了NYUV2数据集</li></ul><p>与Fast R-CNN的不同：</p><ul><li>2D proposal的方法不同</li><li>本文是RGB图和D图的特征的concat，而Fast R-CNN没有“双路并行”和concat。</li><li>本文的网络后端是：classification、3D box regression；而Fast R-CNN后端是classification、2D box regression。</li><li>本文需要将2D proposal转换成3D proposal，才能利用网络输出的“3D box regression”来得到最终的box；而Fast R-CNN直接就是2D proposal + 3D box regression == final box</li></ul><p>TODO IDEA：如果网络中使用的是3D proposal而不是2D proposal（下图中的绿色的“云”的部分），那么效果或不会更好？（这样就更加像Fast R-CNN了）；能否用Region Proposal Networks来替代传统的proposal方式，以实现加速？；如何把2D box proposal转换为3D box proposal的过程使用神经网络来处理（转换成只使用神经网络处理的关键或许是如何设计neural network输入和输出的“数据”和“数据排列的结构”，因为中间的过程是个black box）？</p><h3 id="思想-1"><a href="#思想-1" class="headerlink" title="思想"></a>思想</h3><p>先想办法做出3D bounding box的proposals，然后利用2D的特征来regress你的3D proposals。</p><p>即：从RGB-D图像提议出一个2D proposal，然后将RGB-D和2D proposal送入网络，网络末端则输出classification和3D regression。然后基于2D proposal提议出3D proposal，再利用3D regression便得到最终的3D bounding box位置。</p><p>使用2D proposal竟然可以得到3D regression，表明网络需要从图像中隐式地预估出3D box并且学习得到3D regression。</p><h3 id="实现-1"><a href="#实现-1" class="headerlink" title="实现"></a>实现</h3><p><img src="2_1.png" alt></p><p>仿照Fast R-CNN的结构和方法。</p><p>将RGB图和D图输入到2个VGG16。基于2D object proposals和enlarged contextual patches从RoI pooling layer提取特征，然后将特征cancat，送入后续的multiple tasks learning网络中。</p><p>具体细节如下：</p><ol><li><p>2D RoI proposals</p><p>使用adapted MCG algorithm算法，来获取proposals（只是可能存在的区域，不一定真的含有物体），可以参考这里：<a href="https://zihuaweng.github.io/2017/12/24/region_proposals/" target="_blank" rel="noopener">多种region_proposal算法比较</a></p></li><li><p>3D box proposals</p><p><img src="2_2.jpg" alt></p><p>3D box proposals从2D segment proposals中产生。</p><p>将3D bounding box表示成7维向量：$[x,y,z,l,w,h,\theta]$（假设放置在水平面上，则方位角只需要一个变量$\theta$），（注：$z$是深度方向）</p><p>3D proposals的设定方式：</p><ul><li>3D box中心点的$x$和$y$：2D segment pixels投影到3D空间中获得</li><li>3D box中心点的$z$：设置成segment points深度的中值</li><li>3D box的大小$l,w,h$：设置成该类的平均尺寸（we simply use averaged class-wise box dimensions estimated from training set as base 3D box size.）</li><li>3D box的方向角$\theta$：设为零</li></ul></li><li><p>3D box regression</p><p>将proposals修正得到最终的box，修正量：$[\delta_x,\delta_y,\delta_z,\delta_l,\delta_w,\delta_h,\delta_\theta]$</p><p>训练时，因为有ground-truth boxes，所以可以知道“修正量”的目标值应该是多少。</p><p>与Fast R-CNN的方法类似，将feature和box输入到3D box regressor net，从而训练得到“修正量”。</p></li><li><p>Multi-task Loss</p><p>将“物体种类分类”和“方框回归”的损失函数合在一起，得到多任务的Loss</p></li><li><p>Post processing</p><p>在2D detected boxes中使用NMS，在3D中不使用NMS。</p><p>对结果不pruning（例如不基于物体数值统计上的尺寸来进行筛选去除结果）。</p></li><li><p>Mini-batch sampling</p><p>主要讲怎么从标记的数据中，采样出包含positive和negative样本的minibatch，详略。</p></li></ol><h2 id="Deep-Sliding-Shapes-for-Amodal-3D-Object-Detection-in-RGB-D-Images-👍"><a href="#Deep-Sliding-Shapes-for-Amodal-3D-Object-Detection-in-RGB-D-Images-👍" class="headerlink" title="Deep Sliding Shapes for Amodal 3D Object Detection in RGB-D Images 👍"></a>Deep Sliding Shapes for Amodal 3D Object Detection in RGB-D Images 👍</h2><blockquote><p>CVPR2016《Deep Sliding Shapes for Amodal 3D Object Detection in RGB-D Images》</p><p>paper下载：<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Song_Deep_Sliding_Shapes_CVPR_2016_paper.html" target="_blank" rel="noopener">https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Song_Deep_Sliding_Shapes_CVPR_2016_paper.html</a></p></blockquote><h3 id="概括-2"><a href="#概括-2" class="headerlink" title="概括"></a>概括</h3><p>仿照Faster R-CNN，提出了3D Region Proposal Network（RPN）和joint Object Recognition Network（ORN）。</p><p>其中RPN则利用voxels作为输入，然后输出bounding box（在anchor的基础上学习offset）和objectness score；而ORN则利用voxels和RGB image作为输入，然后输出bounding box offset和class score</p><p>特点：box的位置的proposal使用3D方法；RPN具有两个感受野；ORN结合了3D几何结构特征和2D图像特征。</p><p>效果：SOTA且更快（与Sliding Shapes相比）</p><p>TODO IDEA：能不能将real world的bounding box转化为在depth map上的等效标签？</p><h3 id="实现-2"><a href="#实现-2" class="headerlink" title="实现"></a>实现</h3><ol><li><p>Encoding 3D Representation（using directional TSDF）</p><p>将space划分为等间隔的3d voxel grid，每一个voxel的内容都是一个3维向量$[dx,dy,dz]$。其中$\sqrt{dx^2+dy^2+dz^2}$是这个voxel center到其他points（point on surface）的最近的距离，数值分别沿着坐标轴方向。然后再被$2\delta$ clipped（$\delta$是格子大小）。正负号该voxel center代表在surface的前面还是后面。</p></li><li><p>Multi-scale 3D Region Proposal Network</p><p><img src="3_1.png" alt></p><p>作用：提出区域建议，所以输入是用directional TSDF编码后的voxels；输出的objectness score如果太低则不被采纳；</p><p>特点：两个感受野，分别建议出大框框和小框框。</p><p><img src="3_2.png" alt></p><p>实现细节：</p><ul><li>使用房间的主方向作为所有proposals的朝向（即使用了Manhattan world assumption）</li><li>每个voxel上有个feature，anchor有N个。将其送入softmax分支（2N个nodes）则得到N个objectness score；送入L1 smooth分支（6N个nodes）则得到N个bounding box offset（根据anchor和offset就可以计算出理想值为ground truth的proposal，这是第一次回归）</li><li>感受野更大的是因为有一个pooling，每个位置具有15个anchors；感受野小的每个位置上具有4个anchor；合起来相当于能输出19种不同大小的框框。</li><li>根据框框内点的密度，移除近似真空的box</li><li>训练过程：minibatch包含2张图，每张图采样出256个anchors。positive（iou较大）比negative（iou较小）为1比1</li><li>多任务损失：将softmax和L1 smooth线性结合得到合成的损失函数。</li><li>使用3D NMS剔除重合度大的多余框框</li></ul></li><li><p>Joint Amodal Object Recognition Network</p><p><img src="3_3.png" alt></p><p>实现细节：</p><ul><li>将box各个尺度上扩大12.5%，以容纳更多信息。然后截取框内的部分输入到网络中。</li><li>3D部分（只用了depth map，没有颜色信息）：将box内的space划分成30x30x30的的voxels，然后用directional TSDF编码voxel；2D部分：将box内的3D点投射到2D上，从而获取2D box，然后用预训练的VGG提取feature map，然后用RoI pooling池化到7x7固定尺寸。最后将2D和3D特征concat，预测object label（softmax）和3D box offset（L1 smooth）。</li><li>训练过程：For each mini-batch, we sample 384 examples from different images, with a positive to negative ratio of 1:3.</li><li>多任务损失：将softmax和L1 smooth线性结合得到合成的损失函数。</li><li>使用3D NMS剔除重合度大的多余框框</li><li>使用Object size pruning技术来降低不合常规的框框的得分（比常规太大或太小则得分除以2）</li></ul></li></ol><h2 id="Deep-Continuous-Fusion-for-Multi-Sensor-3D-Object-Detection"><a href="#Deep-Continuous-Fusion-for-Multi-Sensor-3D-Object-Detection" class="headerlink" title="Deep Continuous Fusion for Multi-Sensor 3D Object Detection"></a>Deep Continuous Fusion for Multi-Sensor 3D Object Detection</h2><blockquote><p>ECCV2018《Deep Continuous Fusion for Multi-Sensor 3D Object Detection》</p><p>paper下载：<a href="http://openaccess.thecvf.com/content_ECCV_2018/html/Ming_Liang_Deep_Continuous_Fusion_ECCV_2018_paper.html" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_ECCV_2018/html/Ming_Liang_Deep_Continuous_Fusion_ECCV_2018_paper.html</a></p></blockquote><h3 id="概括-3"><a href="#概括-3" class="headerlink" title="概括"></a>概括</h3><p>将雷达（LIDAR，point cloud数据）和前视摄像头（camera image，前视RGB图像）的数据做特征融合。做object detection（在BEV平面视角下的2D detection，或3D detection都支持）。使用端到端的网络结构，多尺度下的特征图融合。效果：显著的SOTA，推理速度快。</p><p><img src="4_4.png" alt></p><p>注：</p><ul><li>BEV即Bird’s-Eye-View，即俯视图</li><li>难点：如何实现point cloud与RGB的特征融合？</li><li>camera image和point cloud都是车辆的前方视角的数据</li></ul><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>原理概述：将前视图像的特征图转变为俯视的特征图；将point cloud转变为俯视图；然后两个俯视图相加；再在多层级上融合；然后用与RPN类似的方法提取区域建议，最后得到detection result。</p><p>将前视的camera image转变为俯视的BEV的关键原理（continuous fusion layer的功能）：</p><ul><li>用CNN提取特征得到多层级的camera image feature map，将camera image feature map（下图中的红色“camera image”）的feature利用投影关系“加载”到point cloud中，再将point cloud投影到BEV平面上。给定BEV上的一个target pixel（下图中右边的白色正方形），先用KNN的方法寻找其邻居，然后使用使用类似于“点卷积”的方法聚合信息，从而获取到target pixel应得的特征向量。</li></ul><p><img src="4_1.png" alt></p><h3 id="实现-3"><a href="#实现-3" class="headerlink" title="实现"></a>实现</h3><p><img src="4_2.png" alt></p><p>网络的整体架构如上。</p><ul><li>黄色的camera stream：使用ResNet18提取camera image的多层级的feature map。</li><li>红色的LIDAR stream：使用ResNet18（不是同一个）提取LIDAR BEV（雷达点云的俯视图）的多层级的feature map</li><li>蓝色的Fusion Layers：使用continuous fusion layer将前视的feature map转变为俯视的feature map，然后与LIDAR BEV的feature map做融合</li><li>绿色的detection header：使用类似于Faster R-CNN的方法，用RPN的方法提取区域建议，然后分类识别等等。</li></ul><ol><li><p>Continuous Fusion Layer</p><p><img src="4_3.png" alt></p><ul><li>如果要求BEV上的一个浅蓝色正方形像素点对应的feature vector，则首先寻找其K个邻居（邻居是点云的俯视图投影的来的）；从而知道这K个邻居在点云中是哪K个点，并且计算在3D点云空间中的相对坐标$x_j-x_i$（$x_j$是邻居，$x_i$是中心点）</li><li>然后将这K个点投影到前视图（实质上是前视图的feature map），这K个点落在哪个像素上（因为不可能刚好落在像素上，所以实际上是使用双线性插值），就被携带了这个像素所对应的feature vector；这样这K个点就都有各自的feature vector了，第$j$个点的feature vector记作$f_j$</li><li>然后作者仿照“点卷积”（$h_i=\sum_j MLP(x_i-x_j)\cdot f_j$）的公式，作者提出使用：<script type="math/tex">h_i=\sum_j MLP(cancat[f_j,x_j-x_i])</script>公式更快更省内存。使用这条公式得到的$h_i$即是浅蓝色正方形得到的feature vector</li></ul><p>特点：BEV图上的像素点包含了“点云的局部空间信息”、“前视图的特征信息”，并使用MLP使这两个信息”融合处理”，从而生成BEV图。</p></li><li><p>Architecture of our model</p><ul><li>使用ResNet18提取feature map</li><li>continuous fuse layer的输入是对应的image feature map以及所有特征图融合后的输出；特征图之间用逐元素之间的加法加起来</li><li>随后将得到的feature map使用1x1卷积；用2个anchors；anchor的输出是逐像素的类别置信度以及box的center、location、size、orientation；最后用NMS</li><li>使用多任务混合损失函数：$L=L_{cls}+\alpha L_{reg}$，即分类和回归任务的混合</li></ul></li></ol><h2 id="Frustum-PointNets-for-3D-Object-Detection-from-RGB-D-Data"><a href="#Frustum-PointNets-for-3D-Object-Detection-from-RGB-D-Data" class="headerlink" title="Frustum PointNets for 3D Object Detection from RGB-D Data"></a>Frustum PointNets for 3D Object Detection from RGB-D Data</h2><blockquote><p>CVPR2018《Frustum PointNets for 3D Object Detection from RGB-D Data》</p><p>paper下载：<a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Qi_Frustum_PointNets_for_CVPR_2018_paper.html" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2018/html/Qi_Frustum_PointNets_for_CVPR_2018_paper.html</a></p></blockquote><h3 id="概括-4"><a href="#概括-4" class="headerlink" title="概括"></a>概括</h3><p>将RGB-D转为point cloud，然后使用3d的方法来处理点云数据。整体框架：先用2D detection的方法缩小搜索空间，然后做3D Instance Segmentation剔除杂点，最后Amodal 3D Box Estimation来回归得到框框。</p><p>作者声称的优点：efficiency、high recall、precisely estimate 3D bounding boxes even under strong occlusion or with very sparse points、SOTA、having real-time capability</p><p>模型名字：Frustum PointNets</p><h3 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h3><p><img src="5_1.png" alt></p><p>利用2D detection的方法缩小搜素空间（形成frustum）；然后剔除杂点、生成回归框。</p><p>生成frustum的优点：efficiently propose possible locations of 3D objects in a 3D space</p><h3 id="实现-4"><a href="#实现-4" class="headerlink" title="实现"></a>实现</h3><p><img src="5_2.png" alt="Frustum PointNets for 3D object detection."></p><p><img src="5_3.png" alt="Coordinate systems for point cloud."></p><ol><li><p>Frustum Proposal</p><p>利用2D detection的方法（如FPN），生成2D region proposal（predict amodal 2D boxes），并且分类类别（one-hot class vector）；然后region2frustum模块则根据2D region生成3D frustum；然后转而使用“Coordinate systems for point cloud”图中（b）的坐标系，即其中的一个轴是沿着视线的中央位置的。</p></li><li><p>3D Instance Segmentation</p><p>用二分类来对每个点做segmentation，同时利用2D detector得到的prediction，从而能够判断某个点是否属于目标物体（object of interest）的点；这样就能生成一张mask（掩膜），从而得到比较干净的区域；然后将XYZ坐标减去其重心，得到“Coordinate systems for point cloud”图中（c）的坐标系的样子（注意，我们不缩放点云的坐标）。</p><p><img src="5_4.png" alt></p><p>作用：剔除Foreground Occluder（遮挡物）和Background Clutter（背景杂乱），因为depth的区别比较大</p><p>前提：Note that each frustum contains exactly one object of interest.</p></li><li><p>Amodal 3D Box Estimation</p><p>先用T-Net（light-weight regression PointNet）来平移点云（回归出center的残差），使得centroid接近amodal box center（real object center）；</p><p><img src="5_5.png" alt></p><p>然后用Amodal 3D Box Estimation PointNet来回归出框框（网络输出的结点个数是$3+4\times NS+2\times NH$？？？）</p><p><img src="5_6.png" alt></p><p>回归框框的量：</p><ul><li>center residual：理想值是T-net之后的中心位置与真实中心的偏差（绝对坐标下的中心 = mask的中心 + “T-NET”学习到的偏移量 + 框框回归得到的偏移量）</li><li>size</li><li>heading angle$ \theta $（along up-axis）</li></ul></li><li><p>Training</p><ul><li><p>使用multi-task losses（三个网络的混合loss）</p></li><li><p>提出所谓的“corner loss”，能同时优化框框的center、size、heading，而避免了只优化一个量（本质上corner loss是预测框的8个顶点与ground truth的顶点的距离之和）</p></li></ul></li></ol><h2 id="Joint-3D-Proposal-Generation-and-Object-Detection-from-View-Aggregation"><a href="#Joint-3D-Proposal-Generation-and-Object-Detection-from-View-Aggregation" class="headerlink" title="Joint 3D Proposal Generation and Object Detection from View Aggregation"></a>Joint 3D Proposal Generation and Object Detection from View Aggregation</h2><blockquote><p>IROS2018《Joint 3D Proposal Generation and Object Detection from View Aggregation》</p><p>paper下载：<a href="https://ieeexplore.ieee.org/document/8594049" target="_blank" rel="noopener">https://ieeexplore.ieee.org/document/8594049</a></p></blockquote><h3 id="概括-5"><a href="#概括-5" class="headerlink" title="概括"></a>概括</h3><p>提出了一套无人驾驶中的detection方案叫AVOD（Aggregate View Object Detection network）。</p><p>使用“雷达点云数据+RGB图像”作为输入（两种模态所以需要multimodal feature fusion）。仿照Faster R-CNN，AVOD主要由RPN、detector network组成。特征提取操作还是在2D层次上的处理。</p><p>优点：</p><ul><li>可以检测小物体，且high recall（因为使用high resolution feature map（仿照了FPN的做法），以及两个模态融合，所以可以在proposal阶段达到high recall）</li><li>较精确预测3D box的orientation（因为作者提出了新的box表达方式，称作4 corner encoding）</li><li>SOTA、场景泛化性能好、实时、低内存占用（efficiency是因为proposal之前使用1x1 conv进行了feature map channels number的reduction）</li></ul><p>3D box与2D box的区别：3Dbox是3D的，3Dbox还要预测orientation</p><h3 id="实现-5"><a href="#实现-5" class="headerlink" title="实现"></a>实现</h3><p><img src="6_1.png" alt></p><ol><li><p>将point cloud数据通过某种基于voxel grid的方法转化成6-channel BEV map，这个BEV就是上图中的“BEV Input”</p></li><li><p>然后利用下图类似于FPN结构的网络来提取特征，分别得到“Image Feature Maps”和“BEV Feature Maps”（得到的是高分辨率的特征图，有助于识别尺寸小的物体。输出图的尺寸不变，但是通道数可能改变。取最后一层的输出作为特征图）</p><p><img src="6_2.png" alt></p></li><li><p>RPN</p><ul><li><p>目的：回归出anchor与GT的偏移量$(\Delta t_x,\Delta t_y,\Delta t_z,\Delta d_x,\Delta d_y,\Delta d_z)$（这里使用的是axis aligned bounding box），以及objectness score（注意这里并没有回归orientation）</p><p><img src="6_3.png" alt></p></li><li><p>anchor生成方式：centroid是BEV平面上的等间隔采样，中心的高度等于摄像机的高度，size是聚类获得。所以本质上是用2D的anchor表示3D（因为中心点的高度都是一样的）。并且移除空的anchors。暂不考虑orientation，即与坐标轴对齐</p></li><li><p>crop and resize：将3D anchor投影到BEV或前视图上，得到对应的ROI区域。然后将对应区域crop出来，再resize。从而得到固定尺寸的输入</p></li><li><p>流程：</p><ul><li>先1x1卷积将特征图缩小到只有1个通道（减少计算量）</li><li>然后根据anchors来进行crop and resize，得到“feature crop”</li><li>将“feature crops”以element-wise mean operation的方式进行融合</li><li>然后送入FC层，最后再2D NMS（本质是2D的，因为proposals中心点的高度都是一样的）</li></ul></li></ul></li><li><p>Detection Network</p><ul><li>目的：得到最终的detection box</li><li>输入：将3D proposals投影到BEV、前视图得到的区域，再crop and resize，这样输入尺寸才相同</li><li><p>输出：box regression, orientation estimation, and category classification for each proposal</p></li><li><p>框框编码：将3D box用“4 corner encoding”方法编码，即4个底角的xy坐标，以及上下面到地板的距离，那么框框的回归量就是$(\Delta x_1,\Delta x_2,\Delta x_3,\Delta x_4,\Delta y_1,\Delta y_2,\Delta y_3,\Delta y_4,\Delta h_1,\Delta h_2)$，回归量是proposal与GT的偏移量。相比前人的8 corner表示法，参数量从24减少到10个。</p></li><li>Orientation Vector Regression：传统的表示法就是$\theta$。而作者使用$(x_\theta,y_\theta)=(cos(\theta),sin(\theta))$表示orientation。（但是orientation好像已经在“4 corner encoding”暗含了，为什么又要估计方向？？）</li><li>流程：<ul><li>element-wise mean operation进行“feature crops”的融合</li><li>经过FC层，输出“框框10维回归量”+“方向2维估计量”+“one-hot分类”</li><li>最后NMS</li></ul></li></ul></li></ol><h2 id="Multi-View-3D-Object-Detection-Network-for-Autonomous-Driving"><a href="#Multi-View-3D-Object-Detection-Network-for-Autonomous-Driving" class="headerlink" title="Multi-View 3D Object Detection Network for Autonomous Driving"></a>Multi-View 3D Object Detection Network for Autonomous Driving</h2><blockquote><p>CVPR2017《Multi-View 3D Object Detection Network for Autonomous Driving》</p><p>paper下载：<a href="http://openaccess.thecvf.com/content_cvpr_2017/html/Chen_Multi-View_3D_Object_CVPR_2017_paper.html" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2017/html/Chen_Multi-View_3D_Object_CVPR_2017_paper.html</a></p></blockquote><h3 id="概括-6"><a href="#概括-6" class="headerlink" title="概括"></a>概括</h3><p>使用Multi-View的方式，对“雷达点云+RGB图”的输入，进行特征融合。达到了高精度。将点云转换成图像表达，并且需要将3D proposal投射到2D平面上，所以其实MV3D还是基于2D方法。</p><p>方法简述：将稀疏的点云转变成multi-view表示（作者是BEV+FrontView），于是MV3D的输入就是LIDAR BEV + LIDAR FV + RGB Image。然后分别提取特征得到三个feature map。然后在LIDAR BEV的feature map上进行propose得到3D proposals（也用了基于anchor的方法），投影到平面上得到ROI区域，使用ROI pooling得到固定大小的输入，然后用“deep fusion”方法融合特征，最后预测类别和框框的回归量。</p><p>其他：</p><ul><li><p>MV3D == Multi-View 3D object detection network</p></li><li><p>Laser scanners have the advantage of accurate depth information while cameras preserve much more detailed semantic information（所以LIDAR有助于确定框框的位置，RGB有助于确定物体类别）</p></li></ul><h3 id="实现-6"><a href="#实现-6" class="headerlink" title="实现"></a>实现</h3><p>网络分为两部分：3D Proposal Network（类似于RPN） 和 Region-based Fusion Network（其实类似于ORN）</p><p><img src="7_1.png" alt></p><ol><li>数据预处理步骤</li></ol><p>将LIDAR point cloud手工提取特征/手工编码得到LIDAR BEV和LIDAR FV</p><ul><li><p>提取点云在BEV视图上的高度特征、密度特征、强度特征，得到Bird’s eye view features，它是6通道的特征图。</p></li><li><p>提取点云在FV视图上的高度特征、距离特征、强度特征，得到Front view features（FV视图是雷达的横向扫角、雷达的纵向扫角组成的视图。xoy平面是BEV视图）</p><p><img src="7_2.png" alt></p><p>最终得到的MV3D的输入是：</p><p><img src="7_3.png" alt></p></li></ul><ol><li><p>3D Proposal Network</p><ul><li>使用LIDAR BEV进行框框的propose（优点：保留了框框的物理实际尺寸、避免遮挡、垂直高度方向差异不大）。</li></ul></li></ol><ul><li>仿照Faster R-CNN使用基于anchor的方法，3D anchor为$(x,y,z,l,w,h)$，2D anchor为$(x_{BEV},y_{BEV},l_{BEV},w_{BEV})$，3D anchor size（$(l,w,h)$）是聚类获得的，$z$可以根据摄像头高度和$h$计算（假设是物体是放在地面上的），所以实际上是2D anchor。<ul><li>然后使用类似于VGG16的网络来提取特征图。</li></ul></li><li>因为BEV视图分辨率不足，识别小物体有困难，所以作者使用bilinear upsampling（图中的deconv）进行上采样增加分辨率。<ul><li>然后3D Proposal Network的输出就是anchor与GT的偏移量、以及objectness score</li></ul></li><li>移除空框框，使用multi-task loss，使用NMS，取出top2000进行训练，top300进行推断</li></ul><ol><li><p>Region-based Fusion Network</p><ul><li><p>将3D proposal投影到BEV/FV/RGB平面上得到各自的2D proposal（图中的红蓝绿矩形）</p></li><li><p>然后根据ROI来crop，再使用ROI pooling，得到固定大小的feature map</p></li><li><p>再将3个feature map使用“deep fusion”方法来进行特征融合，如下图的（c）。即先算元素均值、再分别用3个FC layers并行处理，再将结果取均值，以此反复。</p><p><img src="7_4.png" alt></p></li><li><p>回归量是$t = (\Delta x_0,…,\Delta x_7,\Delta y_0,…,\Delta y_7,\Delta z_0,…,\Delta z_7)$，也就是使用8 corners的坐标来表征bounding box。于是orientation也能从corners中推断出来。输出是回归量$t$和物体类别。使用multi-task loss，使用NMS。</p><p>可能是由于“deep fusion”训练困难，我们使用 drop-path training 和 auxiliary losses 的方法来正则化训练，不详述。</p></li></ul></li></ol><h2 id="VoxelNet-End-to-End-Learning-for-Point-Cloud-Based-3D-Object-Detection-👍"><a href="#VoxelNet-End-to-End-Learning-for-Point-Cloud-Based-3D-Object-Detection-👍" class="headerlink" title="VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection 👍"></a>VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection 👍</h2><blockquote><p>CVPR2018《VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection》</p><p>paper下载：<a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Zhou_VoxelNet_End-to-End_Learning_CVPR_2018_paper.html" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2018/html/Zhou_VoxelNet_End-to-End_Learning_CVPR_2018_paper.html</a></p></blockquote><h3 id="概括-7"><a href="#概括-7" class="headerlink" title="概括"></a>概括</h3><p>对单纯雷达点云数据的object detection（所以只有几何信息）。将feature extraction和bounding box prediction的过程整合进一个single stage, end-to-end trainable deep network中，取名叫VoxelNet。关键是voxel feature encoding (VFE) layer的构建，能自动学习有效的有鉴别力的特征。最后通过RPN来输出detection box。达到了新的SOTA。</p><p>处理LIDAR点云的难点：点云稀疏、点云密度变化、点云数量比较多（约100k个点）</p><p>基本思想：将点云使用voxel栅格化、且在每个voxel内使用“pointnet”，使其转变为4D tensor。接下来使用卷积的方法就可以了。</p><h3 id="实现-7"><a href="#实现-7" class="headerlink" title="实现"></a>实现</h3><p>整体分为3部分：(1) Feature learning network, (2) Convolutional middle layers, and (3) Region proposal network</p><ol><li><p>Feature Learning Network</p><ul><li><p>Voxel Partition：将3D空间等间距分割成很多个voxels</p></li><li><p>Grouping：在同一个voxel内的点被一起进行后续操作（根据voxel的分割将点云划分分组）</p></li><li><p>Random Sampling：从点数多于$T$的voxel内采样出$T$个点，使得每个voxel内的点数都不超过$T$（有助于解决点云的密度不均，以及节省计算）</p></li><li><p>Stacked Voxel Feature Encoding：将VFE layers级联，提取每个voxel的特征</p><p><img src="8_1.png" alt></p><p>上图是一层VFE layer对一个voxel的操作。图中假设$T=3$（就是采样出蓝、绿、黄三个点）。点$i$的输入特征向量（Point-wise Input）为$(x_i,y_i,z_i,r_i,x_i-v_x,y_i-v_y,z_i-v_z)$，其中$(x_i,y_i,z_i)$是点的坐标、$(r_i)$是雷达接收到的反射强度，$(x_i-v_x,y_i-v_y,z_i-v_z)$是相对重心（重心即所有点的均值）的相对坐标。简言之，有点类似于pointnet中的segmentation的特征操作，好比是一个voxel就是一个pointnet，但是FC layers参数是共享的。</p><p><img src="8_2.png" alt></p><p>经过n个VFE layers之后就得到了上图中的Point-wise Feature-n，然后输入到FC再MP，就得到了描述该voxel的特征Voxel-wise Feature。</p></li><li><p>Sparse Tensor Representation：每个voxel都有个特征向量，所以点云可以用4D tensor表示（尺寸$C\times D’\times H’\times W’$，$C$是每个voxel的特征向量长度，$D’$是Z轴方向上的voxels的个数，$H’$是Y轴方向上的voxels个数，$W’$是X轴方向上的voxels个数，xoy平面就是BEV平面）。超过90%的voxel都是空的，后续操作时不必对其进行操作（似乎就是作者提到的Efficient Implementation）。</p></li></ul></li><li><p>Convolutional Middle Layers</p><p>使用3D卷积对4D tensor进行卷积，并进行reshape到3D tensor。</p><p>举例：输入尺寸（4D tensor）是$128\times 10\times 400\times 352$，输出尺寸（经过Convolutional Middle Layers之后）是$64\times 2 \times400 \times 352$，然后reshape到$128 \times 400\times352$变成3D tensor（注意到$400\times 352$正是BEV视图上的栅格尺寸）</p><p>目的：</p><ul><li>reshape的目的是方便后续的Region Proposal Network进行2D卷积</li><li>3D卷积的目的是聚合周围环境voxels的信息</li></ul></li><li><p>Region Proposal Network</p><p>进行2D卷积，最后输出(1) a probability score map and (2) a regression map</p><p>虽然是在类似于BEV的视图上进行卷积，但是输出的回归量却是3D的。</p><p>回归量是$(\Delta x,\Delta y,\Delta z,\Delta l,\Delta w,\Delta h,\Delta \theta)$。</p><p>使用多任务损失：分类的positive/negative损失、位置回归的positive损失。</p><p>如下图所示：</p><p><img src="8_3.png" alt></p></li><li><p>完整结构</p><p><img src="8_4.png" alt></p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 3d </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 3d </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习中常用的指标</title>
      <link href="/dl/precisionrecall/"/>
      <url>/dl/precisionrecall/</url>
      
        <content type="html"><![CDATA[<h1 id="指标"><a href="#指标" class="headerlink" title="指标"></a>指标</h1><h2 id="题目一（基本概念）"><a href="#题目一（基本概念）" class="headerlink" title="题目一（基本概念）"></a>题目一（基本概念）</h2><p>假设有1000个人。我们用上帝视角知道，患病的有100个，不患病的有900个。</p><div class="table-container"><table><thead><tr><th style="text-align:center">患病（FN+TP）</th><th style="text-align:center">不患病（TN+FP）</th><th style="text-align:center">总数（FN+TP+TN+FP）</th></tr></thead><tbody><tr><td style="text-align:center">100</td><td style="text-align:center">900</td><td style="text-align:center">1000</td></tr></tbody></table></div><p>然后我们有医生，医生一个个检查完这1000个人。判定结果为患病200个，不患病800个。</p><div class="table-container"><table><thead><tr><th style="text-align:center">患病（P）</th><th style="text-align:center">不患病（N）</th><th style="text-align:center">总数（P+N）</th></tr></thead><tbody><tr><td style="text-align:center">200</td><td style="text-align:center">800</td><td style="text-align:center">1000</td></tr></tbody></table></div><p>实际情况如图：</p><p><img src="1.png" alt></p><div class="table-container"><table><thead><tr><th>FN = FN</th><th>TP = 100-FN</th></tr></thead><tbody><tr><td>TN = 800-FN</td><td>FP = 100+FN</td></tr></tbody></table></div><h2 id="分析一"><a href="#分析一" class="headerlink" title="分析一"></a>分析一</h2><p>True/False是指：是否判断对了/是否符合事实（例如，得病说没病or没病说得病，则属于False）</p><p>Positive/Negative是指：医生的判断结果（例如，你说他有病，则属于Positive）</p><ul><li><p>FN： 判断错了（false），表明真实情形是“患病”</p></li><li><p>FP： 判断错了（false），表明真实情形是“不患病”</p></li><li>TP： 判断对了（true），就是”患病“</li><li>TN： 判断对了（true），就是”不患病“</li></ul><h3 id="Precision"><a href="#Precision" class="headerlink" title="Precision"></a>Precision</h3><p><img src="2.png" alt></p><script type="math/tex; mode=display">P = \frac{TP}{TP+FP} = \frac{TP}{Positive} = \frac{你检测出的确实患病的人数}{你认为患病的人数} = \frac{100-FN}{200}</script><h3 id="Recall"><a href="#Recall" class="headerlink" title="Recall"></a>Recall</h3><p><img src="3.png" alt></p><script type="math/tex; mode=display">R = \frac{TP}{TP+FN} = \frac{你检测出的确实患病的人数}{潜在患病者人数} = \frac{100-FN}{100}</script><h3 id="F1-score"><a href="#F1-score" class="headerlink" title="F1-score"></a>F1-score</h3><p>是P和R的综合考量，本质是$\frac{1}{F}=\frac{1}{2}(\frac{1}{R}+\frac{1}{P})$</p><script type="math/tex; mode=display">F = \frac{2PR}{P+R}</script><h3 id="Accuracy"><a href="#Accuracy" class="headerlink" title="Accuracy"></a>Accuracy</h3><script type="math/tex; mode=display">Accuracy = \frac{TP+TN}{TP+TN+FP+FN} = \frac{True}{All} = \frac{判对的人数}{总人数} = \frac{900-2FN}{1000}</script><h3 id="ROC曲线（Receiver-Operating-Characteristic-Curve）"><a href="#ROC曲线（Receiver-Operating-Characteristic-Curve）" class="headerlink" title="ROC曲线（Receiver Operating Characteristic Curve）"></a>ROC曲线（Receiver Operating Characteristic Curve）</h3><ul><li>TruePositiveRate就是Recall，$\frac{TP}{TP+FN}$（患病中有多少被正确诊断为有病，越高越好）</li><li>FalsePositiveRate为$\frac{FP}{FP+TN}$（健康中有多少被误诊成有病，越低越好）</li></ul><p>假设我们的医生可以给出一个“概率值”表示该人患病的概率是多少。那么我们需要设定一个阈值，例如0.6，将0.6以上的全部认为是Positive，以下的全部认为是Negative，则我们得到了一个判别器。</p><p>若要画ROC曲线，你则需要以“TruePositiveRate”为纵坐标，“FalsePositiveRate”为横坐标，以“阈值”为滑动变量（从1滑动到0），则你能在平面画出ROC曲线（其中的点代表某一个阈值下的“TruePositiveRate”和“FalsePositiveRate”）。</p><p><img src="4.png" alt></p><h3 id="AUC（Area-Under-the-Curve）"><a href="#AUC（Area-Under-the-Curve）" class="headerlink" title="AUC（Area Under the Curve）"></a>AUC（Area Under the Curve）</h3><p>就是ROC曲线下的面积，数值越高越好（最高是1）</p><p><img src="5.png" alt></p><h2 id="题目二（Detection）"><a href="#题目二（Detection）" class="headerlink" title="题目二（Detection）"></a>题目二（Detection）</h2><p>相关链接：<a href="http://cocodataset.org/#detection-eval" target="_blank" rel="noopener">Detection Evaluation</a></p><p>在detection任务中，</p><ul><li>IoU：用来控制True/False（我们需要设定一个IoU阈值，用来判定predict box是否属于ground truth）</li><li>score：用来控制Positive/Negative（我们需要一个分数阈值，分数过低的box会被筛掉，剩下的box则属于Positive）</li></ul><p><img src="6.png" alt></p><p>在一张图片中，我们有很多的objects。因此我们就可以计算出这张图片的Precision和Recall（在给定IoU threshold和score threshold的条件下）</p><ul><li>COCO的指标：在多个IoU threshold = {0.05, 0.15, 0.25, …, 0.95}的条件下，计算多个AP值</li><li>PASCAL VOC的指标：在IoU threshold = 0.5的条件下，计算mAP值</li></ul><h3 id="IoU、PR曲线、AP和mAP"><a href="#IoU、PR曲线、AP和mAP" class="headerlink" title="IoU、PR曲线、AP和mAP"></a>IoU、PR曲线、AP和mAP</h3><p>IoU是两个boxes的交集面积除以并集面积。不过要求两个boxes的label一致时才能用面积法计算，否则IoU是零（此时必定是FP）。</p><p>下面默认在IoU threshold = 0.5时讨论。AP是在“precision-recall曲线（PR曲线）”$precision = PR(recall)$中，当recall值分别为$ \{ 0, 0.1, 0.2, …, 1.0 \} $这11个数时，对应的precision的算术平均值，即：</p><script type="math/tex; mode=display">AP = \frac{1}{11} \sum_{i=0}^{10} PR(i \times 0.1)</script><p>换句话说，我们需要选取11个不同的score threshold，使得recall值分别为$\{0, 0.1, 0.2, …, 1.0\}$，然后计算precisions的算术平均值，即是AP（一般来说，precision值都是通过插值得到的），有点类似于PR曲线底下的面积</p><p>每一个class都计算一个AP值，然后将所有classes的AP取算术平均就得到mAP。</p><p>注：</p><ul><li>AP == Average Precision</li><li>mAP = mean Average Precision</li><li>AP是每个class都有一个，而mAP是所有classes的AP值的算术平均</li><li>“precision-recall曲线”：在不同的score threshold下，画出的precision和recall的曲线。与ROC曲线类似。</li><li>如果IoU threshold有多个，则mAP值也有多个。</li></ul><p><img src="7.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> dl </category>
          
      </categories>
      
      
        <tags>
            
            <tag> dl </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>3D点云特征提取笔记</title>
      <link href="/3d/3dclassificationnotes/"/>
      <url>/3d/3dclassificationnotes/</url>
      
        <content type="html"><![CDATA[<h1 id="3D点云特征提取笔记"><a href="#3D点云特征提取笔记" class="headerlink" title="3D点云特征提取笔记"></a>3D点云特征提取笔记</h1><p>下面的papers都是有关3D点云特征提取的，将提取到的特征用于分类或分割任务。</p><p>经历了从<code>深度学习+手工设计</code>方法到<code>几乎完全深度学习</code>的转变。</p><p>本文完整的PDF版本你可以从<a href="3dClassificationNotes.pdf">这里</a>下载。</p><h2 id="PointNet-👍"><a href="#PointNet-👍" class="headerlink" title="PointNet 👍"></a>PointNet 👍</h2><blockquote><p>CVPR2017《PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation》</p><p>在线介绍：<a href="http://stanford.edu/~rqi/pointnet/" target="_blank" rel="noopener">http://stanford.edu/~rqi/pointnet/</a></p><p>paper下载：<a href="http://openaccess.thecvf.com/content_cvpr_2017/html/Qi_PointNet_Deep_Learning_CVPR_2017_paper.html" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2017/html/Qi_PointNet_Deep_Learning_CVPR_2017_paper.html</a></p><p>pytorch代码：<a href="https://github.com/fxia22/pointnet.pytorch" target="_blank" rel="noopener">https://github.com/fxia22/pointnet.pytorch</a></p></blockquote><h3 id="概括"><a href="#概括" class="headerlink" title="概括"></a>概括</h3><p>提出了一种“天然”适用于处理point cloud数据的Net。并且给出理论分析和较多实验。声称达到了state-of-the-art的效果：模型结构简单、精度高、计算复杂度低。</p><p>TODO IDEA：如果对称函数/max pooling换做了sort函数/top k函数（排序后返回前k大的值），那会怎么样？没有层次化特征提取的过程（PointNet++已解决）？</p><h3 id="难点"><a href="#难点" class="headerlink" title="难点"></a>难点</h3><p>最大的难点是如何处理point cloud数据的无序性，因为传统的操作都是有顺序依赖的（例如卷积，它是顺序相关的）。此外，点之间的空间相关性、变换（平移旋转等）的不变性也是难点所在。</p><p>传统的处理方法：将点云数据投影到二维平面然后使用图像的方法处理；将点云数据划分到有空间依赖关系的voxel；数据sort之后再输入；使用RNN尝试消除输入的顺序性。</p><h3 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h3><p>使用具有对称性质的函数（例如max函数），来实现处理点云的无序性。</p><p>形式化表述即：</p><script type="math/tex; mode=display">   f(\{x_1,...,x_n\}) \approx g(h(x_1),...,h(x_n))</script><p>   意思是说，先进行非线性变换$h(\cdot)$，再使用对称函数$g(\cdot)$，就能近似出某一个能实现无序性的函数$f(\cdot)$。且$h(\cdot)$决定了$f(\cdot)$，所以关键是如何学习$h(\cdot)$。</p><p>   其中函数$h(\cdot)$使用多层感知机自动学习出来，$g(\cdot)$是某一个对称函数（例如max函数），$x_i$是第$i$个点。</p><h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><ol><li><p>网络前端（实现变换不变性）</p><p>使用与输入数据相关的spatial transformer network（缩写stn），来生成一个仿射变换矩阵（它是方阵，例如一个object有2500个points且point的维度是3（x、y和z坐标构成3维）的点，那么丢进这个网络则会得到一个3x3大小的方阵）<br>然后将输入数据点施加这个仿射变换（乘以它），就得到预处理后的结果（虽然还是2500个维度是3的点）。可能可以认为这个仿射变换的作用是消除camera的摆放视角（看作是某一种变换）的影响？？</p><pre><code>input data size: [batchsize, 3, 2500]after many conv1d: [batchsize, 1024, 2500]after max function: [batchsize, 1024, 1]after view: [batchsize, 1024]after many fc layer: [batchsize, 9]after reshape to matrix: [batchsize, 3, 3]after add identity matrix: [batchsize, 3, 3] （加上3x3的单位矩阵，why？？可能类似于resnet的思想，或者是其他原因）</code></pre><p>且我们在loss上加了正则约束项$||I-AA^T||^2_F$，使得学习到的矩阵$A$尽可能地是正交矩阵。</p></li><li><p>网络中端（消除输入的顺序性）</p><p>然后按照下图来操作：</p><p><img src="http://stanford.edu/~rqi/pointnet/images/pointnet.jpg" alt></p><p>两处用到了stn网络。stn生成仿射矩阵后对每个points都施加相同的仿射变换，不同sample的仿射矩阵不一样，都需要用stn网络计算出来。至于为什么后面也有个stn，那就只能姑且认为是stn网络起到类似于batchnorm的作用了。。。</p><p>其中max函数对每个feature_dim（共1024个）扫视一遍，然后取max（例如2500个points则取2500个数值中最大的，然后形成相应feature_dim上的值）</p><p>上图中蓝色部分是分类网络，黄色部分是分割网络。分割网络需要cancat global features和local features。（但实际上local features并没有充分地提取局部特征，例如密度表征等，所以后来才提出了其改进版PointNet++，但速度更慢）</p></li><li><p>网络后端（分类or分割）</p><ul><li><p>分类就是正常的Classificaion网络了</p></li><li><p>分割则是继续使用conv1d，然后view得到<code>[batchsize, 2500, classnum]</code>，再view作<code>[batchsize*2500, classnum]</code>然后使用softmax，最后再view回来<code>[batchsize, 2500, classnum]</code>就好了</p></li></ul></li></ol><h2 id="PointNet"><a href="#PointNet" class="headerlink" title="PointNet++"></a>PointNet++</h2><blockquote><p>NIPS2017《PointNet++: Deep Hierarchical Feature Learning onPoint Sets in a Metric Space》</p><p>在线介绍：<a href="http://stanford.edu/~rqi/pointnet2/" target="_blank" rel="noopener">http://stanford.edu/~rqi/pointnet2/</a></p><p>paper下载：<a href="http://papers.nips.cc/paper/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space" target="_blank" rel="noopener">http://papers.nips.cc/paper/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space</a></p></blockquote><h3 id="概括-1"><a href="#概括-1" class="headerlink" title="概括"></a>概括</h3><p>是PointNet的升级版。针对PointNet对local features提取能力弱的缺点（例如不能应对points density的variation），提出使用层级结构来提取local features。简言之，就是模仿了CNN的分层特征提取，它将全体Points划分为许多local regions（类比于CNN中Kernel大小的概念），然后使用原始版本的PointNet提取regions里点的特征，并利用centroid取代regions的点，实现点数的减少，再多次这样，最后就能提取包含local features的features了。</p><p>结果：特征表达更有效、鲁棒、精度高、但是计算代价高且慢（比PointNet慢几倍）</p><p>其他：作者还发现在Non-Euclidean Metric Space（为geodesic distances）中模型的表现也很好，可以抵抗object的形变（例如不同姿态的猫）。作者可视化了the first level kernels，<a href="http://stanford.edu/~rqi/pointnet2/images/features.jpg" target="_blank" rel="noopener">戳图</a>。</p><p>TODO IDEA：可能不同的度量空间对性能也有影响？感觉划定local regions的方式过于粗暴和生硬，有待改进？</p><h3 id="难点-1"><a href="#难点-1" class="headerlink" title="难点"></a>难点</h3><p>如何有效地提取local features以抵抗density variation？（density variation：即点云的密度变化，稀疏的点云和密集的点云对模型精度的影响比较大，模型应该能对这种变化鲁棒才行）</p><h3 id="思想-1"><a href="#思想-1" class="headerlink" title="思想"></a>思想</h3><p>模仿CNN的做法（分层级的特征提取）</p><h3 id="实现-1"><a href="#实现-1" class="headerlink" title="实现"></a>实现</h3><ol><li><p>Hierarchical Point Set Feature Learning using <code>set abstraction levels</code></p><p>set abstraction levels由Sampling layer、Grouping layer和PointNet layer组成</p><ol><li><p>Sampling layer</p><p>作用：确定local regions的centroid位置</p><p>算法：farthest point sampling (FPS)最远点采样</p></li><li><p>Grouping layer</p><p>作用：确定centroid的neighborhood</p><p>算法（选其一）：</p><ul><li><p>ball query（推荐）：确定球面的半径，然后在半径以内的点都作为邻居</p></li><li><p>kNN（不推荐）：排序出最近的K个点作为邻居</p></li></ul></li><li><p>PointNet layer</p><p>  作用：特征提取</p><p>  算法过程：</p><ul><li><p>先将neighborhood的坐标转换为相对于centroid的相对坐标</p></li><li><p>再使用原始版本的PointNet作特征提取，提取到的特征作为centroid的特征。原本的neighborhood则不必再用，centroid作为下一个set abstraction levels的输入。</p></li></ul></li></ol></li><li><p>Robust Feature Learning under Non-Uniform Sampling Density</p><p>刚才所谓的set abstraction levels可能对density不鲁棒，我们需要融合多尺度（multi-scale，可以理解为neighborhood数量的多少，不一定是相同centroid的）的信息来达到鲁棒性。</p><p>不是多尺度的我们叫SSG（ablated PointNet++ with single scale grouping in each level），多尺度的我们叫MSG（multi-scale grouping）或MRG（multi-resolution grouping）。</p><p><img src="2_1.png" alt></p><p>我们提出了两种多尺度融合方式：MSG、MRG。</p><p>举个例子，MSG就是分别使用9999个、999个、99个neighborhood所提取到的features作concatenation所得到的features的方式；而MRG就是使用3个99neighborhood接着串联用1个99neighborhood再cancat（并联）直接使用999个neighborhood得到的特征。</p><p>效果上MSG慢但是精度高，MRG快但是精度不高。</p><p>注意：作者训练时使用了DropPoints（DP）技术（randomly dropping out input points with a randomized probability for each instance），但是model的density variation是否一部分得益于此（data的augmentation）不得而知。</p></li><li><p>训练流程</p></li></ol><p><img src="http://stanford.edu/~rqi/pointnet2/images/pnpp.jpg" alt></p><p>对于segmentation，还要对feature插值回原图的分辨率才能对每个点进行分类。</p><p>而classification则直接fc+softmax就好了。</p><h2 id="KCNet"><a href="#KCNet" class="headerlink" title="KCNet"></a>KCNet</h2><blockquote><p>CVPR2018《Mining Point Cloud Local Structures by Kernel Correlation and Graph Pooling》</p><p>paper下载：<a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Shen_Mining_Point_Cloud_CVPR_2018_paper.html" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2018/html/Shen_Mining_Point_Cloud_CVPR_2018_paper.html</a></p></blockquote><h3 id="概括-2"><a href="#概括-2" class="headerlink" title="概括"></a>概括</h3><ul><li><p>作者针对PointNet对local features提取能力弱的缺点（然而PointNet++速度慢），提出使用Kernel Correlation + Graph Pooling的方法来更有效地提取local features。</p></li><li><p>特点：Kernel的Shape可学习（并且可视化后有直观的几何意义）；不再使用handcrafted的features（如法向量、协方差矩阵）学习了</p></li><li><p>有益效果：在PointNet的基础上，能有效地提取local features</p></li><li><p>与PointNet++的比较：同样是局部化的特征提取，KCNet使用的是人为设计的度量函数，而PointNet++是自动提取特征（套用PointNet）；KCNet是对每一个点都提取了local feature，而PointNet++是只对FPS采样出来的点为centroid以及确定的邻域范围内使用PointNet来提取local feature。KCNet中有明显的模板/卷积核概念，而PointNet/PointNet++中没有明显的这一概念。</p></li><li><p>TODO IDEA：感觉point set的相似度衡量（correlation）过于依赖于选取，是否可能有自动学习这一过程，或者使这一过程更加自然？KCNet实现了Kernel，但是没有对应于stride的东西？是否可以多次地使用这种Kernel Layer而不是只用一次，类似于CNN的叠层使用一样？其中的高斯径向基的$\sigma$是否有自动学习的方法？</p></li></ul><h3 id="难点-2"><a href="#难点-2" class="headerlink" title="难点"></a>难点</h3><p>如何更加有效地提取local features？</p><p>PointNet++是将Point Cloud划分为多个分区，然后各个分区使用PointNet，以实现层次化的特征提取，那么KCNet又能怎么做呢？</p><p>PointNet的致命缺陷：PointNet只能感知点的是否存在，而不能编码/表征该点neighborhood的几何结构信息，造成局部描述能力弱。</p><h3 id="思想-2"><a href="#思想-2" class="headerlink" title="思想"></a>思想</h3><p>更加模仿CNN的做法（Learnable的PointSet模板匹配）</p><p>具体地说：</p><ul><li>我们有L个Kernel，每个Kernel有M个点，也就是说一个Kernel是一个point set，并且Kernel里点的位置都是learnable的，这就好比是我们有L个可变形的模板，或者说我们有L个可学习的“卷积核”。然后我们用数学方法人为地构造了一个叫KC(A,B)的度量函数用于度量point set A和point set B的相似程度，值越大越相似。然后这里的point set A就是Kernel，point set B就是我们model输入的point cloud里的其中一个点p的neighborhood的点集，也就是Kernel在整个point cloud上每一个点走过了一遍，因为有L个Kernel，所以一个点会得到一个L维的向量。以上就是Kernel Correlation的含义。</li><li>然后我们遍历每个点，基于该点的邻居的feature值，每个维度上取max，便是graph max pooling；将邻居的features作算术平均替换掉该点，便是graph average pooling。这就是graph pooling的含义。</li></ul><h3 id="额外的Knowledge"><a href="#额外的Knowledge" class="headerlink" title="额外的Knowledge"></a>额外的Knowledge</h3><ul><li>点云可能不止xyz坐标，或许还包括强度、法向量等信息，构成更高维的描述向量。点云只是描述Object surface的点，无Object内部的点。</li><li>处理点云的4种主流方法：<ul><li>volumetric-based：Volumetric-based approachpartitions the 3D space into regular voxels and apply 3D convolution on the voxels</li><li>patch-based：Patch-based approach parameterizes 3D surface into local patches and apply convolution over these patches</li><li>graph-based：Graph-based approach characterizes point clouds by graphs.</li><li>point-based：Pointbased approach such as PointNet directly operates on point clouds</li></ul></li></ul><h3 id="实现-2"><a href="#实现-2" class="headerlink" title="实现"></a>实现</h3><p><img src="3_2.png" alt></p><ol><li><p>kernel correlation layer（Learning on Local Geometric Structure）</p><p>所定义的度量函数为（我将论文的式子换了个形式好看些）：</p><script type="math/tex; mode=display">KC(K,p) = \frac{1}{|N|}\sum_{p_K\in K} \sum_{p_N\in N}K_\sigma(p_K,p_N-p)</script><p>其中$K$代表Kernel（里面有$M$个点，其中的点用$p_K$表示）；$N$代表点$p$的邻居（不包含点$p$），邻居点记作$p_N$；$|N|$表示点$p$邻居的个数。</p><p>其中的$p_N-p$就是表示相对坐标，暗示Kernel应该是以原点为centroid的。</p><p>其中$K_\sigma$选用高斯径向基函数，那么最相似时这个$KC(K,p)$的值就是$M$（最大值），最小值就是$0$。</p></li><li><p>graph-based pooling layer（Learning on Local Feature Structure）</p><p>作者借助了一个数据结构，他起名叫“KNNG（K最近邻图）”，用来存储每一个点的邻居是谁，使用KNN算法得出邻居成员。</p><p>图结构的Pooling起到了feature aggregation的作用，据作者声称加强了特征的鲁棒性。</p><ul><li><p>Graph max pooling就是neighborhood特征的最大值（各个通道上）</p></li><li><p>Graph average pooling的直观解释：就是neighborhood特征的算术均值。原文中描述比较复杂，直观地说就是这样：</p><p><img src="3_1.png" alt></p><p>值得注意的是，pooling只是以邻居的特征计算后赋给自己。不知道为什么不算上自己的特征？？</p></li></ul></li></ol><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>速度快（只比PointNet稍慢），精度还算高（虽然不是最高的）</p><p>鲁棒性（随机替换point为噪点）：graph max pooling比graph average pooling更鲁棒；单纯使用graph max pooling比结合着使用graph max pooling + kernel correlation鲁棒性能更好，只使用kernel correlation鲁棒性能最差。</p><p>注意：作者训练时没有用data augmentation的方法</p><h2 id="SO-Net"><a href="#SO-Net" class="headerlink" title="SO-Net"></a>SO-Net</h2><blockquote><p>CVPR2018《SO-Net: Self-Organizing Network for Point Cloud Analysis》</p><p>paper下载：<a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper.html" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2018/html/Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper.html</a></p></blockquote><h3 id="概括-3"><a href="#概括-3" class="headerlink" title="概括"></a>概括</h3><ul><li>针对一些网络在处理point cloud时的缺点，如：不能对点的空间分布进行建模（例如PointNet++，只是能获取局部信息不能得到局部区域之间的空间关系），提出了SO-Net。SO的含义是利用Self-organizing map的Net。</li><li>结果：它具有能够对点的空间分布进行建模、层次化特征提取、可调节的感受野范围的优点，并能够用于多种任务如重建、分类、分割等等。取得了相似或超过SOTA的性能，因为可并行化和架构简单使得训练速度很快。</li><li>贡献：<ul><li>We design a permutation invariant network - the SO-Net that explicitly utilizes the <strong>spatial distribution of point clouds</strong>.</li><li>With point-to-node kNN search on SOM, hierarchical feature extraction is performed with systematically <strong>adjustable receptive field overlap</strong>（可调节的、感受野之间有交叠）.</li><li>We propose a <strong>point cloud autoencoder as pre-training</strong> to improve network performance in various tasks.</li><li>Compared with state-of-the-art approaches, similar or better performance is achieved in various applications with <strong>significantly faster training speed</strong>.</li></ul></li><li>TODO IDEA：作者发现将CNN直接用于SOM图上性能不升反降，为什么（推测：可能是SOM的2D map并不是保持了原本的空间对应关系，可能nodes之间是乱序的，导致用conv2d时精度反而降低）？</li></ul><h3 id="难点-3"><a href="#难点-3" class="headerlink" title="难点"></a>难点</h3><p>如何对local regions之间的空间关系进行建模？举个例子，即怎么显式地知道region A在region B的左边？</p><h3 id="额外的Knowledge-1"><a href="#额外的Knowledge-1" class="headerlink" title="额外的Knowledge"></a>额外的Knowledge</h3><ol><li><p>该文Related Work里对点云处理的“综述”比较详尽，怒赞！！！</p></li><li><p>网上找的SOM简介：<a href="https://www.cnblogs.com/surfzjy/p/7944454.html" target="_blank" rel="noopener">【机器学习笔记】自组织映射网络（SOM）</a></p></li></ol><h3 id="思想-3"><a href="#思想-3" class="headerlink" title="思想"></a>思想</h3><ul><li>点云的空间分布编码：使用SOM中nodes自带的拓扑结构实现</li><li>交叠感受野且可调：使用“point-to-node kNN search”，使得感受野大概率交叠，并且感受野大小受超参数控制。</li><li>无序化：使用作者提出改进的SOM即“Permutation Invariant SOM”，使得SOM的result对输入数据的顺序弱化至无。</li></ul><h3 id="做法"><a href="#做法" class="headerlink" title="做法"></a>做法</h3><ol><li><p>Permutation Invariant SOM</p><p>使用2维的SOM。注意2维的SOM并不是指nodes的features的dimension是2，而是指nodes的拓扑排列是2维正方形地排列。实质上nodes的features的dimension可以是任意维度，当是三维时就好比是学习point cloud的“聚类中心”。</p><p>传统SOM不能真正实现对输入数据的顺序不依赖性，需要作出修改：</p><ul><li>Assign fixed initial nodes for any given SOM configuration</li><li>Perform one update after accumulating the effects of all the points.（batch update）</li></ul></li><li><p>Encoder Architecture</p><p>记号：</p><ul><li><p>point cloud（其元素简称point） $P=\{p_i\in \mathbb{R}^3,i=0,…,N-1\}$</p></li><li><p>SOM nodes（其元素简称node） $S=\{s_j\in \mathbb{R}^3,j=0,…,M-1\}$</p></li></ul><ol><li><p>point-to-node kNN search</p><p>在$S$中寻找$p_i$的k个最近邻：$p_i$的第$k$个邻居记作$s_{ik}$</p></li><li><p>normalization</p><p>将$P$中的点都进行normalization：$p_{ik}=p_i-s_{ik}$</p><p>那么每个点$p_i$都有k个被归一化后的版本。一共有N*k个这样被normalization后的点。</p></li><li><p>shared FC layer（作用：施加非线性，提取point features）</p><p>将$p_{ik}$输入一层FC，得到$p_{ik}^1$；若输入$l$层FC层则得到$p_{ik}^l$</p><p>shared的意思大概是对于N*k个这样的$p_{ik}$输入的都是同一个FC网络。</p><p>构造N*k个$p_{ik}$的用途：</p><ul><li>提取point features：每一个point都有3个feature vectors</li><li>形成感受野：每一个node大致有$int(\frac{k N}{M})$个feature vectors。利用这一性质，我们在后面用max pooling操作提取node features。</li></ul></li><li><p>channel-wise max pooling operation（作用：从point features中提取SOM node features）</p><p>注意这里是从MLP尾巴$p_{ik}^l$提取的max pooling。</p><p>这里$max()$函数的里面的元素的个数就是感受野的大小，可知感受野大小不是确定的，容易知道感受野大小的期望就是$\frac{k N}{M}$，所以说感受野是可调的，并且存在overlap。</p><script type="math/tex; mode=display">s_j^0 = \max(\{ p_{ik}^l ,\forall s_{ik}=s_j \})</script><p>原本$s_j$是3维向量，现在max pooling后额外得到了维度未知的向量$s_j^0$（维度取决于MLP最后一层的输出维度）。</p><p>node concatenation：然后可以将两者concat起来得到$[s_j,\;s_j^0]$，feature的维度更长了，融合了不同层次的信息。</p><p>注意到“shared FC layer + channel-wise max pooling operation”好比就是一个small PointNet，对“mini point cloud”进行编码。</p></li></ol></li></ol><p><img src="4_1.png" alt></p><p>网络结构如上。</p><p>总地来说就是，先计算SOM得到$s_j$，然后计算$p_{ik}$，接着将$p_{ik}$输入MLP得到变换后的$p_{ik}^l$，接着对$p_{ik}^l$作max pooling得到$s_j^0$，与原本的cancat使得node features“变长”（在图中表示就是那个网格拥有depth了），接着对SOM nodes作MLP再max pooling得到global feature。</p><p>segmentation与classification略去不说。</p><ol><li><p>Autoencoder</p><p>In this section, we design a decoder network to recover the input point cloud from the encoded global feature vector.</p><p><img src="4_2.png" alt></p></li><li><p>Experiments结果分析</p><p>略</p></li></ol><h2 id="Using-Local-Spectral-Graph-Convolution"><a href="#Using-Local-Spectral-Graph-Convolution" class="headerlink" title="Using Local Spectral Graph Convolution"></a>Using Local Spectral Graph Convolution</h2><blockquote><p>ECCV2018《Local Spectral Graph Convolution for Point Set Feature Learning》</p><p>paper下载：<a href="http://openaccess.thecvf.com/content_ECCV_2018/html/Chu_Wang_Local_Spectral_Graph_ECCV_2018_paper.html" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_ECCV_2018/html/Chu_Wang_Local_Spectral_Graph_ECCV_2018_paper.html</a></p></blockquote><h3 id="概括-4"><a href="#概括-4" class="headerlink" title="概括"></a>概括</h3><p>在局部图的区域上使用“图卷积”提取local features（将局部点图的图卷积转换到频域上进行操作），提出了fancy的池化操作（多次使用clustering+pooling进行的池化操作）。</p><h3 id="思想-4"><a href="#思想-4" class="headerlink" title="思想"></a>思想</h3><p>利用图卷积的方式取代PointNet进行local regions的features extraction。</p><p>提出使用先划分子区域（clustering）再pooling的池化方法，感觉像是分组卷积的思想？</p><h3 id="实现-3"><a href="#实现-3" class="headerlink" title="实现"></a>实现</h3><p><img src="5_1.png" alt></p><ol><li><p>使用FPS计算出centroids，kNN计算出local regions。</p></li><li><p>接着使用频域的图卷积，以及多次使用clustering+pooling进行池化。</p><p><img src="5_2.png" alt></p><p>频域图卷积过程如上图所示。</p><p>局部点图一共有k个点。其中：</p><p>$X$是输入特征矩阵（一共有k行，每一行是该点的特征向量，特征向量维度为m）</p><p>$G$是spectral modulation matrix，$W$是weights of the feature filter（好像是图的邻接矩阵？）</p><p>图的傅里叶变换：首先计算拉普拉斯矩阵$L=I-D^{-1/2}WD^{-1/2}$，然后对$L$作特征值分解得到特征向量矩阵$U$，那么正傅里叶变换就是$\tilde{X}=U^TX$，反傅里叶变换就是$X=U\tilde{X}$</p><p>池化：We partition the Fiedler vector to perform spectral clustering in a local k-NN. We alternate between max pooling and average pooling between different recurrences.</p></li><li><p>多次重复步骤2，最终获取特征用于分类/分割。</p></li></ol><h2 id="PointCNN"><a href="#PointCNN" class="headerlink" title="PointCNN"></a>PointCNN</h2><blockquote><p>NIPS2018《PointCNN: Convolution On X-Transformed Points》</p><p>paper下载：<a href="http://papers.nips.cc/paper/7362-pointcnn-convolution-on-x-transformed-points" target="_blank" rel="noopener">http://papers.nips.cc/paper/7362-pointcnn-convolution-on-x-transformed-points</a></p></blockquote><h3 id="概括-5"><a href="#概括-5" class="headerlink" title="概括"></a>概括</h3><p>作者声称提出了一种“generalization of typical CNNs to feature learning from point clouds”，因而称作“PointCNN”（我个人觉得人造痕迹比较多），并且接近/达到了SOTA的性能。</p><p>主要流程是：用kNN划分regions，然后对neighborhood的feature matrix进行transform，接着与kernel matrix进行conv；多层这样的“conv层”堆叠，最后得到特征输出。</p><p>但是需要注意的是：作者发现矩阵$X$并没有达到“permutation equivalence”的效果；而且作者训练时使用了augmentation的方法，包括shuffle输入数据点的顺序。</p><p>这里的“卷积”是真正传统意义上的卷积（元素间按位相乘再相加）；不同于KCNet提出的3D Kernel大法，不同于图结构的谱图卷积等等。</p><h3 id="思想-5"><a href="#思想-5" class="headerlink" title="思想"></a>思想</h3><p>想直接弄个conv类似的linear conv kernel，发现conv对输入数据的order sensitivity。于是想办法“纠正”输入数据的order，本想用“Permutation matrix”乘以它就好了，可能是ideal的Permutation matrix很难弄到，所以提出弄了个矩阵$X$，美言称能实现“simultaneously weight and permute the input features”，然后实在不知道怎么弄了，就让矩阵$X$自己从输入点集的坐标里学习吧（使用MLP），然后甚至还将点集坐标nonlinear transform后cancat进原本的features。最后施加矩阵$X$，最后卷积。</p><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>传统卷积（暂不考虑kernel的滑动，只考虑在一个location上）可以表示为（这里的conv实际上是matrices间element-wise的product，最后sum）：</p><script type="math/tex; mode=display">f = conv(K,[f_a,f_b,f_c,f_d]^T)</script><p>其中$K$表示卷积核，$f_a$等表示像素的特征向量（横向量，$f_a\in \mathbb{R}^{C_1}$）。上式表示2*2的kernel在一个location上的computation。</p><p>推广开来，在点云上，假设有4个points，然后做“卷积”，式子跟上面是一样的。但是对点云的输入顺序敏感，即$[f_a,f_b,f_c,f_d]^T$与$[f_b,f_a,f_c,f_d]^T$的结果会不一样，所以我们需要对它进行transform再输进去。我们这里选择用一个矩阵$X$与它相乘（矩阵有“置换矩阵”可以实现对输入顺序的矫正，推广到一般矩阵则相当于“置换+加权”的效果）。即：</p><script type="math/tex; mode=display">f = conv(K,X \times [f_a,f_b,f_c,f_d]^T)</script><p>这个矩阵$X$是由输入数据通过MLP生成的。</p><p>在PointCNN中，一次computation（只在一个location上，不“滑动”）的过程就是：</p><script type="math/tex; mode=display">F_p = conv(K,MLP(P-p) \times [MLP_\delta(P-p),F])</script><p>其中$K$是kernel matrix，$P$是输入点集的空间坐标（在region中，如果kernel覆盖了4个点，那么$P$的元素个数就是4），$p$是输出点的空间坐标（类似于该region的centroid，只有一个，$p\in P$），$F$是输入点集的feature matrix（与$P$的元素个数相同），$MLP()$和$MLP_\delta()$是多层感知机函数。</p><p>handcrafted的流程解释：</p><ul><li><p>转化到相对坐标系中$P-p$</p></li><li><p>矩阵$X$通过输入点集的空间坐标学习得来：$X=MLP(P-p)$</p></li><li>将输入点集维度升高，并与点集的特征向量进行cancat，作为最终的特征向量：$[MLP_\delta(P-p),F]$</li><li>进行“置换+加权”：$MLP(P-p) \times [MLP_\delta(P-p),F]$</li><li>最后进行常规的卷积操作得到$f$</li></ul><h3 id="实现-4"><a href="#实现-4" class="headerlink" title="实现"></a>实现</h3><ol><li><p>Hierarchical Convolution</p><p><img src="6_1.png" alt></p><p>模拟CNN，作者对图结构进行information的aggregation。</p></li><li><p>X-Conv Operator</p><p>作者的卷积结构称作“X-Conv”。我觉得直接从点云坐标学习得到矩阵$X$似乎是个很难的任务，感觉$X$矩阵并没有达到预期目的。</p><script type="math/tex; mode=display">F_p = conv(K,MLP(P-p) \times [MLP_\delta(P-p),F])</script></li><li><p>PointCNN Architectures</p><p><img src="6_2.png" alt></p><p>通过多层的堆叠实现特征提取。</p></li></ol><h2 id="A-CNN"><a href="#A-CNN" class="headerlink" title="A-CNN"></a>A-CNN</h2><blockquote><p>CVPR2019《A-CNN: Annularly Convolutional Neural Networks on Point Clouds》</p><p>paper下载：<a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Komarichev_A-CNN_Annularly_Convolutional_Neural_Networks_on_Point_Clouds_CVPR_2019_paper.html" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_CVPR_2019/html/Komarichev_A-CNN_Annularly_Convolutional_Neural_Networks_on_Point_Clouds_CVPR_2019_paper.html</a></p></blockquote><h3 id="概括-6"><a href="#概括-6" class="headerlink" title="概括"></a>概括</h3><p>使用循环卷积作为点云的特征提取方式：对点集投射到2D平面上进行圆周排序再输入到循环卷积中提取特征，输出的点的数量是上一级的centroids数量，多级嵌套实现“层次化”特征提取。</p><p>效果：提取局部几何结构特征（感觉是很明显的人工设计流程），adapt to the geometric variability and scalability at the signal processing level（文中没论述，不知道怎么看出来的），超过了SOTA（其实只是部分指标超过了SOTA）</p><p>BTW，文章Related Work部分的综述部分写的不错</p><p>TODO IDEA：为什么基本都需要kNN，可以不用吗？是否可以以平面极坐标的形式对投影p_j进行了排序后进行输入，而不需要划分ring再cos angle sort？？或者多个rings的特征组成更高维的tensor再滑动卷积？</p><h3 id="思想-6"><a href="#思想-6" class="headerlink" title="思想"></a>思想</h3><p>其实Annular Convolution的整个目的是为了实现neighborhood的<strong>无序化</strong>。在同一个ring上的确实现了无序化（从3D space project到2D plane，然后用angle的方式一一映射到1D line，最后使用1D circular convolution即可实现邻域特征的融合；同时ring的半径对应了感受野）</p><p>模仿PCA将points变动最大的方向保留（向切平面投影），感知centroid在距离远近上的特征（例如城市的内环特征、中环特征、外环特征分别是繁荣、住宅区、郊区这样子；再将这些“环”特征concat起来，作为这座城市的特征），而在“环”区域里面的顺序则是通过余弦角排序后输入循环卷积里面实现的（思路不错！）</p><p>主要创新点：将邻居点投射到2D平面上，然后对2D平面划分出多个不重叠的区域（它是同心圆环区域），然后指定一个标准点，与其计算余弦夹角后排序，从而得到点的圆周顺序排列，输入到循环卷积中提取特征；不同同心圆环的特征concat起来，再在所有邻居点之间作max pooling。</p><h3 id="实现-5"><a href="#实现-5" class="headerlink" title="实现"></a>实现</h3><ol><li><p>Regular and Dilated Rings on Point Clouds And Constraint based KNN Search</p><p><img src="7_1.png" alt></p><p>将平面空间划分为不重叠的区域，不重叠会减少计算量和避免特征冗余。如果是普通的ball query则大的感受野必然会包含小的感受野。作者使用环形的方式，甚至模拟空洞卷积使用带间隔空环的方式划分平面区域（增大了感受野）。最上面的圆形平面代表平面区域划分，中间和最下面的平面代表将最上面的解体开给你看的，说明没有作者提出的划分方式（Rings）没有重叠。</p><p>Constraint based KNN Search：只是将区域受限，进行的KNN算法。例如原本是$r&lt;R$范围内的，现在可能是$R_1&lt;r&lt;R_2$范围内的。</p><p>centroid使用FPS最远点采样算法获得，邻居使用Constraint based KNN Search获得。</p><p>做完这一步我们就可以得到多个centroid和其对应的neighborhood了（区域划分完成）。</p></li><li><p>Ordering Neighbors</p><ol><li><p>Normal Estimation on Point Clouds</p><p>首先估计centroid的法向量：首先沿着法向量，邻居的位置变化肯定是最小的。所以法向量就是邻居点的相对坐标的协方差矩阵最小的特征值所对的特征向量的方向。</p><p>似乎法向的方向颠倒之后得到的排序后的点的顺序也会相反，但好像只要使得法向始终与某一方向的夹角小于90°就好了？？？</p><p>有了法向量就可以估计切平面。</p></li><li><p>Orthogonal Projection</p><p>将邻居点向切平面投影，得到位于切平面上的投影点$p_j,k\in\{1,…,K\}$。</p></li><li><p>Counterclockwise Ordering</p><p>随便指定一个投影点作为参考点，例如就选$p_1$吧。记centroid为$q$。</p><p>那么以此计算$p_j-q$与$p_1-q$夹角的“余弦值”（借助叉积可以辨别是位于圆的上半部分还是下半部分，从而可以拓展到0~360°的识别范围，所以“余弦值”被我们拓展到$(-3,1]$的区间上了），将“余弦值”降序排列，那么就能对邻居点实现逆时针的排序了</p></li></ol></li><li><p>Annular Convolution on Rings And Pooling on Rings</p><p><img src="7_2.png" alt></p><p>将排序后的邻居点$[x_1,x_2,…,x_K]$输入到循环卷积里进行特征提取。</p><p>如果卷积核的大小是3，那么循环卷积相当于对$[x_1,x_2,…,x_K,x_1,x_2]$进行卷积核为3的普通卷积。</p><p>因为采用了循环卷积，所以参考点（$p_1$）到底是谁就不重要了。</p><p>将不同rings的特征concat，然后在所有邻居点之间做maxpooling。然后再把得到的东西再次送入类似的结构作“层次化提取”。</p></li><li><p>A-CNN Architecture</p><p>整体结构如下：</p><p><img src="7_3.png" alt></p><p>它的Segmentation Network上采样插值部分与之前某个网络有点类似（The interpolation method is based on the inverse squared Euclidean distance weighted average of the three nearest neighbors），不再详述。</p></li></ol><h2 id="PointConv-👍"><a href="#PointConv-👍" class="headerlink" title="PointConv 👍"></a>PointConv 👍</h2><blockquote><p>CVPR2019《PointConv: Deep Convolutional Networks on 3D Point Clouds》</p><p>paper下载：<a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Wu_PointConv_Deep_Convolutional_Networks_on_3D_Point_Clouds_CVPR_2019_paper.html" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_CVPR_2019/html/Wu_PointConv_Deep_Convolutional_Networks_on_3D_Point_Clouds_CVPR_2019_paper.html</a></p></blockquote><h3 id="概括-7"><a href="#概括-7" class="headerlink" title="概括"></a>概括</h3><p>这篇文章将卷积比较自然地拓展到点云的情形，思路很赞！</p><p>文章的主要创新点：“weight function”和“density function”，并能实现translation-invariance和permutation-invariance，可以实现层级化特征提取，而且能自然推广到其deconvolution的情形实现分割，在二维CIFAR-10图像分类任务中精度堪比CNN（表明能够充分近似卷积网络），达到了SOTA的性能。</p><p>缺点：每个kernel都需要由“kernel function”生成，而“kernel function”实质上是一个CNN网络，计算量比较大。</p><h3 id="思想-7"><a href="#思想-7" class="headerlink" title="思想"></a>思想</h3><p>察觉到：二维卷积中pixel的相对centroid位置与kernel vector的生成方式有关。</p><p>以二维卷积为例说明一下如何将卷积拓展到点云。这里只考虑使用一个kernel在一个location的一次卷积操作。</p><p>对于二维图像，我们可以将图像的pixels看作是一个点，那么图像就是整齐排列的点阵。每个point都有维度为$C_{in}$的特征向量（相当于图片是多通道的，一个通道对应于特征向量里的一个位置）。在二维卷积中，我们的kernel是“滑动”的，即我们在多个location的计算中共用一个kernel参数。这是为什么呢？我们如果认为这种共享参数的形式得益于整齐排列的points，那么这就是我们的PointConv的出发点了。</p><p><img src="8_1.png" alt></p><p>现在我们考虑一个point位置上的kernel是什么，在传统2D卷积中，这个值就是一个vector（$C_{in}$尺寸），如果points是正方形（2*2）整齐排列，那么我们可以认为kernel是$2\times2\times C_{in}$的尺寸。但是我们现在不这么认为，我们认为kernel的尺寸是$K\times C_{in}$（其中$K$是输入点的个数，这里是4），这样有助于推广到点云的情形。</p><p>在2D图像中，我们不妨可以认为kernel是这样生成的：kernel在一个point上的值（vector），取决于point的位置。那么在2D图像中，这个生成vector的分布，则是4点的狄拉克分布，即在空间坐标中只在这4个位置下，才有对应的向量值。</p><p>我们推广开来，假设这个分布是实数值的连续分布，不妨叫它做“weight function”，那么在3D点云中，这个“weight function”的输入是3D坐标系x、y和z，它的输出则是一个vector（尺寸$C_{in}$）。（在2D图像中，则$[f(0,0),f(1,0);f(0,1),f(1,1)]$组成了我们三维的kernel，尺寸$2\times2\times C_{in}$）。如果点的坐标是实数值（3D点云中其实就是实数值），那么我们点云的kernel尺寸就是$4\times C_{in}$，即我们每个point都有一个vector kernel与它对应。我们的计算过程就是：每个point的的feature与kernel vector做点积，最后把所有邻居点的点积的结果求和，得到的scalar就是我们一次卷积计算后的结果。</p><p>如果每个point的位置上是一个矩阵，那么相当于有多个kernel，最后得到的卷积结果不是scalar而是一个vector（有点像2D卷积中有多个卷积核的情形，输出就形成了多个channels）。</p><h3 id="实现-6"><a href="#实现-6" class="headerlink" title="实现"></a>实现</h3><ol><li><p>PointConv</p><p>在邻居点中实现卷积：</p><script type="math/tex; mode=display">F_{out} = \sum_{k=1}^K \sum_{c_{in}=1}^{C_{in}} S(k)W(k,c_{in})F_{in}(k,c_{in})</script><p>其中$W\in\mathbb{R}^{K\times(C_{in}\times C_{out})}$是$C_{out}$个kernel，$F_{in} \in \mathbb{R}^{K\times C_{in}}$是邻居的特征矩阵（一行一个），$S$是点密度的修正项（density scale），$F_{out}$是卷积后输出的vector。其中的kernel由“kernel function”生成，而“kernel function”接收x、y和z坐标输入，然后通过2D卷积（用1x1卷积核），得到一个尺寸为$K\times C_{in}\times C_{out}$的tensor，就是$W$。输出的$F_{out}$的空间坐标就是centroid的坐标。</p><p><img src="8_2.png" alt></p><p>关于密度：作者首先使用“kernel density estimation”算法估计每一个点的密度，然后将得到的密度（一个点的密度是一个实数值，有$K$个点所以是一个1d vector）丢进MLP里非线性变换（丢进MLP里是为了让网络自己决定是否采用密度估计），估计密度是为了解决点云的非均匀采样导致的非均匀密度的问题。</p><p>作者发现原始版本的PointConv实现内存占用太大，提出了改进版本，在此不详述。</p><p>PointDeconv其实就是邻域3点线性插值上采样，与前面同级的特征concat，再使用PointConv。</p><p><img src="8_3.png" alt></p><p>上图是用于segmentation的pointconv网络。特征提取的过程好像也是先用FPS采样再grouping的。</p></li></ol><h2 id="KPConv👍"><a href="#KPConv👍" class="headerlink" title="KPConv👍"></a>KPConv👍</h2><blockquote><p>ICCV2019《KPConv: Flexible and Deformable Convolution for Point Clouds》</p><p>paper下载：<a href="https://arxiv.org/abs/1904.08889v2" target="_blank" rel="noopener">https://arxiv.org/abs/1904.08889v2</a></p></blockquote><h3 id="概括-8"><a href="#概括-8" class="headerlink" title="概括"></a>概括</h3><p>与“PointConv”和“KCNet”稍微有点类似，它也是将卷积自然推广到3D point cloud中，但是区别是生成kernel transform matrix的方式不一样，而且它的point kernel并不是用于直接度量点云之间的相似度而是用于建立分布来计算kernel transform matrix的值。简单地说是通过线性插值来得到矩阵的，而系数与点之间的距离相关。</p><p>效果：达到SOTA，可分类又可分割</p><p>注：</p><ul><li><p>KPConv == Kernel Point Convolution</p></li><li><p>我这里的kernel transform matrix指的是给定一个邻域内的点$y_i$（相对坐标），通过某一种方法得到该点相关联的特征向量$f_i$的转换矩阵$W$，其$W$就是kernel transform matrix。在一个位置上的卷积计算过程就是$output = \sum_i W_i y_i$，即：</p><script type="math/tex; mode=display">(\mathcal{F}*\mathcal{g})(x) = \sum_{x_i\in\mathcal{N}_x} g(x_i-x)f_i</script></li></ul><p>TODO IDEA：还有没有其他方法来生成这个分布？找邻居的方法是否可以更加自然一点，使得嵌入神经网络的架构中？</p><h3 id="思想-8"><a href="#思想-8" class="headerlink" title="思想"></a>思想</h3><p>与“PointConv”类似，区别是它生成kernel transform matrix的方式不一样。在前篇”PointConv”中，生成方式是使用CNN合成，输入xyz坐标自动生成一个矩阵。在这篇“KPConv”则是利用类似于插值或者说平滑的方法来计算kernel transform matrix。</p><p>那么这个“插值”的算法是怎么计算的呢？这套算法需要我们首先定义一个小点云，小点云里面有$K$个点（$K$是任意的，可以不等于邻域内的邻居数，$K$越大则描述的分布越精细），而小点云里的每个点$d_k$都有个矩阵$W_k$与之关联。现在我们考虑输入点云的centroid的邻域内的一个邻居点$x$，那么计算得到的矩阵则是在小点云周围点的矩阵$W_k$的线性表示：</p><script type="math/tex; mode=display">transform\;matrix = \sum h(d_k,x)W_k</script><p>其中的：</p><script type="math/tex; mode=display">h(d_k,x) = max(0,1-\frac{||d_k-x||}{\sigma})</script><p><img src="9_1.png" alt></p><p>意思是这个分布实质上只是由$K$个“狄拉克分布”支撑起来的，只有在这$K$个地方才有真正的值。想要获取其他地方的值就只能通过线性插值得到。线性插值的“原子”则是与之离得比较近的点所对应的矩阵$W_k$，离得太远则会由于ReLU函数的截断性质而不再作出贡献。</p><p><img src="9_4.png" alt></p><p>上图是假设kernel points只有5个，黑色星星的位置是kernel points的位置，假设weight matrix只是一个scalar，使用KPConv的插值算法得到的分布图。画图的代码见<a href="9_code_to_plot_distribution.py">这里</a>。可以看出这个算法的效果就是：相当于允许邻居点neighbor point的位置有稍微的变动，其理想的等值面是一个球形，在波动允许的范围内则使用一个接近的transform matrix值，而没有受到影响的地方则默认是零。这样带来的好处可能是使得分布平滑（猜的），使用CNN生成的方式应该不能保证分布平滑（瞎猜的）。</p><h3 id="实现-7"><a href="#实现-7" class="headerlink" title="实现"></a>实现</h3><ol><li><p>Rigid or Deformable Kernel</p><p>在“思想”中介绍的就是Rigid Kernel，意思是kernel point的位置一开始就是固定好的，是不可改变、不可学习的参数，改变的只能是其小点云上的矩阵$W_k$。Rigid Kernel的初始化方式形象地说就是假想每个kernel point都是带正电的小球相互排斥，但是同时要使得小球被束缚在一个半径尽可能小的包围球里面，包围住全部的带点小球，那么最终稳态时的小球位置就是kernel point的坐标位置。</p><p>Deformable Kernel并不是指kernel point的位置可变，然后最后共用同一个分布，因为这样没什么意义，还不如选用好的初始化方法。</p><p>而Deformable Kernel则参照了二维图像中的deformable convolution的实现方法，就是在Rigid Kernel的基础上学习一个kernel point shift的量，在固定的基础上进行变动，从而实现这$K$个“狄拉克分布”的位置都是可变的。</p><p><img src="9_2.png" alt></p><p>在使用Deformable Kernel的时候，需要在损失函数上加上正则项，用来约束kernel point和neighbor point不能离太远、kernel points之间不能太近，否则效果奇差。</p></li><li><p>网络结构</p><p><img src="9_3.png" alt></p><p>用grid subsampling下采样，用max-pooling或KPConv来做pooling</p><p>用radius neighborhoods而不是k-nearest-neighbors</p></li></ol><h2 id="DGCNN-using-EdgeConv👍"><a href="#DGCNN-using-EdgeConv👍" class="headerlink" title="DGCNN using EdgeConv👍"></a>DGCNN using EdgeConv👍</h2><blockquote><p>TOG2019？《Dynamic Graph CNN for Learning on Point Clouds》</p><p>paper下载：<a href="https://arxiv.org/abs/1801.07829" target="_blank" rel="noopener">https://arxiv.org/abs/1801.07829</a></p></blockquote><h3 id="概括-9"><a href="#概括-9" class="headerlink" title="概括"></a>概括</h3><p>将传统卷积以另一种视角拓展到点云中来，称作EdgeConv。由EdgeConv组成的网络叫DGCNN。因为EdgeConv的图是动态计算的，所以其神经网络叫做Dynamic Graph CNN (DGCNN)。使用EdgeConv具有以下特点：聚合局部几何信息、EdgeConv模块可以级联使用、语义的长距离感知。并且达到了SOTA。</p><p>EdgeConv： EdgeConv能捕获局部几何结构，并且顺序无关。EdgeConv通过生成描述点与点之间的“边特征”（edge feature）来进行卷积计算。每当前向传播时，KNN图都会被再次计算（因为特征向量被改变，导致邻居也会变），称之为Dynamic Graph。</p><p>其本质上是使用graph结构vertex之间的connectivity关系，实现information通过edge来与vertex交互更新。</p><p>EdgeConv与PointConv的异同大概是：</p><ul><li>PointConv中，neighborhood与centroid的空间相对关系编码在distribution中；neighborhood transform也是编码在distribution中。信息传递显式地通过distribution的超距作用来交互。</li><li>EdgeConv中，neighborhood与centroid的空间相对关系编码在由MLP实现的$h(\cdot,\cdot)$中；neighborhood transform也是编码在由MLP实现的$h(\cdot,\cdot)$中。信息传递显式地依靠连通的edge来交互。</li></ul><p>TODO IDEA：能否不用KNN图？能否逐步减少点的数量？</p><h3 id="思想-9"><a href="#思想-9" class="headerlink" title="思想"></a>思想</h3><p>首先，有一个图$G$，它的每个顶点上有一个feature。下图是点$x_i$的KNN图。</p><p><img src="10_3.png" alt></p><p>然后我们利用“点特征”计算“边特征”：设这条边相邻的两个顶点的特征分别为$x_i$和$x_j$，那么“边特征”：（其中$h(\cdot,\cdot )$采用MLP实现）</p><script type="math/tex; mode=display">e_{ij} = h(x_i,x_j-x_i)</script><p>每个边都由此计算得到它的边特征了（注：边的集合记作$\mathcal{E}$），现在我们利用“边特征”更新得到输出的“点特征”：</p><script type="math/tex; mode=display">x_i^{'} = \max_{j:(i,j)\in \mathcal{E}} e_{ij} = \max_{j:(i,j)\in \mathcal{E}} h(x_i,x_j-x_i)</script><p>其中$x_i$（全局坐标）的输入变量编码了global shape structure，$x_j-x_i$（相对坐标）编码了local neighborhood information。</p><p>其中要求$h(\cdot,\cdot)$函数$\mathbb{R}^F \times \mathbb{R}^F \rightarrow \mathbb{R}^{F^{‘}}$是非线性函数，聚合操作$\max$可以是任何的对称操作符（例如$\sum$也可以）。不难知道，当$h(\cdot,\cdot)$函数是$h(x_i,x_j)=\theta_{j \; relative \; to \;i} x_j$时，聚合操作是$\sum$时，就化成普通的卷积了。</p><h3 id="实现-8"><a href="#实现-8" class="headerlink" title="实现"></a>实现</h3><ol><li><p>Edge Convolution</p><p>“思想”中的方法就是Edge Convolution的实现方式。</p><p>值得注意的是，经过Edge Convolution处理后，点的数量不变，可改变的只能是该点特征向量的维度。</p><p>性质：Permutation Invariance、“partial” translation invariance</p></li><li><p>Dynamic graph update</p><p>假设有两个Edge Convolution模块，输入特征维度是3，处理后的维度是64，再次处理后的维度是128。那么Dynamic graph update的意思是指：对3维的point cloud计算KNN图$G_1$，第一个EdgeConv根据$G_1$计算得到64维的输出。对64维的point cloud再次计算KNN图$G_2$（这次是在64维的空间中），然后第二个EdgeConv依据$G_2$计算得到128维的输出。以此类推。</p><p>所以动态图计算有助于算法自动构建合适的图结构来帮助优化问题。</p></li><li><p>实现</p><p><img src="10_2.png" alt></p><p>注释：</p><ul><li>$\bigoplus$是concat算符。</li><li>n是点云的点数，k是对于一个点的邻居数</li><li>图的上方分别是classification network和segmentation network。</li><li>左下角的网络起到类似于PointNet中的T-Net的作用，即align an input point set to a canonical space</li><li>右下角就是EdgeConv模块的实现方式。$h(\cdot,\cdot)$函数采用MLP实现，MLP的第$i$层的结点数记作$a_i$，输出的结点数为$a_n$（这里的n并不是指点云中点的数目）。pooling操作意指在k个邻居中max pooling。</li></ul></li></ol><h3 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h3><p>   不同层级提取到的特征（从左到右依次层级递增），通过指定一个点（红色点），然后计算其他点在该层级中与红点的距离来可视化，距离越近（代表在KNN图中是比较相邻的顶点）颜色越黄。可见该算法可以提取比较长距离的特征。</p><p>   <img src="10_1.png" alt></p><h2 id="Mo-Net"><a href="#Mo-Net" class="headerlink" title="Mo-Net"></a>Mo-Net</h2><blockquote><p>？？？《Mo-Net: Flavor the Moments in Learning to Classify Shapes》</p><p>paper下载：<a href="https://arxiv.org/abs/1812.07431" target="_blank" rel="noopener">https://arxiv.org/abs/1812.07431</a></p></blockquote><h3 id="概括-10"><a href="#概括-10" class="headerlink" title="概括"></a>概括</h3><p>提出使用几何矩的方式来提升输入点云的维度（$\mathbb{R}^3 \rightarrow \mathbb{R}^9 $），以捕获更多信息，在降低计算复杂度和内存消耗的同时提升精度。</p><p>注意：这篇paper主要是与PointNet相对比的；讨论的是点云的classification问题。</p><h3 id="实现-9"><a href="#实现-9" class="headerlink" title="实现"></a>实现</h3><p>注意，这里的MLP是shared的，只有一个MLP网络。</p><p><img src="11_1.png" alt></p><ol><li><p>使用几何矩</p><p>点云原本的输入是$(x,y,z)$，现在变成了$(x,y,z,x^2,y^2,z^2,xy,yz,xz)$</p></li><li><p>双重池化</p><p>将max pooing与avg pooling的结果进行concat</p></li></ol><h3 id="效果-1"><a href="#效果-1" class="headerlink" title="效果"></a>效果</h3><p><img src="11_3.png" alt><br>比PointNet的精度稍微高上一点点</p><p><img src="11_2.png" alt><br>MLP的层数可以大大减少，使得计算复杂度降低，内存消耗量降低</p><h2 id="Tangent-Convolutions"><a href="#Tangent-Convolutions" class="headerlink" title="Tangent Convolutions"></a>Tangent Convolutions</h2><blockquote><p>CVPR2018《Tangent Convolutions for Dense Prediction in 3D》</p><p>paper下载：<a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper.html" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2018/html/Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper.html</a></p></blockquote><h3 id="概括-11"><a href="#概括-11" class="headerlink" title="概括"></a>概括</h3><p>提出了“Tangent Convolutions”，它本质是切平面上的卷积。将对point cloud feature的卷积转变为对image的卷积。</p><p>优点：适用于大规模场景（数百万个points）、在大规模场景可以precomputation一部分、在大规模场景中达到SOTA。</p><h3 id="思想-10"><a href="#思想-10" class="headerlink" title="思想"></a>思想</h3><p>将3D point cloud卷积转化/投射到2D平面的卷积。</p><p>即：将3D的$\sum_{q\in Neighborhood} point_feature(q)^T vector_kernel(q)$卷积转换到2D的平面卷积（卷积核等于输入图的大小，不滑动）</p><p>将邻居点project到切平面上，通过插值的方式产生位于切平面上的tangent image，然后使用平面卷积来卷积tangent image。</p><p>tangent image编码了局部的空间几何关系、以及原本作用在点云中的特征提取操作$F(q) $，现在对其平面卷积相当于对空间几何关系的operation、以及对$F(q) $的convolution。</p><h3 id="实现-10"><a href="#实现-10" class="headerlink" title="实现"></a>实现</h3><ol><li><p>Tangent Convolution</p><p>不仅是点云，只要3D data支持法向量估计（例如mesh），就能使用“Tangent Convolution”</p><p>Notation：</p><ul><li>点云$\mathcal{P}=\{p\}$</li><li>定义在点云$\mathcal{P}$上的函数$F(p)$（它可以描述该point的颜色/几何/或者其他任何特征）</li><li>点$p$的tangent plane $\pi_p$</li><li>continuous tangent image $S(u)$，其中$u$是tangent plane上的任意一点（可以不是投影点）</li></ul><p>Motivation：</p><ul><li>将离散定义域的$F(p)$连续化，才能对$F(p)$卷积</li></ul><p>Method：</p><ul><li><p>给定点云中的一个点$p$，用$||q-p||&lt;R$的方法选定其邻居$q$，用特征分解的方法来估计法向量$n_p$</p><p><img src="12_2.png" alt></p></li><li><p>将邻居$q$投射到切平面得到投影点$v$（其集合记作$V$，注意$p \notin V$），我们规定$S(v)=F(q)$（即在某几个离散位置值相等），然后其他位置使用插值的方法得到，我们使用最近邻插值：</p><script type="math/tex; mode=display">S(u) = S(g_{plane}(u)) = F(g_{cloud}(u))</script><p>其中$g_{plane}(u)$表示从$V=\{v\}$中选出与$u$最近的点，$g_{cloud}(u)$表示$g_{plane}(u)$在点云中原本的点</p><p>在平面上原本只有几个离散的位置有值（几个狄拉克分布），现在被插值到整个平面上都有值了。</p></li><li><p>那么在平面$\pi_p $上的卷积就是：（$p$生成$\pi_p $，所以讨论$X(p)$）</p><script type="math/tex; mode=display">X(p)=\sum_u c(u)S(u) = \sum_u c(u)F(g_{cloud}(u))</script><p>其中$c(u)$是卷积核。其中$g_{cloud}(u)$这项仅与点云有关，可以事先被计算。</p><p>实际上处理时，tangent image被处理成$l \times l$大小的2D网格图像。那么$u$相当于像素，$c(u)$相当于一个与tangent image同大小的卷积核。</p></li></ul></li><li><p>Implement a convolutional layer using tangent convolutions</p><p>不能算是真正意义上的“卷积层”，只能算是一个module吧</p><p>效果：点的数量不变，但是点的特征的维度改变</p><p>Notation：</p><ul><li><p>the number of points in point cloud: $N$</p></li><li><p>input feature map: $F_{in}$ of size $N \times C_{in}$</p></li><li><p>weight/flatten kernel: $W$ of size $1 \times L$（应该是有$C_{out}$个，尺寸应该是$C_{in}\times L\times 1$）</p></li><li><p>output feature map: $F_{out}$ of size $N \times C_{out}$</p></li><li><p>flatten the tangent image or kernel to 1D size: $L=l^2$</p></li></ul><p>Implement：</p><p><img src="12_1.png" alt></p><ul><li>预计算$g_{cloud}(u) $函数：对于每个来自point cloud中的点$p$，都对应了一副tangent image $S(u)$；对于每个$u\in Grid(l \times l)$，都计算一次$g_{cloud}(u) $的值。于是形成了index matrix$I$（shape: $N \times L$），记录的是点云中点的索引。</li><li>使用$F_{in}$矩阵将索引拓展开，得到tensor $M$，即每个点云中的点的索引值都被检索成其输入feature。$F(\cdot)$函数的作用是从点获取点的特征（from index to $C_{in}$ dimension feature），所以相当于作用了$F(\cdot)$函数。举例：point index为3，通过$F_{in}$查到$F_{in}(3,:)$的feature，相当于把$1$长度展开成$C_{in}$长度。</li><li>用$C_{out}$个卷积核$W$卷积$M$得到输出$F_{out} $</li></ul></li><li><p>Additional Ingredients</p><p>pooling：采用划分3D grid的方法，落在同一个grid的，坐标平均，特征向量平均</p><p>unpooling：将聚合点平均拷贝给其他落在同一个grid的点</p><p>Local distance feature：也要将邻居点$q$到切平面$\pi_{p} $的距离cancat到$q$的特征里</p></li><li><p>Architecture</p><p><img src="12_3.png" alt></p><p>其中的卷积使用tangent convolution，pooling和unpooling采用Additional Ingredients提及的方法。</p></li></ol><h2 id="Mixture-Model-Networks（MoNet）👍"><a href="#Mixture-Model-Networks（MoNet）👍" class="headerlink" title="Mixture Model Networks（MoNet）👍"></a>Mixture Model Networks（MoNet）👍</h2><blockquote><p>CVPR2017《Geometric Deep Learning on Graphs and Manifolds Using Mixture Model CNNs》</p><p>paper下载：<a href="http://openaccess.thecvf.com/content_cvpr_2017/html/Monti_Geometric_Deep_Learning_CVPR_2017_paper.html" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2017/html/Monti_Geometric_Deep_Learning_CVPR_2017_paper.html</a></p></blockquote><h3 id="概括-12"><a href="#概括-12" class="headerlink" title="概括"></a>概括</h3><p>在非欧结构数据（non-Euclidean structured data）的几何深度学习（geometric deep learning）问题上，提出了在空间域上将传统CNN泛化到非欧几何数据（如graph、manifold）的统一的框架（MoNet），使得以往的模型设计都只是此框架内的特例，并且达到SOTA的效果。</p><p>此外本文还粗略地介绍了“Deep learning on graphs”（如Spectral CNN、Smooth Spectral CNN、Chebyshev Spectral CNN （ChebNet）、Graph convolutional network （GCN）、Diffusion CNN （DCNN））和“Deep learning on manifolds”（如Geodesic CNN （GCNN）、Anisotropic CNN （ACNN））</p><p>注：MoNet == mixture model networks</p><p>关键词：流形、图结构（graphs in the spectral domain）</p><p>其他：</p><ul><li>spectral CNN 只能适用于该图本身，不能迁移到其他图（domain-dependent）中使用。因为它的basis不同</li><li>基于空间域方法的优点是可以泛化到其他domain中去</li><li>本文提出的方法在manifold和graph上进行统一，有统一的解释性</li></ul><h3 id="实现-11"><a href="#实现-11" class="headerlink" title="实现"></a>实现</h3><p>Notation:</p><ul><li>a point on a manifold or a vertex of a graph： $x$</li><li>a neighborhood of point $x$： $y\in \mathcal{N(x)}$</li><li>d-dimensional vector of pseudo-coordinates： $u(x,y)$（可以粗略理解为相对坐标，或$e_{ij}$上的d维向量）</li><li>weighting functions： $w_1(u),\;w_2(u),\;…,\;w_J(u)$（each function with some learnable parameters）</li><li>get feature function： $f(y)$（获取该点的feature vector）</li></ul><p>The <strong>patch operator</strong> can therefore be written in the following general form（若$f(y)$是vector则计算结果也是vector，因为$w_j(u) $是一个scalar），这一步可以理解为“有J种聚合邻域信息的方式”：</p><script type="math/tex; mode=display">D_j(x)f = \sum_{y\in \mathcal{N(x)}} w_j(u(x,y))f(y),\quad j=1,2,...,J</script><p>A spatial generalization of the <strong>convolution</strong> on non-Euclidean domains is then given by a template-matching procedure of the form：（$g_j$是scalar，$(f*g)(x)$的输出类型与$D_j(x)f$相同），这一步可以i理解为“将这J种聚合到的信息进行线性加权”</p><script type="math/tex; mode=display">(f*g)(x) = \sum_{j=1}^J g_j D_j(x)f</script><p>所以关键问题是如何选取权重函数$w(u)$和伪坐标$u$。其中$w_j(u)$我们选用高斯函数（$\mathbb{R}^d \rightarrow \mathbb{R}$），因此可以理解为gaussian mixture model（GMM），即：</p><script type="math/tex; mode=display">w_j(u) = e^{-\frac{1}{2}(u-\mu_j)^T \Sigma_j^{-1} (u-\mu_j)}</script><p>注意到，很多方法都能纳入这个框架，只需选取不同的$u(x,y)$和$w_j(u)$：</p><p><img src="13_1.png" alt></p><p>以CNN为例，假设在一个位置上的卷积，$3\times3$大小的卷积核，灰度图，则weight functions需要9个：$w_j(u)=\delta(u-\bar{u}_j),\quad j=1…,$9，其中$\bar{u}_1=(-1,1),\;\bar{u}_2=(0,1),\;\bar{u}_3=(1,1),\;…,\;\bar{u}_9=(0,0)$是kernel所覆盖点的相对坐标。则$D(x)f$是一个9维向量，所使用的卷积核$g$也是9维向量，两者进行内积则产生了一个scalar。</p><p>而且不难注意到，The patch operator公式就是KPConv中所用点卷积的泛化形式，将：</p><script type="math/tex; mode=display">(\mathcal{F}*\mathcal{g})(x) = \sum_{x_i\in\mathcal{N}_x} g(x_i-x)f_i</script><p>泛化成：</p><script type="math/tex; mode=display">D_j(x)f = \sum_{y\in \mathcal{N(x)}} w_j(u(x,y))f(y),\quad j=1</script>]]></content>
      
      
      <categories>
          
          <category> 3d </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 3d </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>理解深度学习中的各种卷积</title>
      <link href="/dl/conv-understanding/"/>
      <url>/dl/conv-understanding/</url>
      
        <content type="html"><![CDATA[<h1 id="理解深度学习中的各种卷积"><a href="#理解深度学习中的各种卷积" class="headerlink" title="理解深度学习中的各种卷积"></a>理解深度学习中的各种卷积</h1><blockquote><p>参考文献：<a href="https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215" target="_blank" rel="noopener">A Comprehensive Introduction to Different Types of Convolutions in Deep Learning</a></p></blockquote><h2 id="1-Convolution-v-s-Cross-correlation"><a href="#1-Convolution-v-s-Cross-correlation" class="headerlink" title="1. Convolution v.s. Cross-correlation"></a>1. Convolution v.s. Cross-correlation</h2><p>卷积（Convolution）与互相关（Cross-correlation）</p><p><img src="1.jpeg" alt="Difference between convolution and cross-correlation in signal processing."></p><p>In Deep Learning, the filters in convolution are not reversed. Rigorously speaking, it’s cross-correlation. We essentially perform element-wise multiplication and addition. But it’s a convention to just call it convolution in deep learning. </p><h2 id="2-Convolution-in-Deep-Learning"><a href="#2-Convolution-in-Deep-Learning" class="headerlink" title="2. Convolution in Deep Learning"></a>2. Convolution in Deep Learning</h2><ol><li><p>理清概念：</p><ul><li>layers and filters are at the same level</li><li>while channels and kernels are at one level below</li><li>Channels and feature maps are the same thing</li><li>“channel” is usually used to describe the structure of a “layer”</li><li>“kernel” is used to describe the structure of a “filter”（A “Kernel” refers to a 2D array of weights. The term “filter” is for 3D structures of multiple kernels stacked together. ）</li></ul></li><li><p>卷积</p><p><img src="2.gif" alt="The first step of 2D convolution for multi-channels: each of the kernels in the filter are applied to three channels in the input layer, separately."></p><p>Then these three channels are summed together（element-wise addition）to form one single channel（3 x 3 x 1）. </p><p><img src="3.gif" alt="The second step of 2D convolution for multi-channels: then these three channels are summed together（element-wise addition）to form one single channel."></p><p>上述过程又可以看作这样：（2D卷积）</p><p><img src="4.png" alt="Standard 2D convolution. Mapping one layer with depth Din to another layer with depth Dout, by using Dout filters."></p><p>上述卷积被称作2D卷积是因为filter的自由度其实只有2维，不能在channel方向上移动（因为kernel size == channel size）。虽然的确实在3D volumetric data上的卷积。一次卷积操作过后得到的是一个单通道的图片。</p><blockquote><p><strong>1 x 1 Convolution</strong></p><p><img src="5.png" alt="1 x 1 convolution, where the filter size is 1 x 1 x D."></p><p>Initially, 1 x 1 convolutions were proposed in the Network-in-network <a href="https://arxiv.org/abs/1312.4400" target="_blank" rel="noopener">paper</a>. They were then highly used in the Google Inception <a href="https://arxiv.org/abs/1409.4842" target="_blank" rel="noopener">paper</a>. A few advantages of 1 x 1 convolutions are:</p><ul><li>Dimensionality reduction for efficient computations</li><li>Efficient low dimensional embedding, or feature pooling</li><li>Applying nonlinearity again after convolution</li></ul><p><strong>Convolution Arithmetic</strong></p><p><img src="6.gif" alt></p><p>尺寸计算：</p><script type="math/tex; mode=display">\begin{align}W_{out} &= \lfloor \frac{W_{equal}-K}{S} \rfloor + 1 \\&= \lfloor \frac{W_{in}-K+(P_{left}+P_{right})}{S} \rfloor + 1\\&= \lfloor \frac{W_{in}-K+2P}{S} \rfloor + 1\end{align}</script></blockquote></li></ol><h2 id="3-3D-Convolution"><a href="#3-3D-Convolution" class="headerlink" title="3. 3D Convolution"></a>3. 3D Convolution</h2><p>   当kernel size &lt; channel size时，filter有3个自由度，一次卷积操作过后得到的是多通道的图片（3D data）</p><p>   <img src="7.png" alt="In 3D convolution, a 3D filter can move in all 3-direction（height, width, channel of the image）. At each position, the element-wise multiplication and addition provide one number. Since the filter slides through a 3D space, the output numbers are arranged in a 3D space as well. The output is then a 3D data."></p><p>   Similar as 2D convolutions which encode spatial relationships of objects in a 2D domain, 3D convolutions can describe the spatial relationships of objects in the 3D space.</p><h2 id="4-Transposed-Convolution（Deconvolution）"><a href="#4-Transposed-Convolution（Deconvolution）" class="headerlink" title="4. Transposed Convolution（Deconvolution）"></a>4. Transposed Convolution（Deconvolution）</h2><ul><li><p>目的：做上采样（up-sampling）</p></li><li><p>别名：transposed convolution == deconvolution（不太合适） == fractionally strided convolution</p></li></ul><p>It is always possible to implement a transposed convolution with a direct convolution.</p><p><img src="8.gif" alt="Up-sampling a 2 x 2 input to a 4 x 4 output."></p><p>Interestingly enough, one can map the same 2 x 2 input image to a different image size, by applying fancy padding &amp; stride.</p><p><img src="9.gif" alt="Up-sampling a 2 x 2 input to a 5 x 5 output."></p><blockquote><p><strong>为什么叫做“转置卷积”？</strong></p><p>我们先看下卷积使用矩阵乘法怎么做（$Kernel \times Large = Small$）：</p><p><img src="10.jpeg" alt="Matrix multiplication for convolution: from a Large input image（4 x 4）to a Small output image（2 x 2）."></p><p>如果我们认为$C^T C \approx I$那么$Kernel^{\, T} \times Small = Large$将可以复原输入：</p><p><img src="11.png" alt="Matrix multiplication for convolution: from a Small input image（2 x 2）to a Large output image（4 x 4）."></p><p>As you can see here, we perform up-sampling from a small image to a large image. That is what we want to achieve. And now, you can also see where the name “transposed convolution” comes from.</p><p><strong>Checkerboard artifacts</strong></p><p>One unpleasant behavior that people observe when using transposed convolution is the so-called checkerboard artifacts（棋盘效应）.</p><p><img src="12.png" alt="A few examples of checkerboard artifacts."></p></blockquote><h2 id="5-Dilated-Convolution（Atrous-Convolution）"><a href="#5-Dilated-Convolution（Atrous-Convolution）" class="headerlink" title="5. Dilated Convolution（Atrous Convolution）"></a>5. Dilated Convolution（Atrous Convolution）</h2><p><img src="13.gif" alt="The dilated convolution."></p><p>参数$l$（使Kernel变大的参数）：</p><ul><li>当$l=1$时，空洞卷积的Kernel就是普通的Kernel</li><li>当$l \geq 1$时，空洞卷积Kernel的Element间插入了$l-1$个间隔（以可理解为插入了零元素，增大了Kernel的尺寸，感受野计算方法与传统的计算方法一致）</li></ul><p>The following image shows the kernel size when $l = 1, 2,$ and $4$.</p><p><img src="14.jpeg" alt="Receptive field for the dilated convolution. We essentially observe a large receptive field without adding additional costs."></p><p>优点：不增大参数量的情况下，加大感受野（实质上是通过增大Kernel面积，然后使用时多个级联，才能使得感受野迅速增大）</p><h2 id="6-Separable-Convolutions"><a href="#6-Separable-Convolutions" class="headerlink" title="6. Separable Convolutions"></a>6. Separable Convolutions</h2><h3 id="6-1-Spatially-Separable-Convolutions"><a href="#6-1-Spatially-Separable-Convolutions" class="headerlink" title="6.1 Spatially Separable Convolutions"></a>6.1 Spatially Separable Convolutions</h3><ol><li><p>动机：</p><p>Conceptually, spatially separable convolution decomposes a convolution into two separate operations. For an example shown below, a Sobel kernel, which is a 3x3 kernel, is divided into a 3x1 and 1x3 kernel.</p><p><img src="15.png" alt="A Sobel kernel can be divided into a 3 x 1 and a 1 x 3 kernel."></p><p>做一次 3x3 卷积，等价于先做一次 3x1 卷积再做一次 1x3 卷积。（从9个参数变到了6个参数）</p><p><img src="16.png" alt="Spatially separable convolution with 1 channel."></p><p>特点：</p><ul><li>计算量减少（举例：对 5x5 的feature map做 3x3 的普通卷积需要81次乘法，而空间分离卷积只需要72次乘法），$N \times N$的feature map用$m \times m$ 的Kernel作卷积，普通卷积需要$(N-2)^2m^2$次乘法，而空间分离卷积只需要$2m(N-1)(N-2)$次乘法，节省了$\frac{2}{m}+\frac{2}{m(N-2)}$倍的乘法计算量。（当特征图较大的时候，对于$m \times m$大的Kernel，可以使计算量变为$\frac{2}{m}$倍）</li><li>在深度学习中的应用不多：不是所有的Kernel都可以这样分解，使用空间分离卷积的结果可能会欠优。</li><li>将2D的Kernel分解为2个1D的Kernel</li><li>Kernel的rank是1</li></ul></li></ol><blockquote><p><strong>Flattened convolutions</strong></p><ol><li><p>动机：</p><p>与Spatially Separable Convolutions类似，我们将一个3D的Kernel分解为3个1D的Kernel。</p><p><img src="17.png" alt></p></li><li><p>特点：</p><ul><li>进一步减少参数量</li><li>Kernel的rank是1</li></ul></li></ol></blockquote><h3 id="6-2-Depthwise-Separable-Convolutions"><a href="#6-2-Depthwise-Separable-Convolutions" class="headerlink" title="6.2 Depthwise Separable Convolutions"></a>6.2 Depthwise Separable Convolutions</h3><p>深度可分离卷积的步骤（10275 multiplications）：</p><ol><li><p>先变小（depthwise convolution）：将Layer和Kernel在深度方向上分离，做一次2D卷积，然后在通道方向上concat，得到深度不变、宽长变小的Layer</p></li><li><p>再加深（1x1 convolution）：做128次$1 \times 1$的卷积得到宽长不变、深度为128的Layer</p></li></ol><p><img src="18.png" alt="The overall process of depthwise separable convolution."></p><p>即：</p><script type="math/tex; mode=display">depthwise \; separable \; convolutions = depthwise \; convolution \; + \; 1 \times 1 \; convolution</script><p>典型的网络有 <a href="https://arxiv.org/abs/1704.04861" target="_blank" rel="noopener">MobileNet</a>、<a href="https://arxiv.org/abs/1610.02357" target="_blank" rel="noopener">Xception</a></p><p>对比普通的卷积（86400 multiplications）：</p><p><img src="19.png" alt="Standard 2D convolution to create output with 128 layer, using 128 filters."></p><p>特点：</p><ul><li><p>乘法数节省比：$\frac{1}{N_C}+\frac{1}{h^2}$，其中$N_C$是输出Layer的深度，$h$是Kernel的尺寸。（一般而言，输出通道数一般很大，使用 3x3 卷积核将减少到$1/9$倍的参数量）</p></li><li><p>对于small model，使用它，反而会减少模型容量（容易欠拟合）；适当地利用则会提高效率，而不会降低很多精度。</p></li></ul><h2 id="7-Grouped-Convolution"><a href="#7-Grouped-Convolution" class="headerlink" title="7. Grouped Convolution"></a>7. Grouped Convolution</h2><p>分组卷积示意图（分组$g=2$）：</p><p><img src="20.png" alt="Grouped convolution with 2 filter groups."></p><p>步骤：</p><ol><li><p>将普通的Kernel拆成$g$段，每一段有$D_{out}/g$个。对Layer来说同理。</p></li><li><p>这就相当于变成了$g$个卷积操作，我们分组地卷积。得到$g$个卷积后的结果（每个结果的深度为$D_{out}/g$）。</p></li><li><p>将结果在channel方向上concat起来，就恰好得到$D_{out}$深度的Layer</p></li></ol><p>典型的网络有： <a href="https://arxiv.org/abs/1611.05431" target="_blank" rel="noopener">ResNeXt</a></p><p><img src="21.png" alt="ResNeXt"></p><p>注：information只是在分组的区域内交互，并没有在所有channels上都进行交互融合，导致学得的特征也具有分组的特性，阻挡了不同分组之间的“信息流”。</p><p><img src="22.png" alt="AlexNet conv1 filter separation: as noted by the authors, filter groups appear to structure learned filters into two distinct groups, black-and-white and colour filters."></p><blockquote><p><strong>Grouped convolution v.s. depthwise convolution</strong></p><p>当grouped convolution中的$g=D_{in}=D_{out}$时，就是depthwise convolution了</p><p>分组卷积的优点：</p><ul><li>model-parallelization：不同的分组可以在不同的GPU上面跑（模型并行），可以一次训练更多的图片。（The model-parallelization is considered to be better than data parallelization. ）</li><li>more efficient：参数量变为原来的$1/g$</li><li>may provide a better model： In reducing the number of parameters in the network in this salient way, it is not as easy to over-fit, and hence a regularization-like effect allows the optimizer to learn more accurate, more efficient deep networks. 详情：<a href="https://blog.yani.io/filter-group-tutorial/" target="_blank" rel="noopener">A Tutorial on Filter Groups（Grouped Convolution）</a></li></ul><p><strong>Shuffled Grouped Convolution</strong></p><p>由<a href="https://arxiv.org/abs/1707.01083" target="_blank" rel="noopener">ShuffleNet</a>（主要用于移动端部署）提出的概念，为了加强不同group之间的信息流动</p><p>Overall, the shuffled grouped convolution involves grouped convolution and channel shuffling.</p><p>The idea of channel shuffle is that we want to mix up the information from different filter groups. In the image below, we get the feature map after applying the first grouped convolution GConv1 with 3 filter groups. Before feeding this feature map into the second grouped convolution, we first divide the channels in each group into several subgroups. The we mix up these subgroups.</p><p><img src="23.png" alt="Channel shuffle."></p><p>After such shuffling, we continue performing the second grouped convolution GConv2 as usual. But now, since the information in the shuffled layer has already been mixed, we essentially feed each group in GConv2 with different subgroups in the feature map layer（or in the input layer）. As a result, we allow the information flow between channels groups and strengthen the representations.</p><p><strong>Pointwise grouped convolution</strong></p><p>The pointwise grouped convolution, as the name suggested, performs group operations for 1 x 1 convolution.（分组卷积 + 使用1*1卷积核）</p><p>计算速度更快</p><p>In the ShuffleNet paper, authors utilized three types of convolutions we have learned: (1) shuffled grouped convolution; (2) pointwise grouped convolution; and (3) depthwise separable convolution.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> dl </category>
          
      </categories>
      
      
        <tags>
            
            <tag> dl </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《Deep Learning》读书笔记（Ch4.NumericalComputation）</title>
      <link href="/dl/dlbooknotes4/"/>
      <url>/dl/dlbooknotes4/</url>
      
        <content type="html"><![CDATA[<h1 id="4-Numerical-Computation"><a href="#4-Numerical-Computation" class="headerlink" title="4. Numerical Computation"></a>4. Numerical Computation</h1><h2 id="4-1-Overflow-and-Underflow"><a href="#4-1-Overflow-and-Underflow" class="headerlink" title="4.1 Overflow and Underflow"></a>4.1 Overflow and Underflow</h2><ol><li><p>Underflow：十分靠近零的数，被舍入到零</p><ul><li>例如$\frac{1}{x}$的除数在负数、正数、零的地方表现都不同；例如$log(x)$不能输入零</li></ul></li><li><p>Overflow：溢出（趋近正无穷/负无穷）</p><ul><li>例如softmax的计算（原式）：<ul><li>当$x_j$很负时，分母接近零，可能会underflow</li><li>当$x_j$很正时，各项非常大，可能会overflow</li><li>变式就不会出现underflow/overflow了，使得数值计算稳定；目前也有一些软件包（例如Theano）可以自动检测和修复数值稳定性</li></ul></li></ul><script type="math/tex; mode=display">\begin{align}softmax(x)_i & = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}} \quad （原式） \\& = \frac{e^{x_i-max(x)}}{\sum_{j=1}^{n} e^{x_j-max(x)}} \quad （变式）\end{align}</script></li></ol><h2 id="4-2-Poor-Conditioning"><a href="#4-2-Poor-Conditioning" class="headerlink" title="4.2 Poor Conditioning"></a>4.2 Poor Conditioning</h2><ol><li><p>条件数：</p><script type="math/tex; mode=display">condition \; number = \max_{i,j} |\frac{\lambda_i}{\lambda_j}|</script><ul><li>条件数越大，$A^{-1}$对输入就越敏感（即解方程$Ax=b$的解$x$误差很大），小的输入扰动对输出的影响很大</li></ul></li></ol><h2 id="4-3-Gradient-Based-Optimization"><a href="#4-3-Gradient-Based-Optimization" class="headerlink" title="4.3 Gradient-Based Optimization"></a>4.3 Gradient-Based Optimization</h2><ol><li><p>Steepest descent（gradient descent）：</p><script type="math/tex; mode=display">x^{'} = x - \epsilon \nabla_x f(x)</script></li><li><p>Line search：从几个待定的$\epsilon$中选出使得$f(x-\epsilon\nabla_x f(x))$值最小的$\epsilon$作为结果</p></li><li><p>Hill climbing：上升离散参数的目标函数</p></li></ol><h3 id="4-3-1-Beyond-the-Gradient-Jacobian-and-Hessian-Matrices"><a href="#4-3-1-Beyond-the-Gradient-Jacobian-and-Hessian-Matrices" class="headerlink" title="4.3.1 Beyond the Gradient: Jacobian and Hessian Matrices"></a>4.3.1 Beyond the Gradient: Jacobian and Hessian Matrices</h3><ol><li><p>Jacobian matrix：向量对向量求导</p><p>如果$f:\mathbb{R}^m \rightarrow \mathbb{R}^n$，那么雅可比矩阵定义为$J_{i,j}=\frac{\partial}{\partial x_j}f(x)_i（注：J \in \mathbb{R}^{n \times m}）$</p></li><li><p>Hessian matrix：标量对向量的二阶导</p><p>如果$f:\mathbb{R}^n \rightarrow \mathbb{R}$，那么海森矩阵的定义为$H_{i,j}=\frac{\partial^2}{\partial x_i \partial x_j}f(x)$</p><ul><li>当二阶偏导连续时，海森矩阵是实对称矩阵。（一般情况下都满足二阶导连续，所以一般默认海森矩阵是实对称矩阵，因而可以将$H$作特征值分解，分解为$Q \Lambda Q^T$）</li><li>在某一点的方向二阶导是$d^THd$，其中$d$是该点方向的单位向量，$H$是海森矩阵</li><li>注意到$d^THd$与$H$的特征值相关联：当$d$是$H$的第$i$个特征向量时，该点的二阶导等于第$i$个特征值；二阶导的取值范围是$[\lambda_n,\lambda_1]$。</li><li>所以海森矩阵的特征值可以近似看作二阶导（曲率）</li><li>一阶导为零且海森矩阵正定：局部最小值；一阶导为零且海森矩阵负定：局部最大值；一阶导为零且海森矩阵的特征值有正有负，则该点一定是鞍点。</li></ul></li><li><p>二阶泰勒展式：</p><script type="math/tex; mode=display">f(x) \approx f(x^{(0)}) + (x-x^{(0)})^T \mathcal{g}+\frac{1}{2}(x-x^{(0)})^TH(x-x^{(0)})</script><ul><li>如果使用一阶的梯度下降法，取学习率为$\epsilon$（更新规则$x \leftarrow x-\epsilon \mathcal{g}$），那么当取$\epsilon=\frac{\mathcal{g}^T\mathcal{g}}{\mathcal{g}^TH\mathcal{g}}$时，能一下子跳到此方向（gradient方向）的局部极小值（抛物面近似）。（英文花书Page88）</li><li>使用Newton’s method时，更新规则为$x \leftarrow x - H^{-1} \mathcal{g}$，能一下子跳到极值点（抛物面近似）。（见英文花书Page311）</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> dl </category>
          
      </categories>
      
      
        <tags>
            
            <tag> dl </tag>
            
            <tag> notes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《Deep Learning》读书笔记（Ch3.ProbabilityandInformationTheory）</title>
      <link href="/dl/dlbooknotes3/"/>
      <url>/dl/dlbooknotes3/</url>
      
        <content type="html"><![CDATA[<h1 id="3-Probability-and-Information-Theory"><a href="#3-Probability-and-Information-Theory" class="headerlink" title="3. Probability and Information Theory"></a>3. Probability and Information Theory</h1><ul><li>概率论：作出概率上的推断</li><li>信息论：定量分析不确定性</li></ul><h2 id="3-1-Why-Probability"><a href="#3-1-Why-Probability" class="headerlink" title="3.1 Why Probability?"></a>3.1 Why Probability?</h2><ol><li><p>产生不确定性的原因：</p><ul><li><p>被建模系统中本身就有的的不确定性</p></li><li><p>不完全的观测</p></li><li><p>不充分的建模</p></li></ul></li><li><p>建模过程中，更喜欢用simple but uncertain rule rather than a complex but certain one</p></li><li><p>概率的分类：</p><ul><li>Frequentist probability（频率论）：直接与事件发生的概率关联</li><li>Bayesian probability（贝叶斯）：与定性的确定性关联</li></ul></li></ol><h2 id="3-2-Random-Variables"><a href="#3-2-Random-Variables" class="headerlink" title="3.2 Random Variables"></a>3.2 Random Variables</h2><p>随机变量（that can take on different values randomly）的类型：</p><ul><li>离散型</li><li>连续性</li></ul><h2 id="3-3-Probability-Distributions"><a href="#3-3-Probability-Distributions" class="headerlink" title="3.3 Probability Distributions"></a>3.3 Probability Distributions</h2><h3 id="3-3-1-Discrete-Variables-and-Probability-Mass-Functions"><a href="#3-3-1-Discrete-Variables-and-Probability-Mass-Functions" class="headerlink" title="3.3.1 Discrete Variables and Probability Mass Functions"></a>3.3.1 Discrete Variables and Probability Mass Functions</h3><ol><li><p>概率质量函数（Probability Mass Functions = PMF）记作$ P(x) $</p><p>联合分布（Joint Probability Distribution）记作$ P(x,y) $</p><p>均匀分布（Uniform Distribution）$ P(x=x_i) = \frac{1}{k} $（with k different states）</p></li></ol><h3 id="3-3-2-Continuous-Variables-and-Probability-Density-Functions"><a href="#3-3-2-Continuous-Variables-and-Probability-Density-Functions" class="headerlink" title="3.3.2 Continuous Variables and Probability Density Functions"></a>3.3.2 Continuous Variables and Probability Density Functions</h3><ol><li><p>概率密度函数（Probability Density Functions = PDF）记作$ p(x) $</p><p>In the univariate example, the probability that x lies in the interval [a, b] is given by $ \int_{[a,b]} p(x) dx $</p><p>均匀分布$ u(x;a,b) = \frac{1}{b-a} $（The “;” notation means “parametrized by”），denote by writing $ x\sim U(a,b) $</p></li></ol><h2 id="3-4-Marginal-Probability"><a href="#3-4-Marginal-Probability" class="headerlink" title="3.4 Marginal Probability"></a>3.4 Marginal Probability</h2><ol><li>边缘概率分布：（已知联合分布，求单变量的分布）</li></ol><script type="math/tex; mode=display">P(x) = \sum_y P(x,y)\\p(x) = \int p(x,y) \; dy</script><h2 id="3-5-Conditional-Probability"><a href="#3-5-Conditional-Probability" class="headerlink" title="3.5 Conditional Probability"></a>3.5 Conditional Probability</h2><ol><li><p>条件概率：$ P(y|x) $（已知$ x $，求$ y $。相当于图像分类问题）</p><p>公式：$ P(x,y) = P(y|x) \times P(x) = P(x|y) \times P(y) $</p></li></ol><h2 id="3-6-The-Chain-Rule-of-Conditional-Probabilities"><a href="#3-6-The-Chain-Rule-of-Conditional-Probabilities" class="headerlink" title="3.6 The Chain Rule of Conditional Probabilities"></a>3.6 The Chain Rule of Conditional Probabilities</h2><ol><li><p>多个变量的联合分布，可以拆成多个条件概率的连乘积：</p><script type="math/tex; mode=display">P(a,b,c) = P(a|b,c) \times P(b|c) \times P(c)</script></li></ol><h2 id="3-7-Independence-and-Conditional-Independence"><a href="#3-7-Independence-and-Conditional-Independence" class="headerlink" title="3.7 Independence and Conditional Independence"></a>3.7 Independence and Conditional Independence</h2><ol><li><p>变量x和y独立：$ \forall x, y, \quad p(x,y)=p(x) \times p(y) $, denote as $ x \perp y $</p></li><li><p>变量x和y基于条件z独立：$ \forall x, y, z, \quad p(x,y|z)=p(x|z) \times p(y|z) $, denote as $ x \perp y \, | \, z $</p></li></ol><h2 id="3-8-Expectation-Variance-and-Covariance"><a href="#3-8-Expectation-Variance-and-Covariance" class="headerlink" title="3.8 Expectation, Variance and Covariance"></a>3.8 Expectation, Variance and Covariance</h2><ol><li>期望（expectation）：</li></ol><script type="math/tex; mode=display">\mathbb{E}_{x \sim P}[f(x)] = \sum \limits_{x} P(x) f(x)</script><script type="math/tex; mode=display">\mathbb{E}_{x \sim p}[f(x)] = \int p(x) f(x) \, dx</script><ol><li><p>方差（variance）：</p><script type="math/tex; mode=display">Var(f(x)) = \mathbb{E}[(f(x)-\mathbb{E}[f(x)])^2]</script><p>标准差（standard deviation）：$ \sqrt{Var(f(x))} $</p></li><li><p>协方差（covariance）：</p><script type="math/tex; mode=display">Cov(f(x),g(y)) = \mathbb{E}[(f(x)-\mathbb{E}[f(x)])(g(y)-\mathbb{E}[g(y)])]</script><p>The covariance gives some sense of how much two values are linearly related to each other.（只是描述线性相关程度的度量）</p><ul><li><p>协方差&gt;0时：一个变大另一个也会变大</p></li><li><p>协方差&lt;0时：一个变大另一个会变小</p></li></ul><p>相关系数（Correlation）： $ \rho_{X,Y} = \frac{Cov(X,Y)}{\sigma_X \sigma_Y} $，该系数广泛用于度量两个变量之间的<strong>线性</strong>相关程度</p><p><img src="3.png" alt></p><p>性质：</p><ul><li>$ Cov=0 \Leftarrow independent $</li><li>$ Cov \neq 0 \Leftrightarrow linear \; dependent $</li><li>$ Cov=0 \Rightarrow there \; mush \; be \; no \; linear \; dependence  $</li></ul></li></ol><p><img src="4.png" alt></p><p>   协方差矩阵：$ Cov(x)_{i,j} = Cov(x_i,x_j) $，其对角元素是方差。</p><h2 id="3-9-Common-Probability-Distributions"><a href="#3-9-Common-Probability-Distributions" class="headerlink" title="3.9 Common Probability Distributions"></a>3.9 Common Probability Distributions</h2><h3 id="3-9-1-Bernoulli-Distribution"><a href="#3-9-1-Bernoulli-Distribution" class="headerlink" title="3.9.1 Bernoulli Distribution"></a>3.9.1 Bernoulli Distribution</h3><p>做一次实验得到1维的随机变量，只有两个可能的取值（0或1），像抛硬币一样：$ P(x) = \phi^x (1-\phi)^{1-x} $（好像类似于做一次实验得到了sigmoid输出的的随机变量，即一个node只有0或1？）</p><h3 id="3-9-2-Multinoulli-Distribution"><a href="#3-9-2-Multinoulli-Distribution" class="headerlink" title="3.9.2 Multinoulli Distribution"></a>3.9.2 Multinoulli Distribution</h3><p>做一次实验得到k维的随机变量$ p=[0,1]^{k} $（意味着每一个entry只能是0或1，且只能有一个entry是1其他都是0），且满足$ \sum_i p_i = 1 $（例如投骰子$p_1=p_2=…=p_6=\frac{1}{6}$。此处的$p_i$不代表随机变量的结果，只代表这个entry取到1的概率），（好像类似于做一次实验然后得到了一个one-hot随机变量，即softmax output layer有k个nodes？）</p><h3 id="3-9-3-Gaussian-Distribution"><a href="#3-9-3-Gaussian-Distribution" class="headerlink" title="3.9.3 Gaussian Distribution"></a>3.9.3 Gaussian Distribution</h3><ol><li>高斯分布：</li></ol><script type="math/tex; mode=display">\mathcal{N}(x;\mu,\sigma^2) = \sqrt{\frac{1}{2\pi \sigma^2}}exp(-\frac{(x-\mu)^2}{2\sigma^2})</script><ul><li><p>精度（precision）：$ \beta = \frac{1}{\sigma^2} $，精度越大则高斯分布越集中</p></li><li><p>当不知道采取什么先验时，就选取高斯分布吧（因为高斯分布包含的先验知识少）</p></li><li><p>L2正则等于在MAP Bayesian inference中使用高斯分布作为prior（参见英文版花书的Page236）</p></li><li><p>多变量时：</p><script type="math/tex; mode=display">\mathcal{N}(x;\mu,\Sigma) = \sqrt{\frac{1}{(2\pi)^n det(\Sigma)}}exp(-\frac{1}{2}(x-\mu)^T \Sigma^{-1} (x-\mu))</script><p>其中$ \Sigma $是协方差阵，$ \mu $是均值向量。当然也可以写成精度矩阵的形式（此时精度矩阵$ \beta = \Sigma^{-1} $）</p></li><li><p>isotropic（各向同性）：协方差阵是scalar乘以identity matrix</p><ul><li>isotropic：像圆一样的分布</li><li>diagonal（协方差阵）：沿着轴向分布（像椭圆一样）</li><li>full-rank（协方差阵）：可以任意斜着分布（像斜着的椭圆一样，不必沿着坐标轴）</li></ul></li></ul><h3 id="3-9-4-Exponential-and-Laplace-Distributions"><a href="#3-9-4-Exponential-and-Laplace-Distributions" class="headerlink" title="3.9.4 Exponential and Laplace Distributions"></a>3.9.4 Exponential and Laplace Distributions</h3><ol><li>指数分布：</li></ol><script type="math/tex; mode=display">p(x;\lambda) = \lambda 1_{x \geq 0} exp(-\lambda x)</script><ol><li>拉普拉斯分布（L1正则等于在MAP Bayesian inference中使用isotropic Laplace distribution作为最大化log-prior term的先验，参见英文版花书的Page236）：</li></ol><script type="math/tex; mode=display">Laplace(x;\mu,\gamma) = \frac{1}{2\gamma}exp(-\frac{|x-\mu|}{\gamma})</script><p><img src="5.png" alt></p><h3 id="3-9-5-The-Dirac-Distribution-and-Empirical-Distribution"><a href="#3-9-5-The-Dirac-Distribution-and-Empirical-Distribution" class="headerlink" title="3.9.5 The Dirac Distribution and Empirical Distribution"></a>3.9.5 The Dirac Distribution and Empirical Distribution</h3><ol><li><p>狄拉克分布：$ p(x) = \delta(x-\mu) $（$ \delta $为冲激函数，积分为1）</p></li><li><p>经验分布（概率密度函数，多个冲激的和。通过采样样本点去近似真实的分布）：</p><script type="math/tex; mode=display">\hat{p}(x) = \frac{1}{m} \sum_{i=1}^m \delta(x-x^{(i)})</script></li></ol><h3 id="3-9-6-Mixtures-of-Distributions"><a href="#3-9-6-Mixtures-of-Distributions" class="headerlink" title="3.9.6 Mixtures of Distributions"></a>3.9.6 Mixtures of Distributions</h3><ol><li><p>混合分布定义：（$P(c)$是multinoulli distribution，表示从多个子分布中选取一个distribution的挑选行为。$P(x|c)$则表示被挑选到之后的子分布的贡献）</p><script type="math/tex; mode=display">P(x) = \sum_i P(c=i) P(x|c=i)</script><ul><li>Empirical Distribution是Mixtures of Distributions的一个例子之一。（此时$P(c=i)=\frac{1}{m}$，说明每个子分布被选中的概率是一样的）</li><li>通过混合分布，可以产生出复杂的概率分布。</li></ul></li><li><p>Latent variable（隐藏变量）：是一种不能被直接观测到的随机变量</p><ul><li>在混合分布中，随机变量$c$就是一个latent variable（因为混合分布你只能观测到表象，并不能观测到它的内部机制）</li></ul></li><li><p>Gaussian mixture model（高斯混合模型）：子分布都是高斯分布</p><ul><li>高斯混合模型是一个universal approximator of densities：任何光滑的分布都可以用高斯混合模型近似</li></ul></li><li><p>Prior probability：$P(c)$，未观测任何东西就能被事先确定下来</p></li><li><p>Posterior probability：$P(c|x)$，因为观测了$x$之后才被计算出来</p></li></ol><h2 id="3-10-Useful-Properties-of-Common-Functions"><a href="#3-10-Useful-Properties-of-Common-Functions" class="headerlink" title="3.10 Useful Properties of Common Functions"></a>3.10 Useful Properties of Common Functions</h2><ol><li>Logistic sigmoid：</li></ol><p><img src="6.png" alt></p><script type="math/tex; mode=display">\sigma(x) = \frac{1}{1+e^{-x}}</script><ul><li>常用于生成Bernoulli分布的参数$\phi$（二分类任务的输出概率值）</li><li>当softmax的两个node分别为0和x时，则退化为单个node输出的sigmoid情况</li></ul><ol><li>Softplus：</li></ol><p><img src="7.png" alt></p><script type="math/tex; mode=display">\zeta(x) = ln(1+e^{x})</script><ul><li>常用于产生高斯分布中的$\beta$或$\sigma$值（因为输出值大于零）</li><li>也用于softmax+nll中的合成计算</li><li>是ReLU的softened版本，plus的名字源自于$y=x$的正数部分的含义</li></ul><h2 id="3-11-Bayes’s-Rule"><a href="#3-11-Bayes’s-Rule" class="headerlink" title="3.11 Bayes’s Rule"></a>3.11 Bayes’s Rule</h2><ol><li>贝叶斯公式</li></ol><script type="math/tex; mode=display">P(x|y) = \frac{P(x)P(y|x)}{P(y)}</script><ul><li>此处的$P(y)$可以通过$P(y)=\sum_x P(y|x)P(x)$计算出来（边缘分布的计算公式）</li><li>那么我们就只需要知道$P(y|x)$和$P(x)$，就可以推测出$P(x|y)$了</li></ul><h2 id="3-12-Technical-Details-of-Continuous-Variables"><a href="#3-12-Technical-Details-of-Continuous-Variables" class="headerlink" title="3.12 Technical Details of Continuous Variables"></a>3.12 Technical Details of Continuous Variables</h2><ol><li>Measure zero：零测度（例如线在面中的测度是零）</li><li><p>Almost everywhere：几乎所有（除了可以忽略的特例之外，几乎所有）</p></li><li><p>若随机变量$x$和$y$满足$y=g(x)$，则：</p></li></ol><script type="math/tex; mode=display">p_x(x)=p_y(g(x))|det(\frac{\partial g(x)}{\partial x})|</script><h2 id="3-13-Information-Theory"><a href="#3-13-Information-Theory" class="headerlink" title="3.13 Information Theory"></a>3.13 Information Theory</h2><ol><li><p>Idea：</p><ul><li>info=0：必然事件</li><li>info很大：难以发生的事件</li><li>info有可加性时：独立事件</li></ul></li><li><p>Self-information（自信息，只对一个outcome进行uncertainty的度量）：</p></li></ol><script type="math/tex; mode=display">I(x)=-log(P(x))</script><ul><li>当使用e为底时，单位是nat；当使用2为底时，单位是bits或shannons</li></ul><ol><li>Shannon entropy（香农熵$H(P)$，自信息在distribution上的期望）：</li></ol><script type="math/tex; mode=display">H(x) = \mathbb{E}_{x \sim P} [I(x)] = - \mathbb{E}_{x \sim P} [log \, P(x)]</script><ul><li>近乎确定性的分布（outcome近乎确定发生）：low entropy</li><li>接近均匀分布（不确定性程度大）：high entropy</li><li>对于连续变量，Shannon entropy又称作differential entropy</li></ul><ol><li><p>Kullback-Leibler divergence（KL散度/相对熵）：</p><script type="math/tex; mode=display">D_{KL}(P||Q) = \mathbb{E}_{x \sim P} [log \, \frac{P(x)}{Q(x)}] = \mathbb{E}_{x \sim P} [log \, P(x) - log \, Q(x)]</script><ul><li>用于描述两个分布的差异性</li><li>不可交换性（非对称的）。非负的。分布的差异越大则数值越大（分布相等时值为零）</li></ul></li><li><p>Cross-entropy（交叉熵）：</p><script type="math/tex; mode=display">H(P,Q) = H(P) + D_{KL} (P||Q) = - \mathbb{E}_{x \sim P} [ log \, Q(x)]</script><ul><li>对$Q$最小化交叉熵，等价于最小化KL散度（因为KL散度的第一项与$Q$无关）。</li><li>对离散分布的$p$和$q$，交叉熵即$H(p,q) = -\sum_x p(x)\; log \, q(x)$</li><li>交叉熵损失函数解读（my perspective）：<ul><li>假设softmax层的输出节点数为$n$，那么softmax层的输出就构成了离散概率分布$Q(x)$，且离散随机变量$x$的取值范围是$0 \sim (n-1)$。每一次inference就对应着输入的sample的分布被映射到$Q(x)$的过程。注意，每一次inference得到的prediction并不是deterministic的，只是我们最后取了分布$Q(x)$中概率最大的$x$值作为prediction罢了。</li><li>那么我们training时制作的one-hot label，就是为$Q(x)$制作的标签。我们假设标签服从分布$P(x)$。</li><li>注意，每一次inference所对应的$P(x)$和$Q(x)$都是不同的（除非你输入的sample是同一个）</li><li>我们classification task的目的就是想要model的输出越接近$P(x)$越好，那么我们的任务就是要最小化模型推断得到的分布$Q(x)$和我们制作的标签分布$ P(x) $之间的差异。</li><li>我们使用KL散度度量时，目标就是最小化$D_{KL}(P||Q)$（必须要让$x \sim P$，否则会出现花书Fig3.6的情况，我们想要的是Fig3.6左边那幅图的样子）。对于同一个sample，我们用optimizer调节parameters相当于优化输出分布$Q(x)$，所以$Q(x)$实质上是一个可变的分布。又因为对$Q$最小化交叉熵，等价于最小化KL散度。所以我们只需要最小化交叉熵就可以了。</li><li>举个例子，假设softmax输出层4个nodes。我们inference一次相当于生成了一个分布$Q(x)$，以及我们从dataset中取出所对应sample的标签分布$P(x)$。假设分布$Q(x)$可以用一个向量定量表示成$q=[0.1,0.1,0.6,0.2]$，标签分布$P(x)$定量表示成$p=[0,0,1,0]$。随机变量$x$的取值则自然是$0 \sim 3$。那么交叉熵（其中$q_y$的意思是取标签所对应的softmax输出值）：</li></ul></li></ul></li></ol><script type="math/tex; mode=display">\begin{align}H(P,Q) & = - \mathbb{E}_{x \sim P} [ log \, Q(x)] \\& = - \sum_{i=0}^{3}\; p_i\;log\,q_i \\& = - log \, q_{y} \\& = - log \, 0.6 \\& = 0.51\end{align}</script><h2 id="3-14-Structured-Probabilistic-Models"><a href="#3-14-Structured-Probabilistic-Models" class="headerlink" title="3.14 Structured Probabilistic Models"></a>3.14 Structured Probabilistic Models</h2><ol><li><p>Motivation：</p><ul><li>举例：如果能将联合分布$p(a,b,c)$拆成几个因子的乘积，例如$p(a)p(b|a)p(c|b)$，那么将能极大地减少参数量（这个例子是有向模型）</li><li>推广开来，即这种思想需要将复杂的分布拆解成几个容易表示出来的因子。我们将这种因子分解的方法，用graph表示，就叫做Structured Probabilistic Models</li></ul></li><li><p>结构概率模型的种类（对于一个distribution可能同时存在两种图描述方式，即有向/无向只是对于distribution的描述方式，而非种类的划分方式）：</p><ul><li><p>有向模型（将因子表示成条件概率分布）：</p><script type="math/tex; mode=display">p(x_1,...,x_n)=\prod_{i=1}^n p(x_i|Pa_\mathcal{G}(x_i))</script><p>其中$Pa_\mathcal{G}(x_i)$表示的是$x_i$的父节点</p><p><img src="8.png" alt></p></li><li><p>无向模型（将因子表示成函数）：</p><script type="math/tex; mode=display">p(x_1,...,x_n) = \frac{1}{Z} \prod_{i=1}^n \phi^{(i)}(\mathcal{C^{(i)}})</script><p>其中$\mathcal{C^{(i)}}$表示第$i$个clique，即彼此连接结点的集合，例如下图有三个clique（第一个：abc，第二个：bd，第三个：ce），$\phi^{(i)}(\cdot)$表示函数。前面的$\frac{1}{Z}$作用是归一化（因为不归一化的话，就不能保证这个连乘积项的求和是1）</p><p><img src="9.png" alt></p></li><li><p>有向图模型着重强调了因果的顺序关系；无向图模型着重强调了变量的集合间的interaction关系。</p></li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> dl </category>
          
      </categories>
      
      
        <tags>
            
            <tag> dl </tag>
            
            <tag> notes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>WSL2配置指南</title>
      <link href="/wsl/wsl2config/"/>
      <url>/wsl/wsl2config/</url>
      
        <content type="html"><![CDATA[<h1 id="安装WSL2"><a href="#安装WSL2" class="headerlink" title="安装WSL2"></a>安装WSL2</h1><h2 id="升级Win10系统"><a href="#升级Win10系统" class="headerlink" title="升级Win10系统"></a>升级Win10系统</h2><p>要使用WSL2需要系统的版本号不低于18917。而一般的Win10系统都是稳定版（意味着你的版本号一般来说都是低于18917的，按下Win+R并输入winver回车可以查看系统版本），需要加入Windows的预览体验计划。</p><p>详细教程，参考这里：<a href="https://jingyan.baidu.com/article/1876c85235c709890a137663.html" target="_blank" rel="noopener">Windows 10 20H1快速预览18917版系统更新教程</a></p><h2 id="安装WSL"><a href="#安装WSL" class="headerlink" title="安装WSL"></a>安装WSL</h2><ol><li><p>管理员身份打开powershell，输入下面的语句以打开“适用于Linux的Windows子系统”开关。</p><pre class=" language-lang-bash"><code class="language-lang-bash">Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux</code></pre></li><li><p>重启电脑</p></li><li><p>打开微软的应用商店，搜索WSL，安装Ubuntu即可。</p></li><li><p>安装完成之后，可以在开始菜单找到橙色的Ubuntu图标，点击它打开Ubuntu。</p></li><li><p>第一次运行需要等待安装并设置Ubuntu的用户名和密码。</p></li></ol><p>详细教程，参考这里：<a href="https://www.cnblogs.com/JettTang/p/8186315.html" target="_blank" rel="noopener">WSL（Windows Subsystem for Linux）的安装与使用</a></p><h2 id="升级WSL到WSL2"><a href="#升级WSL到WSL2" class="headerlink" title="升级WSL到WSL2"></a>升级WSL到WSL2</h2><ol><li><p>刚才安装的WSL还不是WSL2，需要虚拟化技术才可以。管理员身份打开powershell，输入下面指令以启用“虚拟机平台”可选组件：</p><pre class=" language-lang-bash"><code class="language-lang-bash">Enable-WindowsOptionalFeature -Online -FeatureName VirtualMachinePlatform</code></pre></li><li><p>重启计算机</p></li><li><p>在 PowerShell 中运行下面指令以将WSL转变为WSL2：</p><pre class=" language-lang-bash"><code class="language-lang-bash">wsl --set-version Ubuntu 2wsl --set-default-version 2</code></pre><blockquote><p>可能提示需要开启虚拟化技术<br>参考这里：</p><ul><li><a href="https://guwq2014.iteye.com/blog/2426896" target="_blank" rel="noopener">如何开启windows的虚拟化？</a></li><li><a href="https://jingyan.baidu.com/article/8ebacdf0261d3249f65cd531.html" target="_blank" rel="noopener">华硕主板BIOS设置中VT虚拟化技术选项怎么开启</a></li></ul></blockquote></li><li><p>在PowerShell 中输入下面指令确保WSL成功转化为第二版：</p><pre class=" language-lang-bash"><code class="language-lang-bash">wsl -l -v</code></pre><p>详细教程，参考这里：<a href="https://zhuanlan.zhihu.com/p/69121280" target="_blank" rel="noopener">在 Windows 中运行 Linux：WSL 2 使用入门</a></p></li></ol><h1 id="配置Ubuntu"><a href="#配置Ubuntu" class="headerlink" title="配置Ubuntu"></a>配置Ubuntu</h1><p>详细教程，参考这里：</p><ul><li><a href="https://zhuanlan.zhihu.com/p/57556340" target="_blank" rel="noopener">用WSL,MobaXterm,Cmder配置linux开发环境</a></li></ul><h2 id="更换源"><a href="#更换源" class="headerlink" title="更换源"></a>更换源</h2><p>输入下面命令更换软件源，并更新：</p><pre class=" language-lang-bash"><code class="language-lang-bash">sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak sudo sed -i 's/archive.ubuntu.com/mirrors.aliyun.com/g' /etc/apt/sources.listsudo apt updatesudo apt upgrade -y</code></pre><p>如果报错，则按照报错的提示，再输入一遍就好了。</p><h2 id="修改Win10盘的挂载点"><a href="#修改Win10盘的挂载点" class="headerlink" title="修改Win10盘的挂载点"></a>修改Win10盘的挂载点</h2><p>这步的操作是<strong>可选的</strong>，可以跳过。</p><p>默认的Windows磁盘在WSL的访问方式是<code>/mnt</code>开头，可能不太方便，我们下面更换成是<code>/</code>开头的吧。</p><p>打开这个文件：</p><pre class=" language-lang-bash"><code class="language-lang-bash">sudo vim /etc/wsl.conf</code></pre><p>添加下面三行内容，并保存此文件：</p><pre><code>[automount] root = / options = &quot;metadata&quot;</code></pre><p>关掉WSL窗口，然后重启WSL，重启WSL的方法为在CMD输入：</p><pre class=" language-lang-bash"><code class="language-lang-bash">net stop LxssManagernet start LxssManager</code></pre><p>再次打开WSL，输入<code>df -h</code>可以看到Win10盘的挂载点成功变更了。</p><h2 id="安装并配置VSCode"><a href="#安装并配置VSCode" class="headerlink" title="安装并配置VSCode"></a>安装并配置VSCode</h2><p>这步的操作是<strong>可选的</strong>，可以跳过。</p><p>安装VSCode和中文字体，因为WSL2没中文字体将显示为豆腐块</p><ol><li>安装中文字体（Noto Sans Mono CJK SC）：</li></ol><pre class=" language-lang-bash"><code class="language-lang-bash">sudo apt install -y fonts-noto-cjk fonts-noto-cjk-extra</code></pre><ol><li><p>在Win10下载VSCode的deb包：<a href="https://code.visualstudio.com/#alt-downloads" target="_blank" rel="noopener">官网下载页面</a></p></li><li><p>然后cd到deb包的下载目录，用下面的命令来安装此deb包（替换下面包的名字）：</p></li></ol><pre class=" language-lang-bash"><code class="language-lang-bash">sudo apt install ./code_1.31.1-1549938243_amd64.deb</code></pre><ol><li>安装要启动VSCode必要依赖：</li></ol><pre class=" language-lang-bash"><code class="language-lang-bash">sudo apt install libgtk2.0-0 libxss1 libasound2</code></pre><ol><li><p>输入<code>code .</code>以启动VSCode。（若还没图形界面则不会显示，等安装好了再进行第5步和第6步）</p></li><li><p>找到下面属性，并配置VSCode：</p></li></ol><pre class=" language-lang-json"><code class="language-lang-json">"Window.titleBarStyle": "native","editor.fontFamily": "monospace,'Noto Sans Mono CJK SC'"</code></pre><h2 id="安装并配置Python"><a href="#安装并配置Python" class="headerlink" title="安装并配置Python"></a>安装并配置Python</h2><p>这步的操作是<strong>可选的</strong>，可以跳过。</p><p>我们使用Miniconda来安装和配置Python。</p><ol><li><p>下载清华大学开源软件镜像站的Miniconda镜像（下载最新版本即可，可以在Win10下载好然后cd到下载目录中）：<a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/" target="_blank" rel="noopener">下载地址</a></p></li><li><p>输入<code>bash Miniconda3-latest-Linux-x86_64.sh</code>命令安装。按照提示输入即可，建议在最后输入yes初始化python环境（会安装最新版本的python）。</p></li><li><p>重新打开终端窗口，python环境会自动启动。</p></li></ol><p>详细教程，参考这里：<a href="https://ywnz.com/linuxjc/3834.html" target="_blank" rel="noopener">在linux系统中安装与卸载miniconda的方法</a></p><h2 id="配置Xrdp"><a href="#配置Xrdp" class="headerlink" title="配置Xrdp"></a>配置Xrdp</h2><ol><li>安装xrdp及相关：</li></ol><pre class=" language-lang-bash"><code class="language-lang-bash">sudo apt-get install xrdpsudo apt-get install vnc4serversudo apt-get install xubuntu-desktop</code></pre><ol><li>修改port 3389，避免跟本机的端口冲突（例如修改成3390）：</li></ol><pre class=" language-lang-bash"><code class="language-lang-bash">sudo vim /etc/xrdp/xrdp.ini</code></pre><ol><li>设为启动桌面</li></ol><pre class=" language-lang-bash"><code class="language-lang-bash">echo xfce4-session >~/.xsession</code></pre><ol><li>在<code>~</code>创建新文件，例如取名叫<code>run_remote_desktop.sh</code>，添加如下内容</li></ol><pre class=" language-lang-bash"><code class="language-lang-bash">echo 'Going to start xrdp so as to open remote desktop port...'echo 'Please input your password...'echo '========================================='sudo service xrdp restartecho '========================================='ipv4_address=$(/sbin/ifconfig -a|grep inet|grep -v 127.0.0.1|grep -v inet6|awk '{print $2}'|tr -d "addr:")echo "Your WSL2 is running on $ipv4_address:3390"</code></pre><ol><li>最后执行<code>chmod 777 ~/run_remote_desktop.sh</code>以赋予权限</li><li>每次想要启动桌面就输入<code>bash ~/run_remote_desktop.sh</code>，然后输入密码，最后会提示：<code>Your WSL2 is running on 172.28.246.150:3390</code>类似的字样，打开Win10的远程桌面，输入此ip就可以远程连接了。</li><li>然后配置图形界面（例如前面还没有完成的对VSCode的配置）</li><li>Enjoy developing！</li></ol><blockquote><p>注：输入<code>netstat -tulpn | grep LISTEN</code>可以查看到监听的端口，确保3390端口正在监听，否则无法远程桌面。</p></blockquote><p>详细教程，参考这里：</p><ul><li><a href="https://blog.csdn.net/BigWrist/article/details/81208667" target="_blank" rel="noopener">windows 10 WSL：Ubuntu 18.04 + xrdp</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> wsl </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> wsl </tag>
            
            <tag> ubuntu </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《Deep Learning》读书笔记（Ch2.LinearAlgebra）</title>
      <link href="/dl/dlbooknotes2/"/>
      <url>/dl/dlbooknotes2/</url>
      
        <content type="html"><![CDATA[<h1 id="2-Linear-Algebra"><a href="#2-Linear-Algebra" class="headerlink" title="2. Linear Algebra"></a>2. Linear Algebra</h1><h2 id="2-1-Scalars-Vectors-Matrices-and-Tensors"><a href="#2-1-Scalars-Vectors-Matrices-and-Tensors" class="headerlink" title="2.1 Scalars, Vectors, Matrices and Tensors"></a>2.1 Scalars, Vectors, Matrices and Tensors</h2><ul><li>Scalars：标量，就是一个数字</li><li>Vectors：向量，默认是列向量</li><li>Matrices：二维矩阵</li><li>Tensors：张量（多于两个axes）</li></ul><h2 id="2-2-Multiplying-Matrices-and-Vectors"><a href="#2-2-Multiplying-Matrices-and-Vectors" class="headerlink" title="2.2 Multiplying Matrices and Vectors"></a>2.2 Multiplying Matrices and Vectors</h2><ol><li><p>矩阵</p><ul><li><p>按位相乘（Element-wise product / Hadamard product）：$A \odot B$</p></li><li><p>矩阵乘法：$AB$</p></li></ul><p>性质：</p><ul><li>$(AB)^T = B^T A^T$</li></ul></li><li><p>向量的dot product</p><p>性质：</p><ul><li>$x^T y = y^T x$</li></ul></li></ol><h2 id="2-3-Identity-and-Inverse-Matrices"><a href="#2-3-Identity-and-Inverse-Matrices" class="headerlink" title="2.3 Identity and Inverse Matrices"></a>2.3 Identity and Inverse Matrices</h2><ol><li><p>$ n \times n $ 的单位矩阵 $I_n$</p></li><li><p>逆矩阵 $A^{-1}$</p></li></ol><h2 id="2-4-Linear-Dependence-and-Span"><a href="#2-4-Linear-Dependence-and-Span" class="headerlink" title="2.4 Linear Dependence and Span"></a>2.4 Linear Dependence and Span</h2><p>概念：</p><ul><li>无解/一个解/无穷多解</li><li>线性组合</li><li>线性张成空间（span）</li><li>线性相关（linear dependence）/线性独立（linearly independent）</li></ul><h2 id="2-5-Norms"><a href="#2-5-Norms" class="headerlink" title="2.5 Norms"></a>2.5 Norms</h2><ol><li><p>$ L^p $范数（$ p \ge 1 $）：（目标：Measure the size of a vector）</p><script type="math/tex; mode=display">||x||_p=(\sum_i|x_i|^p)^{\frac{1}{p}}</script></li><li><p>$ L^p $范数的特殊情形</p><ul><li><p>$ L^2 $ norm： Euclidean norm, denoted as $ ||x|| $; </p><p>Squared $ L^2 $ norm can be calculated as $ x^T x $ </p></li><li><p>$ L^1 $ norm</p></li><li><p>$ L^0 $ norm：非零元素的个数</p></li><li><p>Max norm：$||x||_{\infty} = \max \limits_{i} | x_{i} |$</p></li></ul></li></ol><ol><li><p>Frobenius norm（目标：Measure the size of a matrix）：<script type="math/tex">|| A ||_{ F } = \sqrt { \sum_{i,j} A_{i,j} ^ 2}</script></p></li><li><p>Dot product and angle： $x^T y=||x||_2 ||y||_2 cos \theta$</p></li></ol><h2 id="2-6-Special-Kinds-of-Matrices-and-Vectors"><a href="#2-6-Special-Kinds-of-Matrices-and-Vectors" class="headerlink" title="2.6 Special Kinds of Matrices and Vectors"></a>2.6 Special Kinds of Matrices and Vectors</h2><ol><li><p>Diagonal matrix（对角阵）</p><p>只有主对角线上是非零元素（可以不是方阵）</p><p>性质：</p><ul><li>$diag(v)^{-1} = diag([1/v_1, … , 1/v_n]^T)$</li></ul></li><li><p>Symmetric matrix（对称阵）</p><p>沿着对角线对称（$ A = A^T $）</p></li><li><p>Unit vector（$ ||x||_2 = 1 $）</p></li><li><p>Orthogonal matrix（正交矩阵，$ A^T A=A A^T = I $）</p><p>性质：</p><ul><li>行/列向量都是标准正交的（Orthonormal, 模长为一、相互垂直）</li><li>$ A^{-1} = A^T $</li></ul></li></ol><h2 id="2-7-Eigendecomposition"><a href="#2-7-Eigendecomposition" class="headerlink" title="2.7 Eigendecomposition"></a>2.7 Eigendecomposition</h2><ol><li><p>特征值$ \lambda $和特征向量$ v $，基本性质：</p><script type="math/tex; mode=display">A v = \lambda v</script></li><li><p>类型：</p><ul><li><p>对一个方阵$ A $的特征分解为：</p><script type="math/tex; mode=display">A = V diag(\lambda) V^{-1}</script><p>其中矩阵$ V = [v^{(1)}, …, v^{(n)}]$ ，向量$ \lambda = [\lambda_1, …, \lambda_n]^T $</p></li><li><p>实对称矩阵$ A $的特征分解（eg: 二次型、Hessian阵）：</p><script type="math/tex; mode=display">A = Q \Lambda Q^{-1} = Q \Lambda Q^T</script></li></ul></li></ol><p>   其中$ Q $是由特征向量组成的正交矩阵（具有性质：$ Q^{-1} = Q^T $）；$ \Lambda $是由特征值组成的对角阵（降序排列）</p><ol><li><p>性质：</p><ul><li>矩阵降秩$ \Leftrightarrow $ 至少其中一个特征值为零（因为$ det(A) = \prod \limits_{i} \lambda_i $）</li><li>正定：特征值都是正数；正定矩阵在二次型中的性质：$ \forall x, x^T A x \ge 0 $</li><li>对实对称矩阵$ A $，其特征向量矩阵$ Q $的第$ i $列记作向量$ d_i $（若特征值按降序排列那么特征矩阵也是唯一确定的），第$ i $大的特征值为$ \lambda_i $，则$ \lambda_i = d_i^T A d_i $（PCA分析要用到这条性质）</li><li>乘以矩阵$ A $之后，单位圆被缩放了：</li></ul><p><img src="2.png" alt></p></li></ol><h2 id="2-8-Singular-Value-Decomposition"><a href="#2-8-Singular-Value-Decomposition" class="headerlink" title="2.8 Singular Value Decomposition"></a>2.8 Singular Value Decomposition</h2><ol><li><p>SVD分解（$A$的size: $ m \times n $）：</p><script type="math/tex; mode=display">A = U D V^T</script><p>其中 $ U $（size: $ m \times m $） 和 $ V $（size: $ n \times n $）都是正交矩阵， $ D $（size: $ m \times n $）是对角阵。<br>且$ D $的组成元素是奇异值，$ U $的列向量是左奇异向量，$ V $的列向量是右奇异向量。</p></li><li><p>性质：</p><ul><li>实矩阵皆可SVD</li><li>$ A $的左奇异向量等于$ A A^T $的特征向量</li><li>$ A $的右奇异向量等于$ A^T A $的特征向量</li><li>$ A $的非零奇异值等于$ A^T A $的特征值的平方根，对$ A A^T $而言同理</li></ul></li></ol><h2 id="2-9-The-Moore-Penrose-Pseudoinverse"><a href="#2-9-The-Moore-Penrose-Pseudoinverse" class="headerlink" title="2.9 The Moore-Penrose Pseudoinverse"></a>2.9 The Moore-Penrose Pseudoinverse</h2><ol><li><p>伪逆的定义式：</p><script type="math/tex; mode=display">A^+ = \lim \limits_{\alpha \rightarrow 0} (A^T A + \alpha I)^{-1} A^T</script></li><li><p>计算式（因为定义式往往不好计算）：</p><script type="math/tex; mode=display">A^+ = V D^+ U^T</script><p>其中$ U $ 、 $ D $ 和 $ V $ 是矩阵$ A $ 的SVD分解。而$ D^+ $是通过对$ D $取元素的倒数、转置得到的。</p></li><li><p>性质（考察解方程$ Ax=y $）：</p><ul><li><p>伪逆的用途：当方程严格求解不出来时，使用伪逆可以求得近似解。</p></li><li><p>当方程数不足时（行数少于列数），解应该有无穷多个。使用伪逆求得的唯一解$ x = A^+ y $自动满足性质：$ ||x||_2 $最小（类似于自动求得在L2正则情况下的最优解？）</p></li><li>当方程数很多时（行数多于列数），解可能不存在，对应于直线拟合。使用伪逆求得的唯一解$ x = A^+ y $自动满足性质：$ ||Ax-y||_2 $最小（类似于欧氏误差/回归误差最小？）</li></ul></li></ol><h2 id="2-10-The-Trace-Operator"><a href="#2-10-The-Trace-Operator" class="headerlink" title="2.10 The Trace Operator"></a>2.10 The Trace Operator</h2><ol><li><p>“迹”的定义：对角线元素的和</p><script type="math/tex; mode=display">Tr(A) = \sum_i A_{i,i}</script></li><li><p>用Trace来简便表示：</p><ul><li>表示Frobenius norm：$ ||A||_F = \sqrt{Tr(A A^T)} $</li></ul></li><li><p>Trace的性质：</p><ul><li>$ Tr(A) = Tr(A^T) $</li><li>循环置换性：$ Tr(ABC) = Tr(CAB) = Tr(BCA) $，即可以把最后一个移到第一个（反之亦然），当两个矩阵时：$ Tr(AB) = Tr(BA) $</li><li>对标量$ a $：$ Tr(a) = a $</li></ul></li></ol><h2 id="2-11-The-Determinant"><a href="#2-11-The-Determinant" class="headerlink" title="2.11 The Determinant"></a>2.11 The Determinant</h2><ol><li>定义式：$ det(A) = \prod \limits_{i} v_i $，其中$ v_i $是第$ i $个特征值</li><li>性质：<ul><li>行列式为零：降秩，奇异</li><li>行列式为一：正交矩阵行列式的绝对值必定为一</li></ul></li></ol><h2 id="2-12-Example-Principal-Components-Analysis"><a href="#2-12-Example-Principal-Components-Analysis" class="headerlink" title="2.12 Example: Principal Components Analysis"></a>2.12 Example: Principal Components Analysis</h2><ol><li><p>Principal Components Analysis (PCA) 问题的数学描述（参考英文版花书的Page48）：</p><ul><li><p>数据集：我们有m个点$ \{x^{(1)},…,x^{(m)}\} $，其中对任意一个点$ x^{(i)}\in \mathbb{R}^n $</p></li><li><p>目标：构造从$ x^{(i)} \in \mathbb{R}^n $到$ c^{(i)} \in \mathbb{R}^l $的映射$ f(x)=c $（此处$ l \leq n $），以及复原的映射$ x \approx g(c) $，使得复原的误差最小。从而获得编码向量$ c $</p></li><li><p>选取：复原映射是线性映射$ g(c)=Dc $，其中矩阵$ D $的列向量都是相互正交的，且模长为一（注意$ D $可以不是一个方阵）</p></li></ul></li><li><p>求解：</p><ul><li>Step1：<br>对于一个点$ x $，以及已知的$ D $，求什么样的$ c $使得重建误差最小，相当于求$ c=f(x) $。通过化简以及通过对$c$求导，可以解得$ c = D^T x $，即$ f(x) = D^T x $</li></ul><script type="math/tex; mode=display">\min \limits_{c} ||x-Dc||^2_2</script><ul><li><p>Step2：</p><p>那么完整的重建回来的值就是$ r(x) = D D^T x $。假设$ l=1 $（压缩到1维），那么$ D $就只是一个列向量$ d $，则优化问题就是（求$ d $）</p><script type="math/tex; mode=display">\min_d \sum_i ||x^{(i)}-dd^Tx^{(i)}||^2,\quad subject \; to \; ||d||=1\\\Leftrightarrow \min_d \sum_i ||x^{(i)}-x^{(i)T}dd||^2,\quad subject \; to \; ||d||=1</script><p>我们用design matrix来表示样本，即$ X \in \mathbb{R}^{m \times n} $，矩阵的第$ i $行是第$ i $个样本$ x^{(i)T} $，那么上述问题则可以用矩阵范数表示成</p><script type="math/tex; mode=display">\Leftrightarrow \min_d  ||X-Xdd^T||_F^2,\quad subject \; to \; d^Td=1</script><script type="math/tex; mode=display">\Leftrightarrow \min_d  Tr((X-Xdd^T)^T(X-Xdd^T)),\quad subject \; to \; d^Td=1</script><script type="math/tex; mode=display">\Leftrightarrow \max_d  Tr(d^TX^TXd),\quad subject \; to \; d^Td=1</script><script type="math/tex; mode=display">\Leftrightarrow \max_d  (d^T(X^TX)d),\quad subject \; to \; d^Td=1</script><p>注意到上述最大化问题形似特征值分解问题（$ \lambda_{1} = d_1^T A d_1 $），即求某一个特征向量，使得特征值最大。那么只需要对矩阵$ X^T X $作特征值分解，找到最大的特征值所对应的特征向量，即为所求的$ d $。</p><p>若$ l $为其他情形，类似地，取降序排列的前$ l $个特征值所对的特征向量（模长要为一）构造出矩阵$ D = (d_1, …, d_l) $即为所求的复原映射矩阵。</p></li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> dl </category>
          
      </categories>
      
      
        <tags>
            
            <tag> dl </tag>
            
            <tag> notes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《Deep Learning》读书笔记（Ch1.Introduction）</title>
      <link href="/dl/dlbooknotes1/"/>
      <url>/dl/dlbooknotes1/</url>
      
        <content type="html"><![CDATA[<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p>Knowledge based approach -&gt; Machine learning </p><p>Representation learning: 学会如何表示东西，而不仅仅是从input到output的映射</p><p>Model深度的计算方法有两种：</p><ul><li>计算图的深度（Instructions）</li><li>概率图的深度（Concepts）</li></ul><p>AI领域的维恩图：</p><p><img src="1.png" alt></p><h2 id="1-1-Who-should-read-this-book"><a href="#1-1-Who-should-read-this-book" class="headerlink" title="1.1 Who should read this book?"></a>1.1 Who should read this book?</h2><ul><li>University students</li><li>Software engineers</li></ul><h2 id="1-2-Historical-trends-in-deep-learning"><a href="#1-2-Historical-trends-in-deep-learning" class="headerlink" title="1.2 Historical trends in deep learning"></a>1.2 Historical trends in deep learning</h2><ol><li><p>一波三折：</p><ul><li><p>控制论（Cybernetics, 1940~1960）</p><blockquote><p>Linear model，分类边界是直线，不能XOR</p></blockquote></li><li><p>连结主义（Connectionism, 1980~1990）</p><blockquote><p>小单元的组合可以形成智能<br>BP出现</p></blockquote><p>其他ML方法兴起</p></li><li><p>深度学习（Deep Learning, 2006~NOW）</p><blockquote><p>逐层预训练（最初用在DBN上），更深，硬件更好</p></blockquote></li></ul></li><li><p>特点：</p><ul><li>数据集变大</li><li>模型变大</li><li>精度高、结构复杂、可落地</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> dl </category>
          
      </categories>
      
      
        <tags>
            
            <tag> dl </tag>
            
            <tag> notes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git快速入门与速查</title>
      <link href="/git/git-beginner/"/>
      <url>/git/git-beginner/</url>
      
        <content type="html"><![CDATA[<h1 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h1><p>Git是什么？说起Git你可能会知道它是一个版本管理的工具，但究竟什么是版本管理？</p><p><img src="1.png" alt></p><p>举个栗子，假如你要写某个很牛逼的论文，你先写了个稿子A（包含idea1和idea2）。然后过了一天你发现idea1不work了，不能写在论文里了，然后你把idea1的部分全部删掉了，并将稿子A保存为稿子B。然后又过了一天，你发现idea1又可以work了，可以将其重新写入文章里了，于是你需要将之前写过的关于idea1的部分重头写过。。。</p><p>是不是很麻烦，不过如果你稍微机智一些，你可以在保存稿子B时选择另存为，那么你就会同时拥有稿子A和B了，后面你就可以方便地直接从稿子A恢复回稿子了。不过当文件一多起来，就会变得很麻烦，想想你的目录下有一堆文件的情形。。。</p><p>如果你会使用Git的话，它就能很好地帮你管理这些稿子的“版本”了。可以这么说，你在第一天保存的稿子A是版本A，在第二天删掉了idea1的部分的稿子B是版本B，第三天你就需要将版本B回退到版本A。</p><p>是不是听起来很方便呢？嘿嘿嘿。。。</p><h1 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h1><h2 id="安装配置"><a href="#安装配置" class="headerlink" title="安装配置"></a>安装配置</h2><p>按照网上的教程，安装好Git，并配置好用户信息，配置SSH等等。</p><h2 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h2><p>打开一个文件夹，点出鼠标右键的菜单，选择“Git Bash Here”，然后就能在此处打开Git的命令行界面了。</p><p>本地用法：</p><pre class=" language-lang-bash"><code class="language-lang-bash">git init # git库初始化git add . # 添加当前目录下的所有文件到缓存区git commit -m "first commit" # 提交缓存区的所有文件到git里，并附上提交的说明</code></pre><p>远程用法（在使用了“本地用法”之后）：</p><pre class=" language-lang-bash"><code class="language-lang-bash">git remote add origin [GitHub的上传网址] # 仅第一次需要：添加远端的推送地址git push -u origin master # 第一次推送到远端用这条，以后不用# git push # 以后推送到远端都用这条</code></pre><h2 id="分支管理"><a href="#分支管理" class="headerlink" title="分支管理"></a>分支管理</h2><ol><li><code>git branch dev</code>：创建名字为dev的分支</li><li><code>git checkout dev</code>：切换到分支dev</li><li><code>git branch</code>：查看分支</li><li>然后和平常一样的add和commit等，这些操作会在当前分支（dev）上进行</li><li><code>git checkout master</code>：切换回master分支</li><li><code>git merge dev</code>：将dev分支合并到master分支上</li><li><code>git branch -d dev</code>：删除dev分支</li></ol><h2 id="其他用法"><a href="#其他用法" class="headerlink" title="其他用法"></a>其他用法</h2><ol><li><code>git status</code>：查看状态和提示（例如哪些文件被修改了）</li><li><code>git diff xxx</code>：查看文件xxx被修改了哪里</li><li><code>git log</code>：查看commit的历史记录</li><li><code>git reset --hard HEAD^</code>：回退到上一个版本<br> <code>git reset --hard xxxx</code>：回退到指定的版本号（xxxx），版本号是用<code>git log</code>看到的一大串的数字字母字符串（版本号不需要打全，它会自动搜索和匹配前缀）<br> <code>HEAD</code>：当前版本<br> <code>HEAD^</code>：上一个当前版本<br> <code>HEAD^^</code>：上上一个当前版本<br> <code>HEAD~100</code>：上100个版本<br> （<strong>注意：回退版本号后，你将看不见你刚才在“未来”的版本，需要使用<code>git reflog</code>查看输入命令的历史记录，例如包括了版本号</strong>）</li><li><code>git checkout -- thefile</code>：把文件thefile回退到版本库中的状态（即取出版本库中的该文件，并覆盖到工作区）</li><li><code>git rm thefile</code>：从版本库移除thefile文件（需要再commit才能提交更改）</li><li><code>git clone [GitHub某仓库的下载地址]</code>：把仓库克隆到本地</li><li><code>git pull</code>：从远端拉取修改（获取最新的版本到本地）</li></ol>]]></content>
      
      
      <categories>
          
          <category> git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>如何从源码编译linux内核并使其从u盘启动</title>
      <link href="/linux/linux-u-drive/"/>
      <url>/linux/linux-u-drive/</url>
      
        <content type="html"><![CDATA[<blockquote><p>准备 linux 环境，可以在 VMware 虚拟机里运行。<br>我这里用的是 Archlinux 系统，不同系统大同小异。</p></blockquote><h1 id="编译内核"><a href="#编译内核" class="headerlink" title="编译内核"></a>编译内核</h1><h2 id="安装环境"><a href="#安装环境" class="headerlink" title="安装环境"></a>安装环境</h2><p>首先安装 <code>base-devel</code> 软件组，这个组包含了 make 和 gcc 等需要的软件包。</p><pre class=" language-lang-bash"><code class="language-lang-bash">sudo pacman -S base-devel</code></pre><h2 id="下载内核并解压"><a href="#下载内核并解压" class="headerlink" title="下载内核并解压"></a>下载内核并解压</h2><p>从<a href="https://www.kernel.org/" target="_blank" rel="noopener">这里</a>下载linux内核。<br>我们以版本<a href="https://cdn.kernel.org/pub/linux/kernel/v5.x/linux-5.0.11.tar.xz" target="_blank" rel="noopener">5.0.11</a>为例子。<br>首先新建个文件夹叫 <code>kernelbuild</code> 然后下载内核压缩包到该文件夹</p><pre class=" language-lang-bash"><code class="language-lang-bash">mkdir ~/kernelbuildcd ~/kernelbuildwget https://cdn.kernel.org/pub/linux/kernel/v5.x/linux-5.0.11.tar.xz</code></pre><p>然后我们解压内核压缩包：</p><pre class=" language-lang-bash"><code class="language-lang-bash">tar -xvJf linux-5.0.11.tar.xz</code></pre><p>为确保内核树绝对干净，进入内核目录并执行 <code>make mrproper</code> 命令：</p><pre class=" language-lang-bash"><code class="language-lang-bash">cd linux-5.0.11make clean && make mrproper</code></pre><h2 id="配置内核并编译"><a href="#配置内核并编译" class="headerlink" title="配置内核并编译"></a>配置内核并编译</h2><p>内核配置，会进入配置界面：</p><pre class=" language-lang-bash"><code class="language-lang-bash">make x86_64_defconfigmake menuconfig</code></pre><p><img src="menuconfig.png" alt></p><p>确保选中以下选项（按 y 选中，按 n 取消选择；按上下左右方向键移动选择的光标位置；回车确认）：</p><ul><li><p>General Setup –&gt; [*] Initial RAM filesystem and RAM disk (initramfs/initrd) support</p></li><li><p>Device Drivers –&gt; Block Devices –&gt; [*] RAM block device support</p></li><li><p>Device Drivers -&gt; Input device support -&gt; [*] Keyboard -&gt; 全部选择为[*]</p></li><li><p>Device Drivers -&gt; HID support -&gt; 全部选择为[*]</p></li><li>Device Drivers -&gt; HID support -&gt; USB HID support -&gt; 全部选择为[*]</li></ul><p>然后编译（需要等上十几分钟吧，再此期间你可以继续进行下一步busybox的准备）：</p><pre class=" language-lang-bash"><code class="language-lang-bash">make -j4 bzImage</code></pre><p>最后生成的 <code>arch/x86_64/boot/bzImage</code> 才是我们想要的内核文件（这个文件才8M多而已）</p><h1 id="准备-BusyBox-工具"><a href="#准备-BusyBox-工具" class="headerlink" title="准备 BusyBox 工具"></a>准备 BusyBox 工具</h1><p>到<a href="https://busybox.net/" target="_blank" rel="noopener">官网</a>下载 BusyBox，以1.30.1版本为例。</p><p>下载并解压：</p><pre class=" language-lang-bash"><code class="language-lang-bash">cd ～wget https://busybox.net/downloads/busybox-1.30.1.tar.bz2tar -jxvf busybox-1.30.1.tar.bz2cd busybox-1.30.1/</code></pre><p>然后进行配置：</p><pre class=" language-lang-bash"><code class="language-lang-bash">make defconfigmake menuconfig</code></pre><p>确保选中：</p><ul><li>BusyBox Setting-&gt;Build Options-&gt;[*]Build Busybox as a static binary (no shared libs)</li></ul><p>然后编译（编译很快）：</p><pre class=" language-lang-bash"><code class="language-lang-bash">make</code></pre><p>最后安装：</p><pre class=" language-lang-bash"><code class="language-lang-bash">make install</code></pre><p>执行 ls ，会发现多了一个 <code>_install</code> 目录，我们要用它来构建 linux 的根目录。</p><h1 id="准备-linux-所需文件"><a href="#准备-linux-所需文件" class="headerlink" title="准备 linux 所需文件"></a>准备 linux 所需文件</h1><p>首先在用户目录下新建一个文件夹 romfs ，然后把 <code>_install</code> 目录中的内容全部复制到 <code>romfs</code> 中。</p><pre class=" language-lang-bash"><code class="language-lang-bash">cd ~mkdir romfscp -r busybox-1.30.1/_install/* romfs/</code></pre><p>然后创建我们 linux 目录下的文件夹：</p><pre class=" language-lang-bash"><code class="language-lang-bash">cd romfs/mkdir proc mnt var tmp dev sys etc</code></pre><p>然后创建软链接：</p><pre class=" language-lang-bash"><code class="language-lang-bash">ln -s bin/sh init</code></pre><p>接着我们创建设备：</p><pre class=" language-lang-bash"><code class="language-lang-bash">cd dev/sudo mknod console c 5 1sudo mknod null c 1 3sudo mknod tty c 5 0sudo mknod tty1 c 4 1sudo mknod tty2 c 4 2sudo mknod tty3 c 4 3sudo mknod tty4 c 4 4</code></pre><p>然后制作压缩镜像（可能还要执行<code>sudo pacman -S cpio</code>）：</p><pre class=" language-lang-bash"><code class="language-lang-bash">cd ~/romfs/find . | cpio -H newc -o > ../romfs.imgcd ../gzip romfs.img -f</code></pre><h1 id="在-u-盘建立文件系统和-EFI-引导"><a href="#在-u-盘建立文件系统和-EFI-引导" class="headerlink" title="在 u 盘建立文件系统和 EFI 引导"></a>在 u 盘建立文件系统和 EFI 引导</h1><blockquote><p><strong>这部分教程使用 GPT+EFI 作为启动引导。</strong><br>如果你执意使用 GPT+BIOS 作为引导，则请按照下面的步骤来操作（应该可行）：</p><blockquote><ol><li>创建两个分区：FAT32（例如64MB）和EXT4（例如256MB）。并设置FAT32的启动标识为<code>bios_grub</code>类型。</li><li>创建文件夹/mnt/usb，将EXT4分区挂载到 /mnt/usb，再创建文件夹 /mnt/usb/boot</li><li>然后安装grub：<code>grub-install --target=i386-pc --debug --removable --boot-directory=/mnt/usb/boot /dev/sdd</code></li><li>复制 bzImage 和 romfs.img.gz 过去到 /mnt/usb/boot</li><li>创建 grub.cfg，并写入一些内容（与下面教程的相应部分类似），酌情修改。</li><li>重启电脑，选择从Legacy启动，保存修改，将u盘的优先级调到最高。再次启动即可。</li></ol></blockquote></blockquote><h2 id="u-盘分区"><a href="#u-盘分区" class="headerlink" title="u 盘分区"></a>u 盘分区</h2><p>插入u盘，使用<code>GParted</code>进行分区。</p><ol><li><p>在右上角选中你的u盘（例如我的是/dev/sdd），删除所有分区，然后点菜单栏的勾勾按钮。<br><img src="gparted.png" alt></p></li><li><p>然后新建两个分区，一个是FAT32，一个是EXT4，大小分别为64M和256M足矣。然后打勾勾确定。</p></li><li><p>更改刚刚建立的FAT32分区的标识为<code>boot,esp</code>。</p></li></ol><p><img src="gparted2.png" alt></p><h2 id="安装-Grub-到-u-盘"><a href="#安装-Grub-到-u-盘" class="headerlink" title="安装 Grub 到 u 盘"></a>安装 Grub 到 u 盘</h2><p>首先挂载刚刚建立的 <code>FAT32</code> 分区到 <code>/mnt/usb</code> 目录下：</p><pre class=" language-lang-bash"><code class="language-lang-bash">cd /mntsudo mkdir efi_partitionsudo mkdir boot_partitionsudo mount /dev/sdd1 ./efi_partitionsudo mount /dev/sdd2 ./boot_partition</code></pre><p>然后将 grub 安装到 u 盘：</p><pre class=" language-lang-bash"><code class="language-lang-bash">sudo grub-install --target=x86_64-efi --boot-directory=/mnt/boot_partition --efi-directory=/mnt/efi_partition --bootloader-id=karbo --removable --debug</code></pre><p>安装成功后应该会在最后的输出看到：<code>安装完成。没有报告错误。</code>类似的字样。</p><p>然后将编译好的文件复制过去：</p><pre class=" language-lang-bash"><code class="language-lang-bash">sudo cp ~/kernelbuild/linux-5.0.11/arch/x86_64/boot/bzImage /mnt/boot_partition/sudo cp ~/romfs.img.gz /mnt/boot_partition</code></pre><p>最后创建个文本文件：</p><pre class=" language-lang-bash"><code class="language-lang-bash">sudo micro /mnt/boot_partition/grub/grub.cfg</code></pre><p>内容为：</p><pre class=" language-lang-bash"><code class="language-lang-bash">menuentry "karbo-linux" {set root='hd0,gpt2'linux /bzImage root=/dev/raminitrd /romfs.img.gz}</code></pre><p>最后重启电脑，狂按F2，调整为<code>UEFI</code>启动，并把你u盘启动的优先级调到最顶上来，保存更改，再次启动即可。</p><p>应该会看到类似如下的界面：</p><p><img src="final.jpeg" alt></p><blockquote><p>如果在UEFI里没有找到你的u盘，那么可能是你插入了多个usb存储设备，尝试将其他usb存储设备拔掉，并重试。</p></blockquote><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ol><li><p><a href="https://wiki.archlinux.org/index.php/Kernels/Compilation/Traditional_(%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87" target="_blank" rel="noopener">Kernels/Compilation/Traditional (简体中文)</a>)</p></li><li><p><a href="https://zhuanlan.zhihu.com/p/27009845" target="_blank" rel="noopener">制作 U 盘 Linux</a></p></li><li><p><a href="https://wiki.archlinux.org/index.php/GRUB/Tips_and_tricks_(%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87" target="_blank" rel="noopener">GRUB/Tips and tricks (简体中文)</a>)</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> kernels </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>用Django搭建第一个投票网站</title>
      <link href="/django/python-django/"/>
      <url>/django/python-django/</url>
      
        <content type="html"><![CDATA[<p>本文教你用Django搭建第一个网站，投票网站。</p><p><img src="0.png" alt></p><blockquote><p>我们的投票网站要实现两个站点：</p><ul><li>一个让人们查看和投票的公共站点。</li><li>一个让你能添加、修改和删除投票的管理站点。</li></ul></blockquote><p>我们使用的操作系统：win10</p><p>关键思想：通过服务器上的数据库存储相关信息，通过网站对数据库的信息呈现并修改。</p><h1 id="Django安装和配置"><a href="#Django安装和配置" class="headerlink" title="Django安装和配置"></a>Django安装和配置</h1><p>创建一个项目工程文件夹，以后所有的操作都要在此文件夹下进行，例如我是<code>web</code>文件夹下。打开系统自带的<code>cmd</code>（最好是cmd），并切换到此文件夹下</p><p>例如我的文件夹路径是<code>E:\PythonProjects\web</code>，那么我先输入<code>e:</code>再敲回车，使得切换到<code>E</code>盘，然后再<code>cd E:\PythonProjects\web</code>即可。</p><p>然后在<code>cmd</code>输入（其中<code>my_env</code>是创建的虚拟环境的名字）：</p><pre class=" language-lang-bash"><code class="language-lang-bash">python -m venv my_env</code></pre><p>来创建<code>python</code>的虚拟环境。然后输入：</p><pre class=" language-lang-bash"><code class="language-lang-bash">my_env\Scripts\activate.bat</code></pre><p>来激活虚拟环境，此时<code>cmd</code>提示输入的前端应该有<code>(my_env)</code>字样，表明进入了虚拟环境。<br>若要退出虚拟环境，请输入：<code>my_env\Scripts\deactivate.bat</code></p><p>最后安装<code>Django</code>：</p><pre><code>pip install Django</code></pre><p>安装后，输入：<code>python -m django --version</code>命令，可以查看<code>Django</code>的版本。<br>我这里的版本是<code>2.2</code>。</p><h1 id="建立工程"><a href="#建立工程" class="headerlink" title="建立工程"></a>建立工程</h1><p>紧接着，我们来创建一些建立我们网站所必备的文件，输入下面的代码来创建一个工程。</p><pre class=" language-lang-bash"><code class="language-lang-bash">django-admin startproject mysite</code></pre><p>我们会发现在当前目录下多了一个叫<code>mysite</code>的文件夹，里面的文件是工程的必备文件。<br>这个<code>startproject</code>命令创建了如下的文件结构：</p><pre><code>mysite/        manage.py        mysite/                __init__.py                settings.py                urls.py                wsgi.py</code></pre><p>这些目录和文件的用处是：</p><ol><li>最外层的<code>mysite/</code> 根目录只是你项目的容器， <code>Django</code> 不关心它的名字，你可以将它重命名为任何你喜欢的名字。</li><li><code>manage.py</code>: 一个让你用各种方式管理 <code>Django</code> 项目的命令行工具。你可以阅读 <a href="https://docs.djangoproject.com/zh-hans/2.2/ref/django-admin/" target="_blank" rel="noopener">django-admin and manage.py</a> 获取所有 <code>manage.py</code> 的细节。</li><li>里面一层的 <code>mysite/</code> 目录包含你的项目，它是一个纯 <code>Python</code> 包。它的名字就是当你引用它内部任何东西时需要用到的 <code>Python</code> 包名。 (比如 <code>mysite.urls</code>).</li><li><code>mysite/__init__.py</code>：一个空文件，告诉 <code>Python</code> 这个目录应该被认为是一个 <code>Python</code> 包。如果你是<code>Python</code> 初学者，阅读官方文档中的<a href="https://docs.python.org/3/tutorial/modules.html#tut-packages" target="_blank" rel="noopener">更多关于包的知识</a>。</li><li><code>mysite/settings.py</code>：<code>Django</code> 项目的配置文件。如果你想知道这个文件是如何工作的，请查看 <a href="https://docs.djangoproject.com/zh-hans/2.2/topics/settings/" target="_blank" rel="noopener">Django settings</a> 了解细节。</li><li><code>mysite/urls.py</code>：<code>Django</code> 项目的 <code>URL</code> 声明，就像你网站的“目录”。阅读 <a href="https://docs.djangoproject.com/zh-hans/2.2/topics/http/urls/" target="_blank" rel="noopener">URL调度器</a>文档来获取更多关于<code>URL</code> 的内容。</li><li><code>mysite/wsgi.py</code>：作为你的项目的运行在<code>WSGI</code> 兼容的<code>Web</code>服务器上的入口。阅读如何<a href="https://docs.djangoproject.com/zh-hans/2.2/howto/deployment/wsgi/" target="_blank" rel="noopener">使用 WSGI 进行部署</a>了解更多细节。</li></ol><p>然后我们运行如下命令：</p><pre><code>cd mysitepython manage.py runserver</code></pre><p>来在本地端口运行服务器。我们在浏览器中输入<code>http://localhost:8000/</code>或<code>https://127.0.0.1:8000/</code>来打开网页。正常情况下应该会看到如下界面：</p><p><img src="1.png" alt></p><p>表示已经成功运行啦！<br>注意：</p><ol><li>千万不要将这个服务器用于和生产环境相关的任何地方。这个服务器只是为了开发而设计的。（Django 在 Web 框架方面是专家，在 Web 服务器方面并不是。）</li><li>仅仅是修改代码后，不必重启服务器来查看效果；但是添加新代码文件后则需要重启服务器来查看效果。</li></ol><h1 id="创建应用"><a href="#创建应用" class="headerlink" title="创建应用"></a>创建应用</h1><p>在 <code>Django</code> 中，每一个应用都是一个<code>Python</code> 包，并且遵循着相同的约定。<code>Django</code> 自带一个工具，可以帮你生成应用的基础目录结构，这样你就能专心写代码，而不是创建目录了。</p><blockquote><p><strong>项目 VS 应用</strong><br>项目和应用有啥区别？应用是一个专门做某件事的网络应用程序——比如博客系统，或者公共记录的数据库，或者简单的投票程序。项目则是一个网站使用的配置和应用的集合。项目可以包含很多个应用。应用可以被很多个项目使用。</p></blockquote><p>请确定你现在处于 <code>manage.py</code> 所在的目录下，然后我们使用如下指令来创建一个名为<code>polls</code>的应用：</p><pre><code>python manage.py startapp polls</code></pre><p>这将会创建一个 <code>polls</code> 目录，它的目录结构大致如下：</p><pre><code>polls/        migrations/                __init__.py        __init__.py        admin.py        apps.py        models.py        tests.py        views.py</code></pre><p>这个目录结构包括了投票应用的全部内容。</p><h1 id="编写简单的视图和绑定URL"><a href="#编写简单的视图和绑定URL" class="headerlink" title="编写简单的视图和绑定URL"></a>编写简单的视图和绑定URL</h1><p>打开 <code>polls/views.py</code>，把下面这些 <code>Python</code> 代码添加进去：</p><pre class=" language-lang-python"><code class="language-lang-python">from django.http import HttpResponsedef index(request):    return HttpResponse("Hello, world. You're at the polls index.")</code></pre><p>这是 <code>Django</code> 中最简单的视图。如果想看见效果，我们需要将一个 <code>URL</code> 映射到它——这就是我们需要 <code>URLconf</code> 的原因了。</p><p>为了创建 <code>URLconf</code>，请在 <code>polls</code> 目录里新建一个 <code>urls.py</code> 文件。<br>在 <code>polls/urls.py</code> 中，输入如下代码：</p><pre class=" language-lang-python"><code class="language-lang-python">from django.urls import pathfrom . import viewsurlpatterns = [    path('', views.index, name='index'),]</code></pre><p>一般来说，每一个应用都应该有一个自己的<code>urls.py</code>文件。第一项为空<code>&#39;&#39;</code>表明不再增加<code>url</code>的下级路径；第二项<code>views.index</code>表明调用刚刚编写的<code>views.py</code>文件里的<code>index()</code>函数来响应请求。</p><p>下一步是要在根 <code>URLconf</code> 文件（可以理解为全局的<code>url</code>配置文件）中记录我们刚刚创建的 <code>polls.urls</code> 模块。在 <code>mysite/urls.py</code> 文件的 <code>urlpatterns</code> 列表里插入一个 <code>include()</code>， 如下：</p><pre class=" language-lang-python"><code class="language-lang-python">from django.contrib import adminfrom django.urls import include, pathurlpatterns = [    path('polls/', include('polls.urls')),    path('admin/', admin.site.urls),]</code></pre><p>函数 <code>include()</code> 允许引用其它 <code>URLconfs</code>。每当 <code>Django</code> 遇到 <code>include</code> 时，它会截断与此项匹配的 <code>URL</code> 的部分，并将剩余的字符串发送到 <code>URLconf</code> 以供进一步处理。举个例子，就是假如输入的地址是<code>http://localhost:8000/polls/</code>，那么它就会截断匹配的部分，在这个例子里也就是整个<code>http://localhost:8000/polls/</code>，剩下的空字符串<code>&#39;&#39;</code>给<code>polls.urls</code>的<code>urlpatterns</code>去做匹配处理，恰好匹配到空字符串<code>&#39;&#39;</code>，然后就会调用<code>polls.view.index</code>函数去处理。</p><p><code>Django</code>设计 <code>include()</code> 的理念是使其可以即插即用。因为投票应用有它自己的 <code>URLconf( polls/urls.py )</code>，他们能够被放在 <code>&quot;/polls/&quot;</code> ，<code>&quot;/fun_polls/&quot;</code> ，<code>&quot;/content/polls/&quot;</code>，或者其他任何路径下，这个应用都能够正常工作。</p><blockquote><p><strong>何时使用 include()</strong><br>当包括其它 <code>URL</code> 文件时你应该总是使用 <code>include()</code>，而 <code>admin.site.urls</code> 是例外。</p></blockquote><p>你刚刚已经把视图<code>view</code>与<code>url</code>绑定了，输入：</p><pre class=" language-lang-bash"><code class="language-lang-bash">python manage.py runserver</code></pre><p>来运行服务器，在浏览器输入<code>http://localhost:8000/polls/</code>，正常情况下应该看到字符串：<code>Hello, world. You&#39;re at the polls index.</code></p><blockquote><p>函数 <code>path()</code> 具有四个参数，两个必须的参数：<code>route</code> 和 <code>view</code>，两个可选参数：<code>kwargs</code> 和 <code>name</code>。现在，是时候来研究这些参数的含义了。<br><strong>path() 参数： route</strong><br>route 是一个匹配 URL 的准则（类似正则表达式）。当 Django 响应一个请求时，它会从 urlpatterns 的第一项开始，按顺序依次匹配列表中的项，直到找到匹配的项。<br>这些准则不会匹配 GET 和 POST 参数或域名。例如，URLconf 在处理请求 <a href="https://www.example.com/myapp/" target="_blank" rel="noopener">https://www.example.com/myapp/</a> 时，它会尝试匹配 myapp/ 。处理请求 <a href="https://www.example.com/myapp/?page=3" target="_blank" rel="noopener">https://www.example.com/myapp/?page=3</a> 时，也只会尝试匹配 myapp/。<br><strong>path() 参数： view</strong><br>当 Django 找到了一个匹配的准则，就会调用这个特定的视图函数，并传入一个 HttpRequest 对象作为第一个参数，被“捕获”的参数以关键字参数的形式传入。稍后，我们会给出一个例子。<br><strong>path() 参数： kwargs</strong><br>任意个关键字参数可以作为一个字典传递给目标视图函数。本教程中不会使用这一特性。<br><strong>path() 参数： name</strong><br>为你的 URL 取名能使你在 Django 的任意地方唯一地引用它，尤其是在模板中。这个有用的特性允许你只改一个文件就能全局地修改某个 URL 模式。</p></blockquote><h1 id="数据库配置"><a href="#数据库配置" class="headerlink" title="数据库配置"></a>数据库配置</h1><h2 id="setting-py文件"><a href="#setting-py文件" class="headerlink" title="setting,py文件"></a>setting,py文件</h2><p>现在，打开 <code>mysite/settings.py</code> 。这是个包含了 <code>Django</code> 项目设置的 <code>Python</code> 模块。</p><p>通常，这个配置文件使用 <code>SQLite</code> 作为默认数据库。如果你不熟悉数据库，或者只是想尝试下 <code>Django</code>，这是最简单的选择。<code>Python</code> 内置 <code>SQLite</code>，所以你无需安装额外东西来使用它。如果你使用 <code>SQLite</code>，那么你不需要在使用前做任何事——数据库会在需要的时候自动创建。</p><p>此外，关注一下文件头部的 <code>INSTALLED_APPS</code> 设置项。这里包括了会在你项目中启用的所有 <code>Django</code> 应用。应用能在多个项目中使用，你也可以打包并且发布应用，让别人使用它们。</p><p>通常， <code>INSTALLED_APPS</code> 默认包括了以下 Django 的自带应用：</p><ul><li><code>django.contrib.admin</code> — 管理员站点， 你很快就会使用它。</li><li><code>django.contrib.auth</code> — 认证授权系统。</li><li><code>django.contrib.contenttypes</code> — 内容类型框架。</li><li><code>django.contrib.sessions</code> — 会话框架。</li><li><code>django.contrib.messages</code> — 消息框架。</li><li><code>django.contrib.staticfiles</code> — 管理静态文件的框架。</li></ul><p>这些应用被默认启用是为了给常规项目提供方便。</p><p>默认开启的某些应用需要至少一个数据表，所以，在使用他们之前，需要在数据库中创建一些表。请执行以下命令：</p><pre><code>python manage.py migrate</code></pre><p>这个 <code>migrate</code> 命令检查 <code>INSTALLED_APPS</code> 设置，为其中的每个应用创建需要的数据表，至于具体会创建什么，这取决于你的 <code>mysite/settings.py</code> 设置文件和每个应用的数据库迁移文件（我们稍后会介绍这个）。这个命令所执行的每个迁移操作都会在终端中显示出来。<code>migrate</code>命令只会为在 <code>INSTALLED_APPS</code> 里声明了的应用进行数据库迁移。</p><h2 id="创建模型"><a href="#创建模型" class="headerlink" title="创建模型"></a>创建模型</h2><p>在 <code>Django</code> 里写一个数据库驱动的 <code>Web</code> 应用的第一步是定义模型 - 也就是数据库结构设计和附加的其它元数据。</p><p>在这个简单的投票应用中，需要创建两个模型：问题 <code>Question</code> 和选项 <code>Choice</code>。<code>Question</code> 模型包括问题描述和发布时间。<code>Choice</code> 模型有两个字段，选项描述和当前得票数。每个选项属于一个问题。</p><p>这些概念可以通过一个简单的 <code>Python</code> 类来描述。按照下面的例子来编辑 <code>polls/models.py</code> 文件：</p><pre class=" language-lang-python"><code class="language-lang-python">from django.db import modelsclass Question(models.Model):    question_text = models.CharField(max_length=200)    pub_date = models.DateTimeField('date published')class Choice(models.Model):    question = models.ForeignKey(Question, on_delete=models.CASCADE)    choice_text = models.CharField(max_length=200)    votes = models.IntegerField(default=0)</code></pre><p>每个字段都是 <code>Field</code> 类的实例 - 比如，字符字段被表示为 <code>CharField</code> ，日期时间字段被表示为 <code>DateTimeField</code> 。这将告诉 <code>Django</code> 每个字段要处理的数据类型。</p><p>每个 <code>Field</code> 类实例变量的名字（例如 <code>question_text</code> 或 <code>pub_date</code> ）也是字段名，所以最好使用对机器友好的格式。你将会在 <code>Python</code> 代码里使用它们，而数据库会将它们作为列名。</p><p>你可以使用可选的选项来为 <code>Field</code> 定义一个人类可读的名字。这个功能在很多 <code>Django</code> 内部组成部分中都被使用了，而且作为文档的一部分。如果某个字段没有提供此名称，<code>Django</code> 将会使用对机器友好的名称，也就是变量名。在上面的例子中，我们只为 <code>Question.pub_date</code> 定义了对人类友好的名字。对于模型内的其它字段，它们的机器友好名也会被作为人类友好名使用。</p><p>注意在最后，我们使用 <code>ForeignKey</code> 定义了一个关系。这将告诉 <code>Django</code>，每个 <code>Choice</code> 对象都关联到一个 <code>Question</code> 对象。<code>Django</code> 支持所有常用的数据库关系：多对一、多对多和一对一。</p><h2 id="激活模型"><a href="#激活模型" class="headerlink" title="激活模型"></a>激活模型</h2><p>首先得把 <code>polls</code> 应用安装到我们的项目里。</p><blockquote><p><strong>设计哲学</strong><br><code>Django</code> 应用是“可插拔”的。你可以在多个项目中使用同一个应用。除此之外，你还可以发布自己的应用，因为它们并不会被绑定到当前安装的 <code>Django</code> 上。</p></blockquote><p>为了在我们的工程中包含这个应用，我们需要在配置类 <code>INSTALLED_APPS</code> 中添加设置。因为 <code>PollsConfig</code> 类写在文件 <code>polls/apps.py</code> 中，所以它的点式路径是 <code>&#39;polls.apps.PollsConfig&#39;</code>。在文件 <code>mysite/settings.py</code> 中 <code>INSTALLED_APPS</code>子项添加点式路径后，它看起来像这样：</p><pre><code>INSTALLED_APPS = [    &#39;polls.apps.PollsConfig&#39;,    &#39;django.contrib.admin&#39;,    &#39;django.contrib.auth&#39;,    &#39;django.contrib.contenttypes&#39;,    &#39;django.contrib.sessions&#39;,    &#39;django.contrib.messages&#39;,    &#39;django.contrib.staticfiles&#39;,]</code></pre><p>现在你的 <code>Django</code> 项目会包含 <code>polls</code> 应用。接着运行下面的命令：</p><pre><code>python manage.py makemigrations polls</code></pre><p><code>makemigrations</code>命令会在<code>polls/migrations/</code>目录下生成一些关于<code>polls/models.py</code>修改记录的文件。但是并没有修改到数据库。</p><p>你将会看到类似于下面这样的输出：</p><pre><code>Migrations for &#39;polls&#39;:  polls/migrations/0001_initial.py:    - Create model Choice    - Create model Question    - Add field question to choice</code></pre><p>现在，再次运行 <code>migrate</code> 命令，在数据库里创建刚刚建立的模型的数据表：</p><pre><code>python manage.py migrate</code></pre><p>这会使得数据库得以修改，使数据库包含了我们刚刚新建立的模型参数。</p><p>我们应该会看到类似的输出：</p><pre><code>Operations to perform:  Apply all migrations: admin, auth, contenttypes, polls, sessionsRunning migrations:  Rendering model states... DONE  Applying polls.0001_initial... OK</code></pre><p>最后一行表明，模型更改成功同步于数据库上（在数据库里创建了与模型相关的存储结构）。</p><p>这个 <code>migrate</code> 命令选中所有还没有执行过的迁移（<code>Django</code> 通过在数据库中创建一个特殊的表 <code>django_migrations</code> 来跟踪执行过哪些迁移）并应用在数据库上 - 也就是将你对模型的更改同步到数据库结构上。</p><p>迁移是非常强大的功能，它能让你在开发过程中持续的改变数据库结构而不需要重新删除和创建表 - 它专注于使数据库平滑升级而不会丢失数据。我们会在后面的教程中更加深入的学习这部分内容。</p><p>现在，你只需要记住，改变模型并生效只需要这三步：</p><ol><li>编辑 <code>models.py</code> 文件，改变模型。</li><li>运行 <code>python manage.py makemigrations &lt;app_name&gt;</code> 为模型的改变生成迁移文件。</li><li>运行 <code>python manage.py migrate</code> 来应用数据库迁移。</li></ol><h1 id="Django命令行"><a href="#Django命令行" class="headerlink" title="Django命令行"></a>Django命令行</h1><p>运行：</p><pre><code>python manage.py shell</code></pre><p>来打开<code>Django</code>的命令行。</p><p>我们使用这个命令而不是简单的使用 <code>Python</code> 是因为 <code>manage.py</code> 会设置 <code>DJANGO_SETTINGS_MODULE</code> 环境变量，这个变量会让 <code>Django</code> 根据 <code>mysite/settings.py</code> 文件来设置 <code>Python</code> 包的导入路径。</p><p>当你成功进入命令行后，来试试数据库的<code>API</code> 吧，它允许你从数据库中读取和修改<code>Choice</code>和<code>Question</code>的数据。</p><p>输入下述Python命令：</p><pre class=" language-lang-python"><code class="language-lang-python">from polls.models import Choice, Question  #导入我们刚刚写的类Question.objects.all() #执行后输出<QuerySet []>，表明Question类还没有对象（还没有数据）from django.utils import timezone #导入时区q = Question(question_text="What's new?", pub_date=timezone.now()) #创建一个问题q.save() #将q保存到数据库中q.id # 现在它有id值了（为1）q.question_text #输出："What's new?"q.pub_date #输出时间q.question_text = "What's up?" #再次改变question_text值q.save() #再次保存到数据库中Question.objects.all() #展示数据库中Question的所有数据</code></pre><p>但是，我们注意到，输入<code>Question.objects.all()</code>时，返回的<code>&lt;QuerySet [&lt;Question: Question object (1)&gt;]&gt;</code>值，并没有让我们了解到这个对象具体是什么样的，我们想让它有个对自己的描述，让我们清楚这是个什么样的对象。</p><p>让我们通过编辑 <code>Question</code> 模型的代码（位于 <code>polls/models.py</code> 中）来修复这个问题。给 <code>Question</code> 和 <code>Choice</code> 增加 <code>__str__()</code>方法：</p><pre class=" language-lang-python"><code class="language-lang-python">from django.db import modelsclass Question(models.Model):    # ...    def __str__(self):        return self.question_textclass Choice(models.Model):    # ...    def __str__(self):        return self.choice_text</code></pre><p>给模型增加<code>__str__()</code> 方法是很重要的，这不仅仅能给你在命令行里使用带来方便，<code>Django</code> 自动生成的 <code>admin</code> 里也使用这个方法来表示对象。</p><p>注意：这些都是常规的 <code>Python</code>方法。让我们添加一个自定义的方法，这只是为了演示：</p><pre class=" language-lang-python"><code class="language-lang-python">import datetimefrom django.db import modelsfrom django.utils import timezoneclass Question(models.Model):    # ...    def was_published_recently(self):        return self.pub_date >= timezone.now() - datetime.timedelta(days=1)</code></pre><p>新加入的 <code>import datetime</code> 和 <code>from django.utils import timezone</code> 分别导入了 Python 的标准 <code>datetime</code> 模块和 <code>Django</code> 中和时区相关的 <code>django.utils.timezone</code>工具模块。如果你不太熟悉 <code>Python</code> 中的时区处理，看看 <a href="https://docs.djangoproject.com/zh-hans/2.2/topics/i18n/timezones/" target="_blank" rel="noopener">时区支持文档</a> 吧。</p><p>保存文件然后通过 <code>python manage.py shell</code> 命令再次打开 <code>Python</code> 交互式命令行：</p><pre class=" language-lang-python"><code class="language-lang-python">from polls.models import Choice, QuestionQuestion.objects.all() #输出：<QuerySet [<Question: What's up?>]>Question.objects.filter(id=1) #依据id查找对象Question.objects.filter(question_text__startswith='What') #依据question_text内容查找对象from django.utils import timezonecurrent_year = timezone.now().year #今年Question.objects.get(pub_date__year=current_year) #依据年份查找对象Question.objects.get(id=2) #访问不存在的对象会引起异常Question.objects.get(pk=1) #大多时候，该语句等价于Question.objects.get(id=1)q = Question.objects.get(pk=1)q.was_published_recently() #调用was_published_recently()方法q = Question.objects.get(pk=1)q.choice_set.all() #显示与q相关联的choice，返回<QuerySet []>，注：choice_set是由于ForeignKey自动生成的子成员# 创建三个choice，会在Choice自动创建对象q.choice_set.create(choice_text='Not much', votes=0)q.choice_set.create(choice_text='The sky', votes=0)c = q.choice_set.create(choice_text='Just hacking again', votes=0)c.question #Choice对象访问自己的question子属性，返回<Question: What's up?># Question对象反之亦能关联到Choice对象q.choice_set.all() #返回q关联到的choice的集合q.choice_set.count() #返回q关联到的choice的个数Choice.objects.filter(question__pub_date__year=current_year) #依据年份返回choice的集合c = q.choice_set.filter(choice_text__startswith='Just hacking') #获取一个choicec.delete() #删除这个choice</code></pre><p>阅读 <a href="https://docs.djangoproject.com/zh-hans/2.2/ref/models/relations/" target="_blank" rel="noopener">访问关系对象</a> 文档可以获取关于数据库关系的更多内容。想知道关于双下划线的更多用法，参见 <a href="https://docs.djangoproject.com/zh-hans/2.2/topics/db/queries/#field-lookups-intro" target="_blank" rel="noopener">查找字段</a> 文档。数据库 API 的所有细节可以在 <a href="https://docs.djangoproject.com/zh-hans/2.2/topics/db/queries/" target="_blank" rel="noopener">数据库 API 参考</a> 文档中找到。</p><h1 id="Django管理页面"><a href="#Django管理页面" class="headerlink" title="Django管理页面"></a>Django管理页面</h1><p>管理界面不是为了网站的访问者，而是为管理者准备的。</p><h2 id="创建一个管理员账号"><a href="#创建一个管理员账号" class="headerlink" title="创建一个管理员账号"></a>创建一个管理员账号</h2><p>首先，我们得创建一个能登录管理页面的用户。请运行下面的命令：</p><pre><code>python manage.py createsuperuser</code></pre><p>然后输入用户名、邮箱和两次密码，以创建管理员账号。</p><h2 id="进入管理界面"><a href="#进入管理界面" class="headerlink" title="进入管理界面"></a>进入管理界面</h2><p><code>Django</code> 的管理界面默认就是启用的。让我们启动开发服务器，看看它到底是什么样的。<br>如果开发服务器未启动，用以下命令启动它：</p><pre><code>python manage.py runserver</code></pre><p>然后输入浏览器地址：<code>http://localhost:8000/admin/</code>来打开管理页面，输入账号和密码登陆即可。</p><p>正常情况下，你会看到类似如下图的界面：</p><p><img src="2.png" alt></p><p>其中<code>Groups</code>和<code>Users</code>是可编辑的。它们是由 <code>django.contrib.auth</code> 提供的，这是 <code>Django</code> 开发的认证框架。</p><h2 id="向管理页面中加入投票应用"><a href="#向管理页面中加入投票应用" class="headerlink" title="向管理页面中加入投票应用"></a>向管理页面中加入投票应用</h2><p>但是我们的投票应用在哪呢？它没在索引页面里显示。</p><p>只需要做一件事：我们得告诉管理页面，问题 <code>Question</code> 对象需要被管理。打开 <code>polls/admin.py</code> 文件，把它编辑成下面这样：</p><pre class=" language-lang-python"><code class="language-lang-python">from django.contrib import adminfrom .models import Questionadmin.site.register(Question)</code></pre><p>类似地，你也可以往<code>admin</code>中添加<code>Choice</code>类。</p><p>添加<code>Question</code>类后，刷新界面，应该会是这个样子：</p><p><img src="3.png" alt></p><p>你可以在这个网页管理页面，管理你的数据库对象，修改<code>Question</code>的数据，或是增删用户等。</p><p>如果你发现<code>Django</code>的时间不对，请修改<code>mysite/settings.py</code>中的<code>TIME_ZONE = &#39;Asia/Shanghai&#39;</code>。</p><h1 id="编写视图"><a href="#编写视图" class="headerlink" title="编写视图"></a>编写视图</h1><p>而在我们的投票应用中，我们需要下列几个视图：</p><ul><li>问题索引页——展示最近的几个投票问题。</li><li>问题详情页——展示某个投票的问题和不带结果的选项列表。</li><li>问题结果页——展示某个投票的结果。</li><li>投票处理器——用于响应用户为某个问题的特定选项投票的操作。</li></ul><p>在 Django 中，网页和其他内容都是从视图派生而来。每一个视图表现为一个简单的 Python 函数（或者说方法，如果是在基于类的视图里的话）。Django 将会根据用户请求的 URL 来选择使用哪个视图（更准确的说，是根据 URL 中域名之后的部分）。</p><p>为了将 URL 和视图关联起来，Django 使用了 URLconfs 来配置。URLconf 将 URL 模式映射到视图。本教程只会介绍 URLconf 的基础内容，你可以看看 <a href="https://docs.djangoproject.com/zh-hans/2.2/topics/http/urls/" target="_blank" rel="noopener">URL调度器</a> 以获取更多内容。</p><h2 id="编写更多视图"><a href="#编写更多视图" class="headerlink" title="编写更多视图"></a>编写更多视图</h2><p>现在让我们向 <code>polls/views.py</code> 里添加更多视图。这些视图有一些不同，因为他们接收参数：</p><pre class=" language-lang-python"><code class="language-lang-python">def detail(request, question_id):    return HttpResponse("You're looking at question %s." % question_id)def results(request, question_id):    response = "You're looking at the results of question %s."    return HttpResponse(response % question_id)def vote(request, question_id):    return HttpResponse("You're voting on question %s." % question_id)</code></pre><p>把这些新视图添加进 <code>polls.urls</code> 模块里，只要添加几个 <a href="https://docs.djangoproject.com/zh-hans/2.2/ref/urls/#django.conf.urls.url" target="_blank" rel="noopener"><code>url()</code></a> 函数调用就行：</p><pre class=" language-lang-python"><code class="language-lang-python">from django.urls import pathfrom . import viewsurlpatterns = [    # example: /polls/    path('', views.index, name='index'),    # example: /polls/5/    path('<int:question_id>/', views.detail, name='detail'),    # example: /polls/5/results/    path('<int:question_id>/results/', views.results, name='results'),    # example: /polls/5/vote/    path('<int:question_id>/vote/', views.vote, name='vote'),]</code></pre><p>然后看看你的浏览器，如果你转到 <code>/polls/34/</code> ，Django 将会运行 <code>detail()</code> 方法并且展示你在 URL 里提供的问题 ID。再试试 <code>/polls/34/vote/</code> 和 <code>/polls/34/vote/</code> ——你将会看到暂时用于占位的结果和投票页。</p><p>当某人请求你网站的某一页面时——比如说，<code>/polls/34/</code> ，Django 将会载入 <code>mysite.urls</code> 模块，因为这在配置项 <a href="https://docs.djangoproject.com/zh-hans/2.2/ref/settings/#std:setting-ROOT_URLCONF" target="_blank" rel="noopener"><code>ROOT_URLCONF</code></a> 中设置了。然后 Django 寻找名为 <code>urlpatterns</code> 变量并且按序匹配正则表达式。在找到匹配项 <code>polls/</code>，它切掉了匹配的文本（<code>polls/</code>），将剩余文本——<code>34/</code>，发送至 <code>polls.urls</code> ，URLconf 做进一步处理。在这里剩余文本匹配了 <code>&lt;int:question_id&gt;/</code>，使得我们 Django 以如下形式调用 <code>detail()</code>:</p><pre class=" language-lang-python"><code class="language-lang-python">detail(request=<HttpRequest object>, question_id=34)</code></pre><p><code>question_id=34</code> 由 <code>&lt;int:question_id&gt;</code> 匹配生成。使用尖括号“捕获”这部分 URL，且以关键字参数的形式发送给视图函数。上述字符串的 <code>:question_id&gt;</code> 部分定义了将被用于区分匹配模式的变量名，而 <code>int:</code> 则是一个转换器决定了应该以什么变量类型匹配这部分的 URL 路径。</p><p>为每个 URL 加上不必要的东西，例如 <code>.html</code> ，是没有必要的。不过如果你非要加的话，也是可以的:</p><pre class=" language-lang-python"><code class="language-lang-python">path('polls/latest.html', views.index),</code></pre><h2 id="写一个真正有用的视图"><a href="#写一个真正有用的视图" class="headerlink" title="写一个真正有用的视图"></a>写一个真正有用的视图</h2><p>每个视图必须要做的只有两件事：返回一个包含被请求页面内容的 <a href="https://docs.djangoproject.com/zh-hans/2.2/ref/request-response/#django.http.HttpResponse" target="_blank" rel="noopener"><code>HttpResponse</code></a> 对象，或者抛出一个异常，比如 <a href="https://docs.djangoproject.com/zh-hans/2.2/topics/http/views/#django.http.Http404" target="_blank" rel="noopener"><code>Http404</code></a> 。至于你还想干些什么，随便你。</p><p>你的视图可以从数据库里读取记录，可以使用一个模板引擎（比如 Django 自带的，或者其他第三方的），可以生成一个 PDF 文件，可以输出一个 XML，创建一个 ZIP 文件，你可以做任何你想做的事，使用任何你想用的 Python 库。</p><p>因为 Django 自带的数据库 API 很方便，我们曾在 <a href="https://docs.djangoproject.com/zh-hans/2.2/intro/tutorial02/" target="_blank" rel="noopener">教程第 2 部分</a> 中学过，所以我们试试在视图里使用它。我们在 <code>index()</code> 函数里插入了一些新内容，让它能展示数据库里以发布日期排序的最近 5 个投票问题，以空格分割：</p><pre class=" language-lang-python"><code class="language-lang-python">from django.http import HttpResponsefrom .models import Questiondef index(request):    latest_question_list = Question.objects.order_by('-pub_date')[:5]    output = ', '.join([q.question_text for q in latest_question_list])    return HttpResponse(output)# Leave the rest of the views (detail, results, vote) unchanged</code></pre><p>这里有个问题：页面的设计写死在视图函数的代码里的。如果你想改变页面的样子，你需要编辑 Python 代码。</p><p>所以让我们使用 Django 的模板系统，只要创建一个视图，就可以将页面的设计从代码中分离出来。</p><h3 id="使用模板"><a href="#使用模板" class="headerlink" title="使用模板"></a>使用模板</h3><p>首先，在你的 <code>polls</code> 目录里创建一个 <code>templates</code> 目录。Django 将会自动地在这个目录里查找模板文件。</p><p>你项目的 <a href="https://docs.djangoproject.com/zh-hans/2.2/ref/settings/#std:setting-TEMPLATES" target="_blank" rel="noopener"><code>TEMPLATES</code></a> 配置项描述了 Django 如何载入和渲染模板。默认的设置文件设置了 <code>DjangoTemplates</code> 后端，并将 <a href="https://docs.djangoproject.com/zh-hans/2.2/ref/settings/#std:setting-TEMPLATES-APP_DIRS" target="_blank" rel="noopener"><code>APP_DIRS</code></a> 设置成了 True。这一选项将会让 <code>DjangoTemplates</code>在每个 <a href="https://docs.djangoproject.com/zh-hans/2.2/ref/settings/#std:setting-INSTALLED_APPS" target="_blank" rel="noopener"><code>INSTALLED_APPS</code></a> 文件夹中寻找 “templates” 子目录。这就是为什么尽管我们没有像在第二部分中那样修改 DIRS 设置，Django 也能正确找到 polls 的模板位置的原因。</p><p>在你刚刚创建的 <code>templates</code> 目录里，再创建一个目录 <code>polls</code>，然后在其中新建一个文件 <code>index.html</code> 。换句话说，你的模板文件的路径应该是 <code>polls/templates/polls/index.html</code>。因为 Django 会寻找到对应的 <code>app_directories</code> ，所以你只需要使用 <code>polls/index.html</code> 就可以引用到这一模板了。</p><blockquote><p><strong>模板命名空间</strong></p><p>虽然我们现在可以将模板文件直接放在 <code>polls/templates</code> 文件夹中（而不是再建立一个 <code>polls</code> 子文件夹），但是这样做不太好。Django 将会选择第一个匹配的模板文件，如果你有一个模板文件正好和另一个应用中的某个模板文件重名，Django 没有办法 <em>区分</em> 它们。我们需要帮助 Django 选择正确的模板，最简单的方法就是把他们放入各自的 <em>命名空间</em> 中，也就是把这些模板放入一个和 <em>自身</em> 应用重名的子文件夹里。</p><p>换句话说，Django不会区分<code>templates</code> 文件夹的父文件夹，即需要 <code>templates</code> 文件夹下的子文件夹来区分所属的应用程序。</p></blockquote><p>将下面的代码输入到刚刚创建的模板文件中：</p><pre class=" language-lang-html"><code class="language-lang-html">{ % if latest_question_list % }    <ul>    { % for question in latest_question_list % }        <li><a href="/polls/{{ question.id }}/">{{ question.question_text }}</a></li>    { % endfor % }    </ul>{ % else % }    <p>No polls are available.</p>{ % endif % }</code></pre><p>然后，让我们更新一下 <code>polls/views.py</code> 里的 <code>index</code> 视图来使用模板：</p><pre class=" language-lang-python"><code class="language-lang-python">from django.http import HttpResponsefrom django.template import loaderfrom .models import Questiondef index(request):    latest_question_list = Question.objects.order_by('-pub_date')[:5]    template = loader.get_template('polls/index.html')    context = {        'latest_question_list': latest_question_list,    }    return HttpResponse(template.render(context, request))</code></pre><p>上述代码的作用是，载入 <code>polls/index.html</code> 模板文件，并且向它传递一个上下文(context)。这个上下文是一个字典，它将模板内的变量映射为 Python 对象。</p><p>用你的浏览器访问 <code>/polls/</code> ，你将会看见一个无序列表，列出了我们添加的 “What’s up” 投票问题，链接指向这个投票的详情页。</p><h3 id="一个快捷函数：render"><a href="#一个快捷函数：render" class="headerlink" title="一个快捷函数：render()"></a>一个快捷函数：render()</h3><p>「载入模板，填充上下文，再返回由它生成的 <a href="https://docs.djangoproject.com/zh-hans/2.2/ref/request-response/#django.http.HttpResponse" target="_blank" rel="noopener"><code>HttpResponse</code></a> 对象」是一个非常常用的操作流程。于是 Django 提供了一个快捷函数，我们用它来重写 <code>index()</code> 视图：</p><pre class=" language-lang-python"><code class="language-lang-python">from django.shortcuts import renderfrom .models import Questiondef index(request):    latest_question_list = Question.objects.order_by('-pub_date')[:5]    context = {'latest_question_list': latest_question_list}    return render(request, 'polls/index.html', context)</code></pre><p>注意到，我们不再需要导入 <a href="https://docs.djangoproject.com/zh-hans/2.2/topics/templates/#module-django.template.loader" target="_blank" rel="noopener"><code>loader</code></a> 和 <a href="https://docs.djangoproject.com/zh-hans/2.2/ref/request-response/#django.http.HttpResponse" target="_blank" rel="noopener"><code>HttpResponse</code></a> 。</p><p><code>render()</code>函数以request作为第一个参数，模板的路径为第二个参数，第三个的字典参数为可选参数。然后返回<a href="https://docs.djangoproject.com/zh-hans/2.2/ref/request-response/#django.http.HttpResponse" target="_blank" rel="noopener"><code>HttpResponse</code></a>类型的对象。</p><h3 id="抛出-404-错误"><a href="#抛出-404-错误" class="headerlink" title="抛出 404 错误"></a>抛出 404 错误</h3><p>现在，我们来处理投票详情视图——它会显示指定投票的问题标题。下面是这个视图的代码：</p><pre class=" language-lang-python"><code class="language-lang-python">from django.http import Http404from django.shortcuts import renderfrom .models import Question# ...def detail(request, question_id):    try:        question = Question.objects.get(pk=question_id)    except Question.DoesNotExist:        raise Http404("Question does not exist")    return render(request, 'polls/detail.html', {'question': question})</code></pre><p>这里有个新原则。如果指定问题 ID 所对应的问题不存在，这个视图就会抛出一个 <a href="https://docs.djangoproject.com/zh-hans/2.2/topics/http/views/#django.http.Http404" target="_blank" rel="noopener"><code>Http404</code></a> 异常。</p><p>我们稍后再讨论你需要在 <code>polls/detail.html</code> 里输入什么，但是如果你想试试上面这段代码是否正常工作的话，你可以暂时把下面这段输进去：</p><pre class=" language-lang-html"><code class="language-lang-html">{{ question }}</code></pre><p>这样你就能测试了。</p><p>输入不存在的id会出现类似如下的结果：</p><p><img src="4.png" alt></p><h3 id="一个快捷函数：get-object-or-404"><a href="#一个快捷函数：get-object-or-404" class="headerlink" title="一个快捷函数：get_object_or_404()"></a>一个快捷函数：get_object_or_404()</h3><p>尝试用 <a href="https://docs.djangoproject.com/zh-hans/2.2/ref/models/querysets/#django.db.models.query.QuerySet.get" target="_blank" rel="noopener"><code>get()</code></a> 函数获取一个对象，如果不存在就抛出 <a href="https://docs.djangoproject.com/zh-hans/2.2/topics/http/views/#django.http.Http404" target="_blank" rel="noopener"><code>Http404</code></a> 错误也是一个普遍的流程。Django 也提供了一个快捷函数，下面是修改后的详情 <code>detail()</code> 视图代码：</p><pre class=" language-lang-python"><code class="language-lang-python">from django.shortcuts import get_object_or_404, renderfrom .models import Question# ...def detail(request, question_id):    question = get_object_or_404(Question, pk=question_id)    return render(request, 'polls/detail.html', {'question': question})</code></pre><p><code>get_object_or_404()</code>函数使用Django的数据对象作为第一个参数，使用任意数量的筛选参数作为余下的参数，这些筛选参数会传递给<code>get()</code>函数。如果对象不存在，会引起404错误。</p><p>也有 <a href="https://docs.djangoproject.com/zh-hans/2.2/topics/http/shortcuts/#django.shortcuts.get_list_or_404" target="_blank" rel="noopener"><code>get_list_or_404()</code></a> 函数，工作原理和 <a href="https://docs.djangoproject.com/zh-hans/2.2/topics/http/shortcuts/#django.shortcuts.get_object_or_404" target="_blank" rel="noopener"><code>get_object_or_404()</code></a> 一样，除了 <a href="https://docs.djangoproject.com/zh-hans/2.2/ref/models/querysets/#django.db.models.query.QuerySet.get" target="_blank" rel="noopener"><code>get()</code></a> 函数被换成了 <a href="https://docs.djangoproject.com/zh-hans/2.2/ref/models/querysets/#django.db.models.query.QuerySet.filter" target="_blank" rel="noopener"><code>filter()</code></a> 函数。如果列表为空的话会抛出 <a href="https://docs.djangoproject.com/zh-hans/2.2/topics/http/views/#django.http.Http404" target="_blank" rel="noopener"><code>Http404</code></a> 异常。</p><h3 id="使用模板系统"><a href="#使用模板系统" class="headerlink" title="使用模板系统"></a>使用模板系统</h3><p>回过头去看看我们的 <code>detail()</code> 视图。它向模板传递了上下文变量 <code>question</code> 。下面是 <code>polls/detail.html</code> 模板里正式的代码：</p><pre class=" language-lang-html"><code class="language-lang-html"><h1>{{ question.question_text }}</h1><ul>{ % for choice in question.choice_set.all % }    <li>{{ choice.choice_text }}</li>{ % endfor % }</ul></code></pre><p>模板系统统一使用点符号来访问变量的属性。在示例 <code></code> 中，首先 Django 尝试对 <code>question</code> 对象使用字典查找（也就是使用 <code>obj.get(str)</code> 操作），如果失败了就尝试属性查找（也就是 <code>obj.str</code> 操作），结果是成功了。如果这一操作也失败的话，将会尝试列表查找（也就是 <code>obj[int]</code> 操作）。</p><p>在 <a href="https://docs.djangoproject.com/zh-hans/2.2/ref/templates/builtins/#std:templatetag-for" target="_blank" rel="noopener"><code>{ % for % }</code></a> 循环中发生的函数调用：<code>question.choice_set.all</code> 被解释为 Python 代码 <code>question.choice_set.all()</code> ，将会返回一个可迭代的 <code>Choice</code> 对象，这一对象可以在 <a href="https://docs.djangoproject.com/zh-hans/2.2/ref/templates/builtins/#std:templatetag-for" target="_blank" rel="noopener"><code>{ %for % }</code></a> 标签内部使用。</p><p>查看 <a href="https://docs.djangoproject.com/zh-hans/2.2/topics/templates/" target="_blank" rel="noopener">模板指南</a> 可以了解关于模板的更多信息。</p><h3 id="去除模板中的硬编码-URL"><a href="#去除模板中的硬编码-URL" class="headerlink" title="去除模板中的硬编码 URL"></a>去除模板中的硬编码 URL</h3><p>还记得吗，我们在 <code>polls/index.html</code> 里编写投票链接时，链接是硬编码的：</p><pre class=" language-lang-html"><code class="language-lang-html"><li><a href="/polls/{{ question.id }}/">{{ question.question_text }}</a></li></code></pre><p>问题在于，硬编码和强耦合的链接，对于一个包含很多应用的项目来说，修改起来是十分困难的。然而，因为你在 <code>polls.urls</code> 的 <a href="https://docs.djangoproject.com/zh-hans/2.2/ref/urls/#django.conf.urls.url" target="_blank" rel="noopener"><code>url()</code></a> 函数中通过 name 参数为 URL 定义了名字，你可以使用 <code>{ % url % }</code> 标签代替它：</p><pre class=" language-lang-html"><code class="language-lang-html"><li><a href="{ % url 'detail' question.id % }">{{ question.question_text }}</a></li></code></pre><p>这个标签的工作方式是在 <code>polls.urls</code> 模块的 URL 定义中寻具有指定名字的条目。你可以回忆一下，具有名字 <code>detail</code> 的 URL 是在如下语句中定义的：</p><pre class=" language-lang-python"><code class="language-lang-python">...# the 'name' value as called by the { % url % } template tagpath('<int:question_id>/', views.detail, name='detail'),...</code></pre><p>也就是说，<code>url &#39;detail&#39; question.id</code>返回的是<code>/polls/2/</code>，因为<code>question.id</code>类似于参数被传入<code>&lt;int:question_id&gt;/</code>。</p><p>如果你想改变投票详情视图的 URL，比如想改成 <code>polls/specifics/12/</code> ，你不用在模板里修改任何东西（包括其它模板），只要在 <code>polls/urls.py</code> 里稍微修改一下就行：</p><pre class=" language-lang-python"><code class="language-lang-python">...# added the word 'specifics'path('specifics/<int:question_id>/', views.detail, name='detail'),...</code></pre><h3 id="为-URL-名称添加命名空间"><a href="#为-URL-名称添加命名空间" class="headerlink" title="为 URL 名称添加命名空间"></a>为 URL 名称添加命名空间</h3><p>教程项目只有一个应用，<code>polls</code> 。在一个真实的 Django 项目中，可能会有五个，十个，二十个，甚至更多应用。Django 如何分辨重名的 URL 呢？举个例子，<code>polls</code> 应用有 <code>detail</code> 视图，可能另一个博客应用也有同名的视图。Django 如何知道 <code>{ % url % }</code> 标签到底对应哪一个应用的 URL 呢？</p><p>答案是：在根 URLconf 中添加命名空间。在 <code>polls/urls.py</code> 文件中稍作修改，加上 <code>app_name</code> 设置命名空间：</p><pre class=" language-lang-python"><code class="language-lang-python">from django.urls import pathfrom . import viewsapp_name = 'polls'urlpatterns = [    path('', views.index, name='index'),    path('<int:question_id>/', views.detail, name='detail'),    path('<int:question_id>/results/', views.results, name='results'),    path('<int:question_id>/vote/', views.vote, name='vote'),]</code></pre><p>现在，编辑 <code>polls/index.html</code> 文件，从：</p><pre class=" language-lang-html"><code class="language-lang-html"><li><a href="{ % url 'detail' question.id % }">{{ question.question_text }}</a></li></code></pre><p>修改为指向具有命名空间的详细视图：</p><pre class=" language-lang-html"><code class="language-lang-html"><li><a href="{ % url 'polls:detail' question.id % }">{{ question.question_text }}</a></li></code></pre><h1 id="TO-BE-CONTINUED…"><a href="#TO-BE-CONTINUED…" class="headerlink" title="TO BE CONTINUED…"></a>TO BE CONTINUED…</h1><p><a href="https://docs.djangoproject.com/zh-hans/2.2/intro/tutorial04/" target="_blank" rel="noopener">https://docs.djangoproject.com/zh-hans/2.2/intro/tutorial04/</a></p>]]></content>
      
      
      <categories>
          
          <category> django </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> web </tag>
            
            <tag> django </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python的网络编程</title>
      <link href="/python/python-web/"/>
      <url>/python/python-web/</url>
      
        <content type="html"><![CDATA[<p>本文教你使用Python进行网络编程。</p><h1 id="API请求"><a href="#API请求" class="headerlink" title="API请求"></a>API请求</h1><h2 id="安装requests包"><a href="#安装requests包" class="headerlink" title="安装requests包"></a>安装requests包</h2><pre class=" language-lang-python"><code class="language-lang-python">pip install --user requests</code></pre><h2 id="处理API响应"><a href="#处理API响应" class="headerlink" title="处理API响应"></a>处理API响应</h2><p>我们以找出<code>GitHub</code>上<code>Stars</code>最高的项目为例。代码如下：</p><pre class=" language-lang-python"><code class="language-lang-python">import requestsimport matplotlib.pyplot as plturl = "https://api.github.com/search/repositories?q=language:python&sort=stars"r = requests.get(url)response_dict = r.json()  #解析成字典print('complete_results:', not response_dict['incomplete_results'])  #获取的结果是否完整print('total_count:', response_dict['total_count'])  #GitHub上的Python项目总数repo_len = len(response_dict['items'])  #我们获取到的项目数量print('get_repo_num:', repo_len)forks = [0 for i in range(repo_len)]stars = [0 for i in range(repo_len)]for i in range(repo_len):    item = response_dict['items'][i]    print('rank:', i + 1)    print('name:', item['name'])    forks[i] = item['forks_count']    stars[i] = item['stargazers_count']    print('fork:', forks[i])    print('star:', stars[i])    print('--------------')plt.plot([i for i in range(repo_len)], forks)plt.plot([i for i in range(repo_len)], stars)plt.legend(loc='upper right', labels=['forks', 'stars'])plt.show()</code></pre><p>输出结果：</p><pre><code>complete_results: Truetotal_count: 3660823get_repo_num: 30rank: 1name: awesome-pythonfork: 12560star: 65258--------------rank: 2...略...star: 20375--------------rank: 30name: python-patternsfork: 4350star: 19983--------------</code></pre><p><img src="0.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> web </tag>
            
            <tag> request </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python和NumPy语法回顾</title>
      <link href="/python/learn-python-and-numpy/"/>
      <url>/python/learn-python-and-numpy/</url>
      
        <content type="html"><![CDATA[<h1 id="Python3语法"><a href="#Python3语法" class="headerlink" title="Python3语法"></a>Python3语法</h1><h2 id="语法特点"><a href="#语法特点" class="headerlink" title="语法特点"></a>语法特点</h2><ol><li>句末不加分号</li><li>用tab对齐的方法行使c++里花括号的功能</li><li>太长需要分行书写时，行末尾需要有连接符，新行需要缩进</li><li>输入<code>help(classname)</code>，即可得到那个类的使用帮助</li><li>输入<code>type(variablename)</code>，即可得到该变量的类型</li><li>列表元组字典都可以相互嵌套</li></ol><h2 id="Print-用法"><a href="#Print-用法" class="headerlink" title="Print()用法"></a>Print()用法</h2><ol><li>自动末尾追加<code>\n</code>（可以设置关键字end=””使得末尾不自动追加换行符）</li><li><code>Print(A,B)</code>中间有个空格：A B</li><li>可以直接打印列表[]</li></ol><h2 id="input-用法"><a href="#input-用法" class="headerlink" title="input()用法"></a>input()用法</h2><ol><li>用户输入函数，返回值就是输入的字符串（返回是字符串！），如：<code>str=input(&quot;Please input something:&quot;)</code></li></ol><h2 id="变量的用法"><a href="#变量的用法" class="headerlink" title="变量的用法"></a>变量的用法</h2><ol><li>不用声明</li></ol><h2 id="字符串的用法"><a href="#字符串的用法" class="headerlink" title="字符串的用法"></a>字符串的用法</h2><ol><li><code>s=&quot;abc123&quot;</code>   # 单引号双引号都可以</li><li><code>s[2]</code>取出第3个字符</li><li><code>s.title()</code>    # 暂时每个单词首字母大写</li><li><code>s.upper()</code>    # 暂时大写</li><li><code>s.lower()</code>    # 暂时小写</li><li><code>s1+s2</code>        # 连接字符串</li><li><code>\n，\t</code>等     # 转义符</li><li><code>s.rstrip()</code>   # 暂时去除右边空白</li><li><code>s.lstrip()</code>   # 暂时去除左边空白</li><li><code>s.strip()</code>    # 暂时去除两边空白</li><li><code>str(number)</code>  # 暂时转换为字符串，以便连接为字符串</li></ol><h2 id="数字的用法"><a href="#数字的用法" class="headerlink" title="数字的用法"></a>数字的用法</h2><ol><li>可以计算复数</li><li><code>3/2</code>为1.5</li><li><code>3**2</code>为三的二次方</li><li><code>7%3</code>求余数（7%3==1）</li><li><code>int(&#39;123&#39;)</code>把字符串转整数，不能包含小数点</li><li><code>float(&#39;12.3&#39;)</code>转换为浮点数</li><li><code>+=，-=，*=，/=，%=</code> 同c++的含义(但没有类似自增++的缩写用法)，注意：式子左边的变量要事先定义</li></ol><h2 id="注释的用法"><a href="#注释的用法" class="headerlink" title="注释的用法"></a>注释的用法</h2><ol><li>井号#等效于c++里的//</li><li>三个单引号’’’等效于c++里的/*或*/</li></ol><h2 id="列表-的用法【class的一种】"><a href="#列表-的用法【class的一种】" class="headerlink" title="列表[]的用法【class的一种】"></a>列表[]的用法【class的一种】</h2><ol><li>元素类型可以不同</li><li><code>a=[77,&#39;AB&#39;]</code></li><li><code>a[0]</code>访问第一个元素</li><li><code>a[-2]</code>访问倒数第二个元素</li><li><code>len(a)</code>返回元素个数</li><li><code>a.append(elem)</code>末尾添加元素</li><li><code>a.insert(pos,elem)</code>在位置pos插入元素（列表头是pos==0，列表尾是pos==len(a)）</li><li><code>del a[1]</code>删除列表a里第二个元素</li><li><code>elem = a.pop()</code>弹出（删除）列表尾的元素并赋值给elem</li><li><code>elem = a.pop(i)</code>取出（删除）列表里索引为i（可为负数，表示倒数）的元素并赋值给elem</li><li><code>a.remove(value)</code>删除列表a里第一个值为value的元素</li><li><code>a.sort()</code>永久性的升排序（数字增序或字典顺序）（参数填reverse=True则是降序）</li><li><code>sorted(a)</code>暂时性的升排序</li><li><code>a.reverse()</code>永久性的逆转序列</li><li><code>min(a)</code>返回列表最小值</li><li><code>max(a)</code>返回列表最大值</li><li><code>sum(a)</code>返回元素之和（元素必须是数字）</li><li><code>a[i1:i2]</code>返回子列表（又称切片），范围是索引i1&lt;=i&lt;i2，即不包括i2，好处是i2-i1就是子列表的元素个数</li><li><code>a[:i2]</code>等价于a[0:i2]</li><li><code>a[i1:]</code>等价于a[i1:len(a)]</li><li><code>a[-3:]</code>等价于a[len(a)-3:len(a)]，即返回末尾三个元素组成的列表</li><li><code>b=a[:]</code>列表深复制（因为切片并不与a共用内存空间）</li><li><code>b=a</code>列表浅复制（b是a的引用）</li></ol><h2 id="元组-的用法【class的一种】"><a href="#元组-的用法【class的一种】" class="headerlink" title="元组()的用法【class的一种】"></a>元组()的用法【class的一种】</h2><ol><li>元组里一个元素的值不可修改，但可以给整个元组赋值如：<code>a=(7,8,9,10)</code></li><li><code>a=(1,2,3)</code> # 定义元组a</li><li><code>a[0]</code>调用</li><li>其他用法和列表[]类似</li></ol><h2 id="字典-的用法【class的一种】"><a href="#字典-的用法【class的一种】" class="headerlink" title="字典{}的用法【class的一种】"></a>字典{}的用法【class的一种】</h2><ol><li><p>即键值表，并不关心多对键值对的顺序，可修改</p><pre class=" language-lang-python"><code class="language-lang-python">rect = {'x':10, 'y':5, 0:20}</code></pre><p>上述字典rect里有三个键（key），分别是’x’，’y’和0。作为下标带入rect[key]就可以得到相应的值</p></li><li><p>新建键值对：直接赋值即可（如<code>rect[&#39;newkey&#39;]=&#39;newval&#39;</code>）</p></li><li>删除键值对：如<code>del rect[&#39;newkey&#39;]</code></li></ol><h2 id="集合-的简单介绍【class的一种】"><a href="#集合-的简单介绍【class的一种】" class="headerlink" title="集合{}的简单介绍【class的一种】"></a>集合{}的简单介绍【class的一种】</h2><ol><li>通过函数<code>set(a)</code>可以将数组a的元素去除重复，返回一个集合类型的量</li></ol><h2 id="逻辑的用法"><a href="#逻辑的用法" class="headerlink" title="逻辑的用法"></a>逻辑的用法</h2><ol><li>True真，False假</li><li>空列表==False，非空列表==True（如while mylist然后逐个pop实现逐个取出）</li><li>空字符串==False，非空字符串==True</li><li>==判断等号</li><li>!=不等号</li><li>and与，or或，not非</li><li>in被包含（如elem in a），not in不被包含</li></ol><h2 id="for循环的用法"><a href="#for循环的用法" class="headerlink" title="for循环的用法"></a>for循环的用法</h2><ol><li>基本格式：<pre class=" language-lang-python"><code class="language-lang-python">for elem in arr: print(elem)</code></pre>例子：</li></ol><ul><li><code>for elem in a</code> # a是列表</li><li><code>for i in range(1,11)</code> # i=1~10</li><li><code>for key,val in a.items()</code> # a是字典</li><li><code>for key in a.keys()</code> # a是字典</li><li><code>for val in a.values()</code> # a是字典</li><li><code>for key in a</code> # a是字典(仅遍历键key)</li></ul><ol><li>注意事项：</li></ol><ul><li>for语句行末尾有冒号，下一行tab缩进（不推荐space缩进）</li><li>循环退出后elem的值可访问，且值是a的最后一个元素a[-1]</li><li><code>range(start,end,step)</code>返回迭代对象(区间[A,B))，用作for循环的循环域，不是列表,（但通过<code>list(range(...))</code>可以变为列表）</li><li><code>a=[val**2 for val in range(1,11)]</code>列表解析，用于快速生成列表[1，4，…,100]</li><li><code>dict.items()</code>返回一个元素是元组(key_i,val_i)的dict_items对象（而<code>list(dict.items())</code>才是返回真正的列表）</li><li><code>dict.keys()</code>返回一个元素是键的dict_keys对象（而<code>list(dict.keys())</code>才是真正的列表）</li><li><code>dict.values()</code>返回一个元素是值的dict_values对象（而<code>list(dict.values())</code>才是真正的列表）</li></ul><h2 id="if语句的用法"><a href="#if语句的用法" class="headerlink" title="if语句的用法"></a>if语句的用法</h2><ol><li>例子：<pre class=" language-lang-python"><code class="language-lang-python"> if a==1:     #... elif a==2:     #... else:     #...</code></pre></li></ol><h2 id="while循环的用法"><a href="#while循环的用法" class="headerlink" title="while循环的用法"></a>while循环的用法</h2><ol><li>例子<pre class=" language-lang-python"><code class="language-lang-python"> while boolvar:     #TODO</code></pre></li><li>用break退出while或for循环</li><li>continue跳过此次循环，进入下一轮</li></ol><h2 id="函数的用法"><a href="#函数的用法" class="headerlink" title="函数的用法"></a>函数的用法</h2><ol><li>例子<pre class=" language-lang-python"><code class="language-lang-python"> def 函数名(参数表): # 参数可以是列表（与实参共用内存空间，除非传递副本如a[:]）     # 计算     return ReturnVal # 非必需，ReturnVal可以是字典</code></pre></li><li>有形参与实参之分，实参传值给形参进入函数内部</li><li>参数传递的两种方法：<ul><li>按参数表的顺序依次传递如f(2,5,-5)</li><li>给形参赋值，如f(x=3,y=7) # 赋值顺序不重要</li></ul></li><li>参数可以有默认值，如def f(x,y=0)，默认值要列在最后</li><li>传递任意数量的实参的写法：(“任意数量的参数*inputs”必须放在参数表的末尾)<pre class=" language-lang-python"><code class="language-lang-python"> def f(*inputs): # 将多个输入的参数封装到一个名为inputs的元组里，调用例子：f(4,8,7,1,3)     print(inputs)</code></pre></li><li>传递任意数量的关键字实参的写法：（同理，放在参数表的末尾）<pre class=" language-lang-python"><code class="language-lang-python"> def f(x,**dict):     #获得x和字典dict（调用例子：f('3',name='karbo',age=99)）</code></pre></li></ol><h2 id="类的用法"><a href="#类的用法" class="headerlink" title="类的用法"></a>类的用法</h2><ol><li>例子：<pre class=" language-lang-python"><code class="language-lang-python"> class Rect():  # 约定：首字母大写的是类（推荐驼峰命名法）</code></pre></li><li>类里的函数（方法）必定包含参数self<pre class=" language-lang-python"><code class="language-lang-python">def __init__(self, x=1, y=1):  # 构造函数（不包含return语句） self.x = x #有默认值1 self.y = y #有默认值1 self.is_active = True  # 置默认值def compute_area(self): if self.is_active:     return self.x * self.y return 0</code></pre></li><li>使用方法<pre class=" language-lang-python"><code class="language-lang-python">MYRECT = Rect(3)  #用赋值的方法创建对象（不必传递self参数）print(MYRECT.compute_area())  # 调用函数并打印</code></pre></li><li>继承的例子：<pre class=" language-lang-python"><code class="language-lang-python">class Cube(Rect): #Cube继承自Rect def __init__(self, x=1, y=1, z=1):     super().__init__(x, y) #初始化继承到的内容（通过super()函数的返回值可以访问继承到的东西）     self.z = z</code></pre></li><li>注意事项：<ul><li>子类同名方法会覆盖父类的</li><li>类的成员可以是类</li></ul></li></ol><h2 id="模块的用法"><a href="#模块的用法" class="headerlink" title="模块的用法"></a>模块的用法</h2><ol><li>在模块文件（.py）写入函数，然后在另一个.py文件import模块文件名即可。<br> 如：<pre class=" language-lang-python"><code class="language-lang-python"> #在模块PRINTABC.py def printabc():     print('abc') #调用者main.py import PRINTABC #import模块 PRINTABC.printabc() #要加上作用域PRINTABC.</code></pre></li><li>import其他用法：<ul><li>使用模块别名:import tensorflow as tf # 调用时需要加上tf.</li><li>显式导入特定函数或类:from tensorflow import constant, Session # 调用时不需加tensorflow.</li><li>显式导入特定函数或类并使用别名:from tensorflow import constant as c, Session as s # 调用时不需加tensorflow.</li><li>显式导入所有:from tensorflow import * # 调用时不需加tensorflow.且容易重复命名造成覆盖</li></ul></li><li>模块中也可以import哦</li></ol><h2 id="文件操作的用法-文本操作"><a href="#文件操作的用法-文本操作" class="headerlink" title="文件操作的用法(文本操作)"></a>文件操作的用法(文本操作)</h2><ol><li>【读】</li></ol><ul><li>全部一次性读取：<pre class=" language-lang-python"><code class="language-lang-python">with open('a.txt') as myfile: #使用with使得文件在不再被调用后自动关闭（不用with的写法：打开myfile=open("a.txt")关闭myfile.close()）  print(myfile.read()) #read()函数将内容全部读取</code></pre>文件路径是相对路径或绝对路径</li><li>逐行读取：<pre class=" language-lang-python"><code class="language-lang-python">for line in myfile: #读取一行到line里，注意：它不抛弃末尾的\n，即line字符串末尾有换行符</code></pre></li><li>读取所有行到一个列表中：<pre class=" language-lang-python"><code class="language-lang-python">arr_lines = myfile.readlines() #一行为一个元素的列表，同理不抛弃末尾的\n</code></pre></li></ul><ol><li>【写】<pre class=" language-lang-python"><code class="language-lang-python">with open('a.txt', 'w') as myfile: # 可选项：读r（默认），写w，追加a，读写r+ myfile.write('hello!') # 只将内容写入，并不会自动添加\n</code></pre></li></ol><ul><li>注意事项：<ul><li>以’w’方式会重写文件（不存在则创建文件）</li><li>以’a’方式会追加到文件尾（不存在则创建文件）</li></ul></li></ul><h2 id="存储数据结构（JSON）："><a href="#存储数据结构（JSON）：" class="headerlink" title="存储数据结构（JSON）："></a>存储数据结构（JSON）：</h2><ol><li>完整例子：<pre class=" language-lang-python"><code class="language-lang-python">import jsondef READ_JSON_FILE(FILENAME): with open(FILENAME) as myfile:     return json.load(myfile) #返回读取到的文件内容def WRITE_JSON_FILE(WHAT, FILENAME): with open(FILENAME, 'w') as myfile:     json.dump(WHAT, myfile) #将WHAT写入路径为FILENAME的文件里</code></pre><h2 id="异常机制"><a href="#异常机制" class="headerlink" title="异常机制"></a>异常机制</h2></li><li>例子：<pre class=" language-lang-python"><code class="language-lang-python"> try:     with open('a.txt') as myfile:         contents = myfile.read() except FileNotFoundError: #try失败后     print('Sorry, the file not found.') #换成语句pass可以跳过（pass相当于占位符，不起作用，是空语句） else: #try成功后执行else部分     print('It has', str(len(contents.split())), 'words.')</code></pre></li><li>常见的异常有：</li></ol><div class="table-container"><table><thead><tr><th style="text-align:center">错误类型</th><th style="text-align:center">解释</th></tr></thead><tbody><tr><td style="text-align:center">OverflowError</td><td style="text-align:center">数值运算超出最大限制</td></tr><tr><td style="text-align:center">ZeroDivisionError</td><td style="text-align:center">除(或取模)零 (所有数据类型)</td></tr><tr><td style="text-align:center">IOError</td><td style="text-align:center">输入/输出操作失败</td></tr><tr><td style="text-align:center">IndexError</td><td style="text-align:center">序列中没有此索引(index)</td></tr><tr><td style="text-align:center">FileNotFoundError</td><td style="text-align:center">文件未找到</td></tr><tr><td style="text-align:center">NameError</td><td style="text-align:center">访问一个不存在的变量</td></tr></tbody></table></div><h1 id="NumPy语法"><a href="#NumPy语法" class="headerlink" title="NumPy语法"></a>NumPy语法</h1><h2 id="ndarray基本属性"><a href="#ndarray基本属性" class="headerlink" title="ndarray基本属性"></a>ndarray基本属性</h2><ol><li><code>ndarray.ndim</code>维度</li><li><code>ndarray.shape</code>形状尺寸</li><li><code>ndarray.size</code>元素总数（等于shape的乘积）</li><li><code>ndarray.dtype</code>元素的类型（如：numpy.int32，numpy.int16，numpy.float64）</li><li><code>ndarray.itemsize</code>每个元素所占的字节数（如float64为8，相当于ndarray.dtype.itemsize）</li><li><code>ndarray.data</code>数据内存区域</li></ol><h2 id="ndarray生成"><a href="#ndarray生成" class="headerlink" title="ndarray生成"></a>ndarray生成</h2><p>array()是函数，ndarray()是类</p><ol><li>list转ndarray<pre class=" language-lang-python"><code class="language-lang-python">a = np.array([1,2,3,4])b = np.array([[1,2],[3,4]], dtype=complex) #显式指定为复数类型c = np.array([i for i in range(1, 11)]) #0~10</code></pre></li><li>arange生成<pre class=" language-lang-python"><code class="language-lang-python">a = np.arange(15) #0~14b = np.arange(5, 10) #5~9c = np.arange(1, 10, 0.5) #1~9.5，间隔0.5</code></pre></li><li>zeros、ones、eye、empty生成（默认是np.float64类型）<pre class=" language-lang-python"><code class="language-lang-python">a = np.zeros([2, 3], dtype=np.int64) #指定为np.int64类型，元素全是0b = np.ones([2, 3]) #默认为np.float64类型，元素全是1c = np.eye(3) #默认为np.float64类型，大小为3*3的单位矩阵d = np.empty([2, 3], dtype=complex) #指定为复数，元素未初始化（依据内存状态）</code></pre></li><li>linspace生成<br>返回等间隔分布的数组<pre class=" language-lang-python"><code class="language-lang-python">a = np.linspace(1, 10, 5) #a == [ 1.    3.25  5.5   7.75 10.  ]</code></pre></li></ol><h2 id="ndarray的操作"><a href="#ndarray的操作" class="headerlink" title="ndarray的操作"></a>ndarray的操作</h2><ol><li>reshape<br>注意<code>reshape</code>函数返回的是引用，指向同一内存空间。<br>若要不同请使用深拷贝，请使用深拷贝<code>.copy()</code>。<pre class=" language-lang-python"><code class="language-lang-python">x = np.array([i for i in range(1, 13)]) #生成长度为12的ndarraya = np.reshape(x, [3, 4]) #执行后x的值仍不变b = x.reshape([3, 4]) #执行后x的值仍不变c = x.reshape(3, 4) #执行后x的值仍不变</code></pre>输出效果（a、b和c都一样）：<pre class=" language-lang-python"><code class="language-lang-python">[[ 1  2  3  4][ 5  6  7  8][ 9 10 11 12]]</code></pre></li><li>读取单个元素<pre class=" language-lang-python"><code class="language-lang-python">a[1,2] #取出第2行第3列的元素（建议）a[1][2] #取出第2行第3列的元素（先取出第二行[ 5  6  7  8]，再取出第三个元素7）</code></pre></li><li>读取整行/列<pre class=" language-lang-python"><code class="language-lang-python">a[:,1] #取出第2列（建议）a[:][1] #取出第2行（注意：a[:]等价于a）a[1] # 取出第2行（建议）a[1,:] #取出第2行（建议）a[1][:] #取出第2行（注意：[:]不起作用）</code></pre></li><li>拷贝<br>ndarray深拷贝请使用<code>y=x.copy()</code>，而不是<code>y=x[:]</code>！<br>对数值型的赋值都是深拷贝</li></ol><p>拷贝实验一：</p><pre class=" language-lang-python"><code class="language-lang-python">import numpy as npx = np.array([i for i in range(1, 13)])a = x.reshape([3, 4])b = a #b是a的引用b[0, 0] = 99 #实质上是修改了a输出结果：[[99  2  3  4] [ 5  6  7  8] [ 9 10 11 12]] #a被修改了print(a)</code></pre><p>拷贝实验二：</p><pre class=" language-lang-python"><code class="language-lang-python">import numpy as npx = np.array([i for i in range(1, 13)])a = x.reshape([3, 4])b = a[0] #取出a第1行的引用b[0] = 99 #实质上是修改了a的第1行的第1个元素print(a)输出结果：[[99  2  3  4] [ 5  6  7  8] [ 9 10 11 12]] #a被修改了</code></pre><p>拷贝实验三：</p><pre class=" language-lang-python"><code class="language-lang-python">import numpy as npx = np.array([i for i in range(1, 13)])a = x.reshape([3, 4])b = a[0, 0] #数值型赋值，b不是a[0,0]的引用b = 99 #仅修改了bprint(a)输出结果：[[ 1  2  3  4] [ 5  6  7  8] [ 9 10 11 12]] #a保持原样</code></pre><p>拷贝实验四：</p><pre class=" language-lang-python"><code class="language-lang-python">import numpy as npx = np.array([i for i in range(1, 13)])a = x.reshape([3, 4])b = a[:] #这种操作对ndarray无效，仅对list有效b[0, 0] = 99print(a)输出结果：[[99  2  3  4] [ 5  6  7  8] [ 9 10 11 12]] #a被修改了</code></pre><p>拷贝实验五：</p><pre class=" language-lang-python"><code class="language-lang-python">import numpy as npx = np.array([i for i in range(1, 13)])a = x.reshape([3, 4])b = a.copy() #使用了ndarray的深拷贝b[0, 0] = 99print(a)输出结果：[[1  2  3  4] [ 5  6  7  8] [ 9 10 11 12]] #a保持原样</code></pre><ol><li>连续区域赋值<br>```python<br>import numpy as np<br>x = np.array([i for i in range(1, 13)])<br>y = x.reshape([3, 4]).copy()<br>x[3:6] = 99<br>y[1:3, 0:3] = 99<br>print(x)<br>print(y)</li></ol><p>输出结果：<br>[ 1  2  3 99 99 99  7  8  9 10 11 12]<br>[[ 1  2  3  4]<br> [99 99 99  8]<br> [99 99 99 12]]</p><pre><code>6. 逻辑运算```pythonimport numpy as npx = np.array([i for i in range(1, 13)])y = x.reshape([3, 4]).copy()a = y &gt;= 8b = y.copy()b[a] = 0print(a)print(b)输出结果：[[False False False False] [False False False  True] [ True  True  True  True]][[1 2 3 4] [5 6 7 0] [0 0 0 0]]</code></pre><ol><li>ndarray保存到文件</li></ol><ul><li>单个ndarray的二进制保存（.npy后缀）<br>```python<br>import numpy as np<br>x = np.array([i for i in range(1, 13)]).reshape([3, 4])<br>np.save(‘x.npy’, x)<br>y = np.load(‘x.npy’)<br>print(y)</li></ul><p>输出结果：<br>[[ 1  2  3  4]<br> [ 5  6  7  8]<br> [ 9 10 11 12]]</p><pre><code>- 单个ndarray的文本保存（.txt后缀）```pythonimport numpy as npx = np.array([i for i in range(1, 13)]).reshape([3, 4])np.savetxt(&#39;x.txt&#39;, x)y = np.loadtxt(&#39;x.txt&#39;)print(y)输出结果：[[ 1.  2.  3.  4.] [ 5.  6.  7.  8.] [ 9. 10. 11. 12.]]</code></pre><ul><li>多个ndarray的二进制保存（.npz后缀）<br>```python<br>import numpy as np<br>a = np.array([[1, 2, 3], [4, 5, 6]])<br>b = np.arange(0, 1.0, 0.1)<br>c = np.sin(b)<br>np.savez(“result.npz”, a, b, sin_array=c)<br>r = np.load(“result.npz”)<br>print(r[“arr_0”])<br>print(r[“arr_1”])<br>print(r[“sin_array”])</li></ul><p>输出结果：<br>[[1 2 3]<br> [4 5 6]]<br>[0.  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9]<br>[0.         0.09983342 0.19866933 0.29552021 0.38941834 0.47942554<br> 0.56464247 0.64421769 0.71735609 0.78332691]<br>```</p><h1 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接"></a>相关链接</h1><ol><li><a href="https://docs.python.org/zh-cn/3/" target="_blank" rel="noopener">Python 3.7.3 文档</a></li><li><a href="https://docs.python.org/zh-cn/3/tutorial/" target="_blank" rel="noopener">Python 教程</a></li><li><a href="https://www.numpy.org/devdocs/user/quickstart.html" target="_blank" rel="noopener">NumPy Tutorial</a></li><li><a href="https://www.numpy.org/devdocs/user/numpy-for-matlab-users.html" target="_blank" rel="noopener">NumPy for Matlab users</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> grammar </tag>
            
            <tag> python </tag>
            
            <tag> numpy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一种颜色识别算法</title>
      <link href="/algorithm/colorclassify/"/>
      <url>/algorithm/colorclassify/</url>
      
        <content type="html"><![CDATA[<h1 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h1><ol><li>将RGB色彩空间转化为HSV色彩空间</li><li>对HSV立方体色彩空间进行变换</li><li>度量色彩距离</li><li>对于在阈值范围内的色彩赋予标签</li><li>若该色彩在所有备选颜色的阈值范围外，则拒绝识别</li></ol><p><img src="2.png" alt></p><h1 id="色彩空间RGB转HSV"><a href="#色彩空间RGB转HSV" class="headerlink" title="色彩空间RGB转HSV"></a>色彩空间RGB转HSV</h1><p>通过调用<code>rgb2hsv</code>函数来实现转换。</p><p>输入参数r、g和b范围均在0到1，输出参数h范围0到360，输出参数s和v范围在0到1</p><pre class=" language-lang-c++"><code class="language-lang-c++">/* RGB转HSV *//* 0~1, 0~1, 0~1 -> 0~360, 0~1, 0~1 */void rgb2hsv(float r, float g, float b, float& h, float& s, float& v){    float max_val = MAX3(r, g, b), min_val = MIN3(r, g, b);    float diff = max_val - min_val;    // 计算H    if (max_val == min_val)        h = 0;    else if (max_val == r)    {        if (g >= b)            h = 60 * ((g - b) / diff) + 0;        else            h = 60 * ((g - b) / diff) + 360;    }    else if (max_val == g)    {        h = 60 * ((b - r) / diff) + 120;    }    else if (max_val == b)    {        h = 60 * ((r - g) / diff) + 240;    }    // 计算S    if (max_val == 0)        s = 0;    else        s = (diff / max_val);    // 计算V    v = max_val;}</code></pre><h1 id="HSV色彩空间变换"><a href="#HSV色彩空间变换" class="headerlink" title="HSV色彩空间变换"></a>HSV色彩空间变换</h1><h2 id="HSV立方体色彩空间"><a href="#HSV立方体色彩空间" class="headerlink" title="HSV立方体色彩空间"></a>HSV立方体色彩空间</h2><p>传统的HSV立方体色彩空间并不能很好地度量颜色的相似程度。</p><blockquote><p>举个例子，考虑四种颜色。（假定h、s和v范围均在0~1）</p><p>颜色A为<code>h=0, s=0.1, v=0.5</code>，颜色B为<code>h=0.5, s=0.1, v=0.5</code></p><p>采用<code>L2</code>距离度量，颜色A到B的距离为0.5</p><p>颜色C为<code>h=0, s=0.9, v=0.5</code>，颜色D为<code>h=0.5, s=0.9, v=0.5</code></p><p>采用<code>L2</code>距离度量，颜色C到D的距离仍为0.5</p></blockquote><p><img src="1.png" alt></p><p>这两个距离居然相同！这显然是不符合直觉的。我们直觉上应当认为A到B的距离应该小于C到D的距离，因为C和D两者色彩差异更大，而A和B都近似是某种灰色。</p><p>在实践应用上，我们感兴趣的往往不是灰度，而是某一种鲜艳的颜色。而且灰度颜色受光照的影响比较大，难以用于实践。</p><p>造成这种度量不均衡的原因是单纯在HSV立方体色彩空间中颜色的分布是不均衡的。如下图，我们发现灰度占据了绝大多数的空间，造成灰度对色彩距离度量的影响比较大。</p><p><img src="2.png" alt></p><p>所以我们要做的就是减少灰度对颜色距离度量的影响，突出不同色相的差异性。</p><p>接下来我们将其变换到HSV圆盘色彩空间，以突出不同色相区别。</p><h2 id="HSV圆盘色彩空间"><a href="#HSV圆盘色彩空间" class="headerlink" title="HSV圆盘色彩空间"></a>HSV圆盘色彩空间</h2><p>我们将HSV立方体色彩空间变换到锥体中。</p><p><img src="3.png" alt></p><p>考虑到色彩距离应该具备光照不变性，我们丢弃掉V轴，仅保留H和S分量构成的圆盘结构。</p><p><img src="4.png" alt></p><p>我们在圆盘中度量点A、B的距离便得到颜色相似度的度量值。</p><p><img src="5.png" alt></p><h1 id="度量颜色距离"><a href="#度量颜色距离" class="headerlink" title="度量颜色距离"></a>度量颜色距离</h1><p>在HSV圆盘色彩空间中，我们可以采用直线距离度量方式，也可以采用弧线形的距离度量方式。</p><p>为简便起见，我们采用直线型的距离度量。</p><p>为了更好的调节色彩识别的鲁棒性因素，我们引入调节参数<code>COLOR_ROBUST</code>，用于调节色彩饱和度的鲁棒性。该参数值越大，则将会具有更大的允许饱和度变化的范围；该参数越小，则对饱和度变化允许的范围越小。</p><p>其中参数<code>COLOR_ROBUST</code>的实现是通过调节半径<code>s</code>的缩放来实现的。我们在半径<code>s</code>方向上对其进行套用函数：</p><script type="math/tex; mode=display">{1-(1-x)^n}</script><p>其中n就是<code>COLOR_ROBUST</code>参数。</p><p><img src="6.png" alt></p><p>计算颜色距离的代码如下：</p><pre class=" language-lang-c++"><code class="language-lang-c++">/* 计算颜色距离(输入范围均在0~1) */float getColorDistance(float h, float s, float h_dst, float s_dst){    float x_src, y_src;    float x_dst, y_dst;    x_src = (1 - powf(1 - s, COLOR_ROBUST)) * cos(h * 2 * CV_PI);    y_src = (1 - powf(1 - s, COLOR_ROBUST)) * sin(h * 2 * CV_PI);    x_dst = (1 - powf(1 - s_dst, COLOR_ROBUST)) * cos(h_dst * 2 * CV_PI);    y_dst = (1 - powf(1 - s_dst, COLOR_ROBUST)) * sin(h_dst * 2 * CV_PI);    return sqrt(pow(x_src - x_dst, 2) + pow(y_src - y_dst, 2));}</code></pre><h1 id="颜色距离阈值"><a href="#颜色距离阈值" class="headerlink" title="颜色距离阈值"></a>颜色距离阈值</h1><h2 id="阈值计算原理"><a href="#阈值计算原理" class="headerlink" title="阈值计算原理"></a>阈值计算原理</h2><p>由于颜色是任意指定的，如果选取固定的距离阈值将不能广泛适应实际情况。我们在这里应该采取一种自动的方式来计算这个阈值。</p><p>标准颜色类似于聚类中心，我们把它抽象为颜色空间中的一个点，两两之间的距离抽象为线段。那么这个阈值可以用下图的方式计算。</p><p><img src="7.png" alt></p><p>这个阈值就是上图中各个圆的半径。原则是圆的半径从小到大依次选取。</p><ol><li><p>一开始选取最小的半径必定是由最短的边决定的，半径为最短边的一半。如上图的圆A。</p></li><li><p>第二个圆是与第一个相邻的圆，半径等于第一个圆的半径，如上图的圆B。</p></li><li>然后就从剩余的可选择半径中选取半径较小的，即圆C。</li><li>最后选取圆D。</li></ol><h2 id="实现代码"><a href="#实现代码" class="headerlink" title="实现代码"></a>实现代码</h2><p>如下的<code>initColorInfo</code>函数实现了颜色信息的初始化功能。<br>调用该函数后，二维数组<code>color_table</code>存储了各个标准颜色的HSV值，数组<code>color_range_table</code>存储了颜色阈值信息。</p><blockquote><p><code>COLOR_NUM</code>是颜色数量，在此例子中<code>#define COLOR_NUM 6</code><br><code>color_table</code>的定义：<code>float color_table[COLOR_NUM][3]</code><br><code>color_range_table</code>的定义：<code>float color_range_table[COLOR_NUM]</code></p></blockquote><pre class=" language-lang-c++"><code class="language-lang-c++">/* 初始化颜色信息 */void initColorInfo(){    float h_dst, s_dst, v_dst;    for (int i = 0; i < COLOR_NUM; ++i)    {        // RGB空间        switch (i)        {        case 0: // Unknown(White)            color_table[i][0] = 255;            color_table[i][1] = 255;            color_table[i][2] = 255;            break;        case 1: // Red            color_table[i][0] = 148;            color_table[i][1] = 19;            color_table[i][2] = 24;            break;        case 2: // Orange            color_table[i][0] = 198;            color_table[i][1] = 115;            color_table[i][2] = 35;            break;        case 3: // Yellow            color_table[i][0] = 177;            color_table[i][1] = 152;            color_table[i][2] = 23;            break;        case 4: // Green            color_table[i][0] = 45;            color_table[i][1] = 93;            color_table[i][2] = 19;            break;        case 5: // Blue            color_table[i][0] = 39;            color_table[i][1] = 92;            color_table[i][2] = 132;            break;        }        // 转化为HSV空间        rgb2hsv(color_table[i][0] / 255.0, color_table[i][1] / 255.0, color_table[i][2] / 255.0, h_dst, s_dst, v_dst);        color_table[i][0] = h_dst / 360.0f;        color_table[i][1] = s_dst;        color_table[i][2] = v_dst;    }    // 计算可识别的颜色距离    float dist;    float min_dist;    float adjMat[COLOR_NUM][COLOR_NUM]; // 邻接矩阵(非对称)    bool activeNode[COLOR_NUM];    // 初始化邻接矩阵    for (int i = 0; i < COLOR_NUM; ++i)    {        for (int j = i + 1; j < COLOR_NUM; ++j)        {            dist = getColorDistance(color_table[i][0], color_table[i][1], color_table[j][0], color_table[j][1]);            adjMat[i][j] = dist;        }    }    // 初始化activeNode    for (int i = 0; i < COLOR_NUM; ++i) activeNode[i] = true;    // 初始化color_range_table    for (int i = 0; i < COLOR_NUM; ++i) color_range_table[i] = 0;    // 计算距离    while (1)    {        // 求出最短距离        min_dist = 1e9;        for (int i = 0; i < COLOR_NUM; ++i)        {            for (int j = i + 1; j < COLOR_NUM; ++j)            {                dist = adjMat[i][j];                if ((activeNode[i] == false) ^ (activeNode[j] == false)) dist *= 2;                if (dist>1e-6 && dist < min_dist)                {                    min_dist = dist;                }            }        }        // 一起减去最短距离        for (int i = 0; i < COLOR_NUM; ++i)        {            for (int j = i + 1; j < COLOR_NUM; ++j)            {                if (adjMat[i][j] > 1e-6)                {                    if ((activeNode[i] == false) ^ (activeNode[j] == false)) adjMat[i][j] -= min_dist / 2;                    else adjMat[i][j] -= min_dist;                }            }        }        // 更新color_range_table        for (int i = 0; i < COLOR_NUM; ++i)        {            if (activeNode[i] == true)            {                color_range_table[i] += min_dist / 2;            }        }        // 更新有效结点        for (int i = 0; i < COLOR_NUM; ++i)        {            for (int j = i + 1; j < COLOR_NUM; ++j)            {                if (adjMat[i][j] < 1e-6)                {                    activeNode[i] = false;                    activeNode[j] = false;                }            }        }        // 退出条件        int activeNodeNum = 0;        for (int i = 0; i < COLOR_NUM; ++i)        {            activeNodeNum += int(activeNode[i]);        }        if (activeNodeNum == 0) break;    }}</code></pre><h1 id="颜色识别"><a href="#颜色识别" class="headerlink" title="颜色识别"></a>颜色识别</h1><p>当距离小于阈值时则判定为该颜色。如果都不符合，则拒绝识别。</p><blockquote><p>代码中使用参数<code>COLOR_RANGE_A</code>来控制颜色的容限，值越接近1代表颜色的可变化性越大，值越接近0代表容不得颜色变化过大。<br><code>ColorType</code>是颜色的枚举类型数组</p></blockquote><pre class=" language-lang-c++"><code class="language-lang-c++">/* 从HSV识别颜色 *//* H = 0~360, S = 0~1, V = 0~1 */ColorType hsvColorReg(float h, float s, float v){    float dist;    h /= 360.0f;    for (int i = 0; i < COLOR_NUM; ++i)    {        dist = getColorDistance(h, s, color_table[i][0], color_table[i][1]);        if (dist <= COLOR_RANGE_A * color_range_table[i])        {            return ColorType(i);        }    }    return Unknown;}</code></pre>]]></content>
      
      
      <categories>
          
          <category> algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> algorithm </tag>
            
            <tag> classifier </tag>
            
            <tag> ml </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>在Kaggle免费使用GPU训练自己的神经网络</title>
      <link href="/dl/kaggle-gpu/"/>
      <url>/dl/kaggle-gpu/</url>
      
        <content type="html"><![CDATA[<h1 id="Kaggle是什么"><a href="#Kaggle是什么" class="headerlink" title="Kaggle是什么"></a>Kaggle是什么</h1><p><code>Kaggle</code>是一个数据建模和数据分析竞赛平台。企业和研究者可在其上发布数据，统计学者和数据挖掘专家可在其上进行竞赛以产生最好的模型。</p><p><img src="0.png" alt></p><p>在<code>Kaggle</code>，你可以：</p><ol><li><p>参加竞赛赢取奖金。<code>Kaggle</code>上会发布一些赛题，做的好会赢得奖金。</p></li><li><p>下载数据集。<code>Kaggle</code>上包含了众多的数据集供大家免费下载，常见的数据集都可以在上面找到。</p></li><li><p>学习别人的代码。类似<code>GitHub</code>，你可以在<code>Kaggle</code>上学习冠军的代码来强化数据科学技能。</p></li><li><p>免费使用计算资源。<code>Kaggle</code>的<code>Kernels</code>功能允许你在浏览器编程、并通过服务器的<code>GPU</code>来加速你的计算。</p></li><li><p>讨论交流学习。<code>Kaggle</code>上有论坛交流功能，允许你与相同的爱好者一起交流学习。</p></li><li><p>学习<code>Python</code>、<code>ML</code>、<code>Pandas</code>、<code>DL</code>等技能。<code>Kaggle</code>上提供了免费的微课给大家学习，供初学者快速入门学习。</p></li></ol><p>本篇文章侧重点是第<code>4</code>条，教你如何将自己的代码丢到<code>Kaggle</code>上训练。</p><p>注意，<code>Kaggle</code>目前只支持<code>Python</code>和<code>R</code>两种编程语言。</p><h1 id="Kernel硬件配置"><a href="#Kernel硬件配置" class="headerlink" title="Kernel硬件配置"></a>Kernel硬件配置</h1><p><code>GPU</code>：Nvidia Tesla P100-PCIE-16GB 1.3285GHz</p><p><code>GPU连续使用时间</code>：6h</p><p><code>CPU Frequency</code>： 2.3GHz</p><p><code>RAM</code>：14GB</p><p><code>Disk</code>：5.2GB</p><h1 id="使用教程"><a href="#使用教程" class="headerlink" title="使用教程"></a>使用教程</h1><h2 id="基本介绍"><a href="#基本介绍" class="headerlink" title="基本介绍"></a>基本介绍</h2><ol><li><p>登陆<code>Kaggle</code><a href="https://www.kaggle.com/" target="_blank" rel="noopener">官网</a>，注册账号并登陆。在<code>Kaggle</code>注册账号是免费的。</p></li><li><p>点击导航栏的<code>Kernels</code></p></li></ol><p><img src="2.png" alt></p><ol><li>点击页面上部的<code>New Kernel</code>来创建一个新的<code>Kernel</code>。粗略地说<code>Kernel</code>就是一个代码的工程项目。</li></ol><p><img src="3.png" alt></p><ol><li>点击左边的<code>Script</code>来创建一个脚本。这个脚本就是你项目运行的主要文件。</li></ol><p><img src="4.png" alt></p><ol><li>顶部的标题栏的功能。</li></ol><p><img src="5.png" alt></p><ol><li>侧边状态栏的主要功能。<code>Sessions</code>显示资源占用状态，<code>Versions</code>显示版本管理，<code>Draft Environment</code>显示你上传数据（注意：上传后该区域只读，不能写），<code>Settings</code>显示设置（如<code>GPU</code>开关、包的管理）</li></ol><p><img src="6.png" alt></p><ol><li>底部状态栏功能。</li></ol><p><img src="7.png" alt></p><p>用完<code>Kernel</code>建议点击类似电源键的按钮关闭<code>Kernel</code>哦（关闭后所有输出文件将会丢失）</p><ol><li>代码输入窗口。它已预先帮你输入一些示例代码，可以删掉重写。</li></ol><p><img src="8.png" alt></p><h2 id="使用示例"><a href="#使用示例" class="headerlink" title="使用示例"></a>使用示例</h2><p>我们以<code>Tensorflow</code>平台的<code>YoloV3-Tiny</code>模型在数据集<code>VOC2007</code>的训练为例，介绍如何使用<code>Kaggle</code>训练我们的模型，并保存结果，将模型下载到本地。</p><p>提示：<code>Kaggle</code>已经为我们准备好常用的环境了，无需我们从头搭建开发环境。一般直接用就好了。</p><h3 id="文件准备"><a href="#文件准备" class="headerlink" title="文件准备"></a>文件准备</h3><p>首先我们要在本地弄好相关文件，再上传到<code>Kaggle</code>上去。</p><p>本地的准备参考这篇：<a href="https://my.oschina.net/u/876354/blog/1927881" target="_blank" rel="noopener">【AI实战】动手训练自己的目标检测模型（YOLO篇）</a></p><p>有关<code>YOLO</code>参考这篇：<a href="https://karbo.online/dl/yolo_starter/#more" target="_blank" rel="noopener">用YOLO实现目标检测</a></p><p>然后按照实际情况修改<code>train.py</code>的相关参数，例如将<code>batch_size</code>改成<code>128</code>，<code>epochs</code>改小一点等等。</p><p>注意训练时间不能超过<code>6</code>个小时，否则<code>Kaggle</code>会自动关闭你的<code>Kernel</code>。</p><p>并且<code>Keras</code>版的<code>YOLO</code>的标签文件与<code>Darknet</code>版的不同，标签文件要重新生成。然后执行：</p><pre><code> cat 2007_train.txt 2007_val.txt &gt; train.txt</code></pre><p>即我们使用验证集和训练集混合起来一起训练，最后替换下路径前缀。</p><p>但有以下几点要注意下：</p><ol><li><p>上传后不能在线修改你上传的东西，只能删除该压缩包（删除方法见第三节：再次训练）后重新上传（如果数据量巨大，重新上传十分费时）。所以最好需要确保第一次上传的东西就没有问题，否则更改会比较繁琐。</p></li><li><p>上传时，建议是分别上传几样东西（分别压缩打包上传）：</p><ul><li>模型的配置文件</li><li>训练的数据文件</li><li>模型<code>.h5</code>文件</li></ul></li></ol><p>上传方式：点击右侧白色的侧边状态栏中的<code>+ Add Data</code>按钮，在弹出的窗口中，点击右上角的<code>Upload</code>，然后选择文件去上传（只能上传单个文件，这就是为什么叫你打包压缩的原因）。</p><p>上传后，<code>Kaggle</code>会自动帮你解压缩，点击右边的文件树，点选其中的一个文件，会在左侧弹出白色的文件管理弹窗，弹窗的上端会显示该文件的路径：</p><p><img src="9.png" alt></p><p>点击中间的那个蓝色的按钮你可以复制路径到剪切板中。</p><p>当你上传了多个压缩包或文件时，路径的命名规则一般是这样的：</p><ul><li>对于上传了文件：<code>../input/数据集的名字/上传的文件名字</code></li><li>对于上传了压缩包：<code>../input/数据集的名字/压缩包的名字/压缩包底下的路径</code></li></ul><ol><li><p>其中上述的 <code>图片路径的</code>.txt<code>文件</code> 不能单纯按照<a href="https://my.oschina.net/u/876354/blog/1927881" target="_blank" rel="noopener">【AI实战】动手训练自己的目标检测模型（YOLO篇）</a>来做，你要将路径替换成上述第二点描述的那样。因为你执行的主脚本文件并不是在你上传的东西里面，你需要使用类似<code>../input/XXX/XXX</code>的格式来调用你上传的东西。</p></li><li><p>对于脚本中的文件路径也是如此，类似于上述的第三点来做。否则会提示会找不到你上传的文件。实际上有关路径的一切东西都要按照上述的路径规则来做，否则就找不到文件。</p></li><li><p>如果提示<code>import</code>时找不到文件，这是因为你上传的包没有加入系统变量，那么你需要：</p><pre class=" language-lang-python"><code class="language-lang-python">import syskaggle_path_prefix = "../input/keras-yolov3tiny-voc2007/keras-yolo3/"sys.path.append(kaggle_path_prefix)</code></pre></li></ol><p>这里请根据你的实际情况修改上述<code>kaggle_path_prefix</code>的值。</p><p>这里<code>kaggle_path_prefix</code>目录下需要包含那个你刚刚上传的压缩包里名叫<code>yolo3</code>的<code>Python</code>包的文件夹。</p><ol><li>保存文件的路径请直接填写文件名，像这样：</li></ol><pre class=" language-lang-python"><code class="language-lang-python">model.save_weights('trained_weights_final.h5')</code></pre><p>这是因为<code>input</code>文件夹是只读的，且保存到其他地方去无法输出下载，你也找不到输出的文件。况且当<code>Kernel</code>关闭后你的一切东西就会丢失。</p><h3 id="运行并提交"><a href="#运行并提交" class="headerlink" title="运行并提交"></a>运行并提交</h3><p>点击顶部标题栏亮起的蓝色<code>Commit</code>按钮，以运行全部代码并保存结果，最后它会保存你输出的文件。</p><p>如果允许的窗口不慎点没了，可以右侧的<code>Versions</code>中，点击：</p><p><img src="10.png" alt></p><p>重新弹出运行的窗口（除非你点了<code>Cancel commit</code>）。</p><p><img src="1.png" alt></p><p>运行完毕后，点击：</p><p><img src="11.png" alt></p><p>来打开<code>Kernel</code>页面。</p><p>如果你有输出文件，在左侧的：</p><p><img src="12.png" alt></p><p>点击<code>Output</code>就可以切换到输出的文件列表，然后就可以下载你输出的文件啦，选中你想要的模型下载即可。</p><p>如果运行出错，请点击上图所示的<code>Log</code>查看错误日志（有必要时点击<code>Download Log</code>按钮下载日志到本地），按照错误提示修复错误即可。</p><h3 id="再次训练"><a href="#再次训练" class="headerlink" title="再次训练"></a>再次训练</h3><p>只需将原本的模型文件数据集删除，然后再添加上传上去，再次<code>Commit</code>就好了。</p><p>删除数据集的步骤：</p><ol><li><p>点击数据集旁边的那个红色的叉叉，将数据集从当前<code>Kernel</code>移除</p></li><li><p>点击自己的头像，进入<code>My Profile</code>页面，然后点击<code>Datasets</code></p></li></ol><p><img src="13.png" alt></p><ol><li>然后点击<code>Settings</code></li></ol><p><img src="14.png" alt></p><ol><li>最后点击<code>Delete Dataset</code>并确认即可</li></ol><p><img src="15.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> dl </category>
          
      </categories>
      
      
        <tags>
            
            <tag> dl </tag>
            
            <tag> kaggle </tag>
            
            <tag> gpu </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>工具</title>
      <link href="/tools/tools/"/>
      <url>/tools/tools/</url>
      
        <content type="html"><![CDATA[<h1 id="数学"><a href="#数学" class="headerlink" title="数学"></a>数学</h1><blockquote><p><a href="https://www.geogebra.org/" target="_blank" rel="noopener">GeoGebra绘图</a><br><a href="https://www.wolframalpha.com/" target="_blank" rel="noopener">Wolfram|Alpha问答系统</a><br><a href="https://calcme.com/a" target="_blank" rel="noopener">CalcMe在线计算</a></p></blockquote><h1 id="图片处理类"><a href="#图片处理类" class="headerlink" title="图片处理类"></a>图片处理类</h1><blockquote><p><a href="https://www.gaoding.com/koutu" target="_blank" rel="noopener">在线抠图（国内）</a><br><a href="https://www.remove.bg/" target="_blank" rel="noopener">在线抠图（国外）</a></p><p><a href="https://zhitu.isux.us/" target="_blank" rel="noopener">图片压缩（国内）</a><br><a href="https://recompressor.com/" target="_blank" rel="noopener">图片压缩（国外）</a></p><p><a href="http://bigjpg.com/" target="_blank" rel="noopener">图片放大（国内）</a><br><a href="https://bulkresizephotos.com/" target="_blank" rel="noopener">图片放大（国外）</a></p><p><a href="https://zh.vectormagic.com/" target="_blank" rel="noopener">位矢转换（国内）</a><br><a href="https://www.vectorization.org/" target="_blank" rel="noopener">位矢转换（国外）</a></p></blockquote><h1 id="特效类"><a href="#特效类" class="headerlink" title="特效类"></a>特效类</h1><blockquote><p><a href="https://pissang.github.io/little-big-city/" target="_blank" rel="noopener">地球城市卡通化</a><br><a href="http://planetmaker.wthr.us/#" target="_blank" rel="noopener">太空看地球仿真</a><br><a href="https://codepen.io/pissang/full/geajpX" target="_blank" rel="noopener">渐变波浪生成</a><br><a href="https://codepen.io/Yakudoo/full/rJjOJx" target="_blank" rel="noopener">彩色流动生成</a><br><a href="https://pissang.github.io/papercut-box-art/" target="_blank" rel="noopener">等高层风格生成</a><br><a href="https://coolbackgrounds.io/" target="_blank" rel="noopener">简洁背景图制作</a><br><a href="https://trianglify.io/" target="_blank" rel="noopener">三角形渐变图生成</a></p></blockquote><h1 id="文字云"><a href="#文字云" class="headerlink" title="文字云"></a>文字云</h1><blockquote><p><a href="https://wordart.com/" target="_blank" rel="noopener">文字云制作1</a><br><a href="https://www.jasondavies.com/wordcloud/" target="_blank" rel="noopener">文字云制作2</a><br><a href="http://word2art.com/" target="_blank" rel="noopener">文字云艺术图制作</a></p></blockquote><h1 id="文件转换"><a href="#文件转换" class="headerlink" title="文件转换"></a>文件转换</h1><blockquote><p><a href="https://cn.office-converter.com/" target="_blank" rel="noopener">文档、视频、音乐、图片等转换（国内）</a><br><a href="https://cloudconvert.com/" target="_blank" rel="noopener">文件转换（国外）</a></p></blockquote><h1 id="PDF操作"><a href="#PDF操作" class="headerlink" title="PDF操作"></a>PDF操作</h1><blockquote><p><a href="https://smallpdf.com/cn" target="_blank" rel="noopener">PDF操作（国内）</a><br><a href="https://pdfcandy.com/" target="_blank" rel="noopener">PDF操作（国外）</a></p></blockquote><h1 id="在线PS"><a href="#在线PS" class="headerlink" title="在线PS"></a>在线PS</h1><blockquote><p><a href="http://ps.xunjiepdf.com/" target="_blank" rel="noopener">在线PS（国内）</a><br><a href="https://www.photopea.com/" target="_blank" rel="noopener">在线PS（国外）</a></p></blockquote><h1 id="二维码工具"><a href="#二维码工具" class="headerlink" title="二维码工具"></a>二维码工具</h1><blockquote><p><a href="https://cli.im/" target="_blank" rel="noopener">二维码工具</a></p></blockquote><h1 id="Windows-Office"><a href="#Windows-Office" class="headerlink" title="Windows/Office"></a>Windows/Office</h1><blockquote><p><a href="https://msdn.itellyou.cn/" target="_blank" rel="noopener">MSDN</a><br><a href="http://kms.cangshui.net/" target="_blank" rel="noopener">KMS激活</a><br><a href="https://otp.landian.vip/zh-cn/" target="_blank" rel="noopener">Office Tool Plus</a></p></blockquote><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><blockquote><p><a href="https://www.zhuangbi.info/" target="_blank" rel="noopener">图片表情包搜索</a><br><a href="http://geektyper.com/" target="_blank" rel="noopener">黑客打代码装逼网页</a><br><a href="https://colordrop.io/" target="_blank" rel="noopener">纯色配色方案</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> tools </category>
          
      </categories>
      
      
        <tags>
            
            <tag> tools </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>用YOLO实现目标检测</title>
      <link href="/dl/yolo-starter/"/>
      <url>/dl/yolo-starter/</url>
      
        <content type="html"><![CDATA[<h1 id="Why-YOLO"><a href="#Why-YOLO" class="headerlink" title="Why YOLO?"></a>Why YOLO?</h1><p>You only look once (YOLO)是顶尖的实时目标检测模型。</p><p>下面是YOLO与其他模型的性能对比。</p><p><img src="https://pjreddie.com/media/image/map50blue.png" alt></p><p>可以看出YOLO 具有耗时较少，准确率不低的优点。</p><h1 id="配置环境"><a href="#配置环境" class="headerlink" title="配置环境"></a>配置环境</h1><p>建议环境：Win10、支持CUDA的Nvidia显卡、Python3、CUDA&gt;=9.0、CUDNN&gt;=7.0、VS2015、OPENCV&lt;4.0</p><p>详细操作步骤参考：<br><a href="https://blog.csdn.net/sinat_26940929/article/details/80342660" target="_blank" rel="noopener">Yolov3+windows10+VS2015部署安装</a><br><a href="https://github.com/AlexeyAB/darknet#how-to-compile-on-windows-legacy-way" target="_blank" rel="noopener">How to compile on Windows (legacy way)</a></p><p>编译时可能遇到形如compute_75的错误，解决方法：用文本的方式打开darknet.vcxproj文件，将所有的compute_75替换为compute_50，将所有的sm_75替换为sm_50，具体替换成什么，请参考<a href="https://github.com/tpruvot/ccminer/wiki/Compatibility" target="_blank" rel="noopener">Compatibility</a></p><h1 id="YOLO初体验"><a href="#YOLO初体验" class="headerlink" title="YOLO初体验"></a>YOLO初体验</h1><h2 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h2><p>这一步我们尝试使用下刚刚编译好的YOLO。</p><p>由于可能缺少模型的权重文件，我们从这里下载<a href="https://pjreddie.com/media/files/yolov3.weights" target="_blank" rel="noopener">YOLO-V3权重文件（236MB）</a></p><p>然后将目录切换到<code>D:\darknet-master\build\darknet\x64</code>，打开命令行，输入以下语句：</p><pre><code>./darknet.exe detect cfg/yolov3.cfg yolov3.weights data/dog.jpg</code></pre><p>正常情况下会得到以下效果：</p><p><img src="1.png" alt></p><p>同时也会得到<code>predictions.jpg</code>保存在相同目录下。</p><p>运行一次模型需要：</p><ul><li>配置文件（.cfg）</li><li>权重文件（.weights）</li><li>被测图片</li></ul><p>同时尝试将上述语句最后的<code>data/dog.jpg</code>分别替换为<code>data/eagle.jpg</code>, <code>data/dog.jpg</code>, <code>data/person.jpg</code>, or <code>data/horses.jpg</code>，查看效果吧。</p><p>上述语句中的<code>detect</code>是一种缩写，上述语句也等同于</p><pre><code>./darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights data/dog.jpg</code></pre><p>当然也可以载入一次模型进行多次预测，输入以下指令（就是去掉图片选项）：</p><pre><code>./darknet.exe detect cfg/yolov3.cfg yolov3.weights</code></pre><p>然后它会提示你输入图片路径：</p><p><img src="2.png" alt></p><p>输入路径后回车，按<code>Ctrl+C</code>退出输入状态。</p><p>除此之外，<code>YOLO</code>还提供设定阈值方法来剔除置信度过低的结果。例如若想显示所有结果则使用以下代码（此处阈值设置为0）：</p><pre><code>./darknet.exe detect cfg/yolov3.cfg yolov3.weights data/dog.jpg -thresh 0</code></pre><p>默认的阈值是0.25。</p><h2 id="Tiny-YOLOv3"><a href="#Tiny-YOLOv3" class="headerlink" title="Tiny YOLOv3"></a>Tiny YOLOv3</h2><p>首先下载<code>Tiny YOLOv3</code>的<a href="https://pjreddie.com/media/files/yolov3-tiny.weights" target="_blank" rel="noopener">权重文件（34MB）</a>，丢到与<code>darknet.exe</code>同级的目录下。</p><p>使用以下命令运行：</p><pre><code>./darknet.exe detect cfg/yolov3-tiny.cfg yolov3-tiny.weights data/dog.jpg</code></pre><p><img src="3.png" alt></p><p>可以看到<code>tiny</code>版本的精度略低，但是速度快。</p><h2 id="使用摄像头或视频"><a href="#使用摄像头或视频" class="headerlink" title="使用摄像头或视频"></a>使用摄像头或视频</h2><p>使用以下命令在摄像头<code>0</code>（OPENCV默认使用摄像头<code>0</code>）运行<code>Tiny YOLOv3</code></p><pre><code>./darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights</code></pre><p>使用参数<code>-c &lt;num&gt;</code>指定使用哪一只摄像头。<br>或者使用以下命令实现<code>Tiny YOLOv3</code>对视频的目标检测：</p><pre><code>./darknet detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights &lt;video file&gt;</code></pre><h1 id="训练自己的YOLO"><a href="#训练自己的YOLO" class="headerlink" title="训练自己的YOLO"></a>训练自己的YOLO</h1><p>这里我们我们使用<code>Pascal VOC2007</code>数据集训练<code>YOLOv3-tiny</code>模型。</p><p>关于该数据集的介绍，可以查看<a href="https://arleyzhang.github.io/articles/1dc20586/" target="_blank" rel="noopener">这篇文章</a></p><p>具体步骤详情：<br><a href="https://github.com/AlexeyAB/darknet#how-to-train-to-detect-your-custom-objects" target="_blank" rel="noopener">How to train (to detect your custom objects)</a><br><a href="https://github.com/AlexeyAB/darknet#how-to-train-tiny-yolo-to-detect-your-custom-objects" target="_blank" rel="noopener">How to train tiny-yolo (to detect your custom objects)</a></p><h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><p>为了训练<code>YOLO</code>我们需要<code>2007</code>年的<code>VOC</code>数据集，可以从<a href="https://pjreddie.com/projects/pascal-voc-dataset-mirror/" target="_blank" rel="noopener">这里</a>下载。下载完后解压，解压完训练数据都在<code>VOCdevkit/</code>文件夹下。</p><p>训练<code>YOLO</code>需要使用特别格式的标签数据文件，它是一个<code>.txt</code>文本文件。<br>这个<code>.txt</code>文件的每一行是一个标签，一个文件对应一张图片，它看起来像这样：<br><code>&lt;object-class&gt; &lt;x&gt; &lt;y&gt; &lt;width&gt; &lt;height&gt;</code><br>注意此处的中心<code>x</code>、中心<code>y</code>、框<code>width</code>和框<code>height</code>是相对于图片宽度和高度的值，都是不大于<code>1</code>的小数。</p><p>转换公式：</p><script type="math/tex; mode=display">x=\frac{x_{min}+(x_{max}-x_{min})/2}{W_{image}}</script><script type="math/tex; mode=display">y=\frac{y_{min}+(y_{max}-y_{min})/2}{H_{image}}</script><script type="math/tex; mode=display">w=\frac{x_{max}-x_{min}}{W_{image}}</script><script type="math/tex; mode=display">h=\frac{y_{max}-y_{min}}{H_{image}}</script><p>为了得到这些<code>.txt</code>文件，我们可以方便地通过运行一个叫<code>voc_label.py</code>的脚本来生成。<br>脚本的内容如下：</p><pre class=" language-lang-python"><code class="language-lang-python">import xml.etree.ElementTree as ETimport pickleimport osfrom os import listdir, getcwdfrom os.path import joinsets = [('2007', 'train'), ('2007', 'val'), ('2007', 'test')]classes = [    "aeroplane", "bicycle", "bird", "boat", "bottle", "bus", "car", "cat",    "chair", "cow", "diningtable", "dog", "horse", "motorbike", "person",    "pottedplant", "sheep", "sofa", "train", "tvmonitor"]# 位置坐标转换def convert(size, box):    dw = 1. / (size[0])    dh = 1. / (size[1])    x = (box[0] + box[1]) / 2.0 - 1    y = (box[2] + box[3]) / 2.0 - 1    w = box[1] - box[0]    h = box[3] - box[2]    x = x * dw    w = w * dw    y = y * dh    h = h * dh    return (x, y, w, h)# label转换def convert_annotation(year, image_id):    in_file = open('VOC%s/Annotations/%s.xml' % (year, image_id))    out_file = open('VOC%s/labels/%s.txt' % (year, image_id), 'w')    tree = ET.parse(in_file)    root = tree.getroot()    size = root.find('size')    w = int(size.find('width').text)    h = int(size.find('height').text)    for obj in root.iter('object'):        difficult = obj.find('difficult').text        cls = obj.find('name').text        if cls not in classes or int(difficult) == 1:            continue        cls_id = classes.index(cls)        xmlbox = obj.find('bndbox')        b = (float(xmlbox.find('xmin').text), float(xmlbox.find('xmax').text),             float(xmlbox.find('ymin').text), float(xmlbox.find('ymax').text))        bb = convert((w, h), b)        out_file.write(            str(cls_id) + " " + " ".join([str(a) for a in bb]) + '\n')wd = getcwd()for year, image_set in sets:    if not os.path.exists('VOC%s/labels/' % (year)):        os.makedirs('VOC%s/labels/' % (year))    image_ids = open('VOC%s/ImageSets/Main/%s.txt' %                     (year, image_set)).read().strip().split()    list_file = open('%s_%s.txt' % (year, image_set), 'w')    for image_id in image_ids:        list_file.write('%s/VOC%s/JPEGImages/%s.jpg\n' % (wd, year, image_id))        convert_annotation(year, image_id)    list_file.close()os.system("cat 2007_train.txt 2007_val.txt > train.txt")os.system("cat 2007_train.txt 2007_val.txt 2007_test.txt > train.all.txt")</code></pre><p>将脚本保存到与<code>VOC2007</code>文件夹同级的目录，命名为<code>voc_label.py</code>，然后在此目录下打开命令行，执行：</p><pre><code>python voc_label.py</code></pre><p>很快，这个脚本会生成一些必要的文件。它生成了很多标签文件，位于<code>VOCdevkit/VOC2007/labels/</code>路径下。<br>并且在与<code>VOC2007</code>同级的目录下，你应该会看到如下的文件：</p><pre><code>2007_train.txt2007_val.txt2007_test.txttrain.txttrain.all.txt</code></pre><p>如果是自己采集的数据，需要标注，请使用<a href="https://github.com/tzutalin/labelImg" target="_blank" rel="noopener">LabelImg</a>或<a href="https://github.com/AlexeyAB/Yolo_mark" target="_blank" rel="noopener">Yolo_mark</a>工具，以生成<code>YOLO</code>格式的文本文件。然后将图片的路径汇总到一个文本文件，如<code>train.txt</code>、<code>val.txt</code>和<code>test.txt</code>里，一行一个图片路径。</p><h2 id="准备模型"><a href="#准备模型" class="headerlink" title="准备模型"></a>准备模型</h2><p>新建个文件夹，我们用来保存与模型有关的数据。我这里路径为：<code>D:/model/voc_model/</code><br>我这里<code>VOC2007</code>文件夹位于：<code>D:/dataset/VOCdevkit/</code>，<code>Darknet.exe</code>位于<code>D:/darknet-master/build/darknet/x64/</code></p><h3 id="准备权重文件"><a href="#准备权重文件" class="headerlink" title="准备权重文件"></a>准备权重文件</h3><p>首先下载默认的权重文件到你刚刚新建的模型文件夹（我这里是<code>D:/model/voc_model/</code>）：<br><a href="https://pjreddie.com/media/files/yolov3-tiny.weights" target="_blank" rel="noopener">默认权重文件</a></p><p>在模型文件夹运行如下指令，获取预训练的权重文件<code>yolov3-tiny.conv.15</code>，使用如下命令：</p><pre><code>D:/darknet-master/build/darknet/x64/darknet.exe partial D:/darknet-master/build/darknet/x64/cfg/yolov3-tiny.cfg yolov3-tiny.weights yolov3-tiny.conv.15 15</code></pre><p>其他预训练权重可以从<a href="https://pjreddie.com/darknet/imagenet/" target="_blank" rel="noopener">这里</a>下载</p><h3 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h3><ol><li><p>在模型文件夹创建一份<code>VOC2007.names</code>文本文件，其中该文件的每一行都是种类的名字，应该使得行数等于种类数<code>classes</code>的值。</p></li><li><p>在模型文件夹创建一份<code>VOC2007.data</code>文本文件，填入以下内容。<code>classes</code>是种类的个数、<code>train</code>是训练图片路径的文本文件，<code>valid</code>是验证图片路径的文本文件，<code>names</code>是种类名字的文件，<code>backup</code>路径则用于保存备份的权重文件（每迭代<code>100</code>次保存一次文件（带<code>_last</code>后缀），每<code>1000</code>次保存一次文件（带<code>_xxxx</code>后缀））。<br>如果没有验证集，则设置<code>valid</code>为与<code>train</code>相同的值即可，那么将测试在训练集上的精度。</p><pre><code>classes = 20train  = D:/dataset/VOCdevkit/train.txtvalid  = D:/dataset/VOCdevkit/2007_test.txtnames = VOC2007.namesbackup = backup/</code></pre></li><li><p>复制<code>D:/darknet-master/build/darknet/x64/cfg/yolov3-tiny_obj.cfg</code>文件，在模型文件夹另存为<code>yolov3-tiny-obj.cfg</code>，然后按照下述规则修改该文件：</p></li></ol><ul><li>修改使得<code>batch=64</code></li><li>修改使得<code>subdivisions=8</code></li><li>修改所有的<code>classes</code>值为<code>20</code>（这里<code>classes</code>是目标检测物体的种类个数）</li><li>修改所有位于行<code>[yolo]</code>之上的<code>[convolutional]</code>层的<code>filters</code>值为：$ filters = (classes + 5) * 3 $， <code>filters</code>的值需要计算出来再填入。注意，这不是修改所有<code>filters</code>的值，仅仅是修改恰好位于<code>[yolo]</code>这行之上该层的<code>filters</code>的值，可能需要修改多处。</li><li>如果你要修改输入图像的<code>width</code>和<code>height</code>值，请注意这两个值必须能被<code>32</code>整除。</li></ul><h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><p>在模型文件夹运行命令：</p><pre><code>D:/darknet-master/build/darknet/x64/darknet.exe detector train VOC2007.data yolov3-tiny-obj.cfg yolov3-tiny.conv.15</code></pre><ul><li>如果你在<code>avg loss</code>里看到<code>nan</code>，意味着训练失败；在其他地方出现<code>nan</code>则是正常的。</li><li><p>如果出错并显示<code>Out of memory</code>，尝试将<code>.cfg</code>文件的<code>subdivisions</code>值增大（建议为$ 2^n $）。</p></li><li><p>使用附加选项<code>-dont_show</code>来关闭训练时默认显示的损失曲线窗口</p></li><li><p>使用附加选项<code>-map</code>来显示<code>mAP</code>值</p></li><li><p>训练完成后的权重将保存于你在<code>.data</code>文件中设置的<code>backup</code>值路径下</p></li><li><p>你可以从<code>backup</code>值的路径下找到你的备份权重文件，并以此接着训练模型</p></li><li><p>多GPU训练：<a href="https://github.com/AlexeyAB/darknet#how-to-train-with-multi-gpu" target="_blank" rel="noopener">How to train with multi-GPU</a></p></li><li><p>训练完成后使用命令<code>darknet.exe detector test data/obj.data yolo-obj.cfg yolo-obj_8000.weights</code>针对输入的图片查看识别结果。</p></li></ul><h1 id="在COCO上训练YOLO"><a href="#在COCO上训练YOLO" class="headerlink" title="在COCO上训练YOLO"></a>在COCO上训练YOLO</h1><p>从<a href="http://cocodataset.org/#overview" target="_blank" rel="noopener">这里</a>下载<code>COCO</code>数据集。<br>也可以使用位于<code>scripts/get_coco_dataset.sh</code>的脚本来下载<code>COCO</code>数据集。</p><pre><code>cp scripts/get_coco_dataset.sh datacd databash get_coco_dataset.sh # 会下载到data文件夹下</code></pre><p>然后修改<code>cfg/coco.data</code>文件，指定你的数据路径。<br>接着修改<code>cfg/yolo.cfg</code>文件，配置训练使用的参数。<br>然后训练：</p><pre><code>./darknet detector train cfg/coco.data cfg/yolov3.cfg darknet53.conv.74</code></pre><p>如果你想使用4个GPU来跑，在上述语句附加参数<code>-gpus 0,1,2,3</code>即可<br>如果你想从检查点停止或重新运行，使用：</p><pre><code>./darknet detector train cfg/coco.data cfg/yolov3.cfg backup/yolov3.backup</code></pre><h1 id="YOLO进阶"><a href="#YOLO进阶" class="headerlink" title="YOLO进阶"></a>YOLO进阶</h1><h2 id="预训练模型下载地址"><a href="#预训练模型下载地址" class="headerlink" title="预训练模型下载地址"></a>预训练模型下载地址</h2><p><a href="https://github.com/AlexeyAB/darknet#pre-trained-models" target="_blank" rel="noopener">GitHub预训练模型</a><br><a href="https://pjreddie.com/darknet/yolo/" target="_blank" rel="noopener">官网</a> 的 <code>Performance on the COCO Dataset</code>部分</p><h2 id="配置文件地址"><a href="#配置文件地址" class="headerlink" title="配置文件地址"></a>配置文件地址</h2><p>位于路径<code>darknet/cfg/</code>下<br><a href="https://pjreddie.com/darknet/yolo/" target="_blank" rel="noopener">官网</a> 的 <code>Performance on the COCO Dataset</code>部分</p><h2 id="特殊模型"><a href="#特殊模型" class="headerlink" title="特殊模型"></a>特殊模型</h2><p><a href="https://github.com/AlexeyAB/darknet/blob/master/cfg/yolov3-tiny_xnor.cfg" target="_blank" rel="noopener">XNOR-net (2到4倍的性能)</a><br><a href="https://github.com/AlexeyAB/yolo2_light" target="_blank" rel="noopener">INT8-quantization (快30%)</a></p><h2 id="命令行语法"><a href="#命令行语法" class="headerlink" title="命令行语法"></a>命令行语法</h2><p>在<code>Linux</code>使用 <code>./darknet</code>而不是 <code>darknet.exe</code>，像这样：<code>./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights</code>。在<code>Linux</code>中可执行文件 <code>./darknet</code> 在根目录，而 <code>Windows</code>则在<code>/build/darknet/x64</code>路径下。</p><ul><li><code>Yolo v3 COCO</code> - 图像: <code>darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25</code></li><li>另一种方法 <code>Yolo v3 COCO</code> - 图像： <code>darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25</code></li><li>输出物体坐标： <code>darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg</code></li><li><code>Yolo v3 COCO</code> - 视频：<code>darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4</code></li><li><code>Yolo v3 COCO</code> - 摄像头<code>0</code>： <code>darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0</code></li><li><code>Yolo v3 COCO</code> - 网络摄像头： <code>darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg</code></li><li><code>Yolo v3</code> - 保存结果视频： <code>darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi</code></li><li><code>Yolo v3 Tiny COCO</code> - 视频： <code>darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4</code></li><li><code>JSON</code>和<code>MJPEG</code>服务器，允许从您的软件或<code>Web</code>浏览器<code>IP</code>地址：<code>8070</code>和<code>8090</code>进行多个连接： <code>./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output</code></li><li><code>Yolo v3 Tiny</code> 在<code>GPU1</code>上运行：<code>darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4</code></li><li>在 <code>Amazon EC2</code> 服务器上训练，使用<code>Chrome</code>或<code>Firefox</code>浏览器，通过像这样（<code>http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090</code>）的链接查看 <code>mAP</code> 和 <code>Loss</code> 曲线图（注：<code>Darknet</code>应该与<code>OpenCV</code>一起编译）： <code>./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map</code></li><li><code>186 MB Yolo9000</code> - 图像： <code>darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights</code></li><li>处理 <code>data/train.txt</code>中记载路径的图片，然后保存检测结果到 <code>result.json</code>文件中：<code>darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json &lt; data/train.txt</code></li><li>处理 <code>data/train.txt</code>中记载路径的图片，然后保存检测结果到 <code>result.txt</code>中：<br><code>darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output &lt; data/train.txt &gt; result.txt</code></li><li>伪标记 - 识别文本文件 <code>data/new_train.txt</code>中记载路径的图片，然后以<code>YOLO</code>训练数据的格式保存识别结果<code>&lt;image_name&gt;.txt</code>（这样子可以增大训练数据量）： <code>darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels &lt; data/new_train.txt</code></li><li>计算 <code>anchors</code>：<code>darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416</code></li><li>计算 <code>IoU=50</code>下的 <code>mAP</code>值： <code>darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights</code></li><li>计算 <code>IoU=75</code>下的 <code>mAP</code>值： <code>darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75</code></li></ul><h2 id="炼丹技巧"><a href="#炼丹技巧" class="headerlink" title="炼丹技巧"></a>炼丹技巧</h2><h3 id="早停"><a href="#早停" class="headerlink" title="早停"></a>早停</h3><p>粗略来讲，对于每个类别<code>2000</code>次迭代，总迭代次数不低于<code>4000</code>次。</p><p>具体来说：</p><ul><li>多次迭代仍不能降低平均损失值（<code>avg loss</code>）时（<code>avg loss</code>可能最终收敛于<code>0.05 ~ 3.0</code>之间的值）</li><li>早停后，你应该从多个权重文件中选取表现最好的，这样或许可以避免过拟合。使用类似如下的指令来验证训练的好坏：<pre><code>darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights</code></pre>然后选取<code>mAP</code>最大的或<code>IoU</code>最大的作为最终权重。</li></ul><h3 id="提高精度策略"><a href="#提高精度策略" class="headerlink" title="提高精度策略"></a>提高精度策略</h3><ul><li>在<code>.cfg</code>文件中设置<code>random=1</code>，它会通过对不同分辨率的图片进行训练以提高精度</li><li>使用高分辨率的图像输入。在<code>.cfg</code>文件中设置<code>height</code>和<code>width</code>值。但是你无需重头训练，只需使用回<code>416x416</code>分辨率的权重数据就好了。</li><li>检查数据集标注是否正确符合规范</li><li>检查训练数据集数据量是否过少</li><li>迭代次数推荐不低于<code>2000 * classes</code></li><li>你的训练样本希望包含没有目标物体的图像，即该图像中没有出现目标物体，标签文件是空的文本。</li><li>如果图片里有很多数量的目标物体，那么在<code>.cfg</code>文件中最后的<code>[yolo]</code>层或<code>[region]</code>层中添加参数<code>max=200</code>，这也可以设定成更高的值。</li><li>如果目标物体很小（缩放成<code>416x416</code>尺寸后小于<code>16x16</code>），那么将<a href="https://github.com/AlexeyAB/darknet/blob/6390a5a2ab61a0bdf6f1a9a6b4a739c16b36e0d7/cfg/yolov3.cfg#L720" target="_blank" rel="noopener">第720行</a>设置为<code>layers = -1, 11</code>，将<a href="https://github.com/AlexeyAB/darknet/blob/6390a5a2ab61a0bdf6f1a9a6b4a739c16b36e0d7/cfg/yolov3.cfg#L717" target="_blank" rel="noopener">第717行</a>设置为<code>stride=4</code></li><li>如果目标物体有些很大有些又很小，那么请使用修改后的模型：<br><a href="https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov3_5l.cfg" target="_blank" rel="noopener">Full-model: 5 yolo layers</a><br><a href="https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov3-tiny_3l.cfg" target="_blank" rel="noopener">Tiny-model: 3 yolo layers</a><br><a href="https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov3-spp.cfg" target="_blank" rel="noopener">Spatial-full-model: 3 yolo layers</a></li><li>如果你的模型需要区分左右手性，例如区分左手和右手、左转和右转，那么需要关闭翻转数据增强选项，即添加<code>flip=0</code>到<a href="https://github.com/AlexeyAB/darknet/blob/3d2d0a7c98dbc8923d9ff705b81ff4f7940ea6ff/cfg/yolov3.cfg#L17" target="_blank" rel="noopener">这里</a></li><li>如果想要模型具有尺度的鲁棒性，则必须训练样本中包含多尺度的照片。这是因为<code>YOLO</code>不具有尺度变化的适应性。</li><li>要想加速模型的训练（但会降低预测精度）应该使用<code>Fine-Tuning</code>而不是<code>Transfer-Learning</code>，需要在<a href="https://github.com/AlexeyAB/darknet/blob/6d44529cf93211c319813c90e0c1adb34426abe5/cfg/yolov3.cfg#L548" target="_blank" rel="noopener">这里</a>设置参数<code>stopbackward=1</code>，然后运行<code>./darknet partial cfg/yolov3.cfg yolov3.weights yolov3.conv.81 81</code>，这会创建文件<code>yolov3.conv.81</code>，然后使用该文件<code>yolov3.conv.81</code>训练。</li><li>复杂物体应该使用复杂的神经网络来训练</li><li>你可以修改<code>anchors</code>的大小。略。</li></ul><h2 id="如何计算mAP"><a href="#如何计算mAP" class="headerlink" title="如何计算mAP"></a>如何计算mAP</h2><p><a href="https://github.com/AlexeyAB/darknet#how-to-calculate-map-on-pascalvoc-2007" target="_blank" rel="noopener">How to calculate mAP on PascalVOC 2007</a></p><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><h3 id="Open-Images数据集"><a href="#Open-Images数据集" class="headerlink" title="Open Images数据集"></a>Open Images数据集</h3><pre><code>wget https://pjreddie.com/media/files/yolov3-openimages.weights./darknet detector test cfg/openimages.data cfg/yolov3-openimages.cfg yolov3-openimages.weights</code></pre><h3 id="Yolo9000"><a href="#Yolo9000" class="headerlink" title="Yolo9000"></a>Yolo9000</h3><p>能够检测多达<code>9000</code>个物体，需要<code>4G</code>显存<br><a href="https://github.com/AlexeyAB/darknet#using-yolo9000" target="_blank" rel="noopener">Using Yolo9000</a></p><h3 id="如何以库的形式调用YOLO"><a href="#如何以库的形式调用YOLO" class="headerlink" title="如何以库的形式调用YOLO"></a>如何以库的形式调用YOLO</h3><p><a href="https://github.com/AlexeyAB/darknet#how-to-use-yolo-as-dll-and-so-libraries" target="_blank" rel="noopener">How to use Yolo as DLL and SO libraries</a></p><h1 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h1><ul><li>类似教程：<br><a href="https://chtseng.wordpress.com/2018/09/01/%E5%BB%BA%E7%AB%8B%E8%87%AA%E5%B7%B1%E7%9A%84yolo%E8%BE%A8%E8%AD%98%E6%A8%A1%E5%9E%8B-%E4%BB%A5%E6%9F%91%E6%A9%98%E8%BE%A8%E8%AD%98%E7%82%BA%E4%BE%8B/" target="_blank" rel="noopener">建立自己的YOLO辨識模型 – 以柑橘辨識為例</a><br><a href="https://medium.com/%E9%9B%9E%E9%9B%9E%E8%88%87%E5%85%94%E5%85%94%E7%9A%84%E5%B7%A5%E7%A8%8B%E4%B8%96%E7%95%8C/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-ml-note-yolo-%E5%88%A9%E7%94%A8%E5%BD%B1%E5%83%8F%E8%BE%A8%E8%AD%98%E5%81%9A%E7%89%A9%E4%BB%B6%E5%81%B5%E6%B8%AC-object-detection-%E7%9A%84%E6%8A%80%E8%A1%93-3ad34a4cac70" target="_blank" rel="noopener">YOLO!!!如何簡單使用YOLO訓練出自己的物件偵測!!! (Windows+Anaconda)</a><br><a href="https://blog.liebes.top/2018/01/30/ML-darknet-1/index.html" target="_blank" rel="noopener">机器学习之 darknet YOLO 训练 VOC 数据集</a></li></ul><ul><li>参考文献：<br><a href="https://pjreddie.com/darknet/yolo/" target="_blank" rel="noopener">YOLO: Real-Time Object Detection</a><br><a href="https://github.com/AlexeyAB/darknet" target="_blank" rel="noopener">Yolo-v3 and Yolo-v2 for Windows and Linux</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> dl </category>
          
      </categories>
      
      
        <tags>
            
            <tag> dl </tag>
            
            <tag> yolo </tag>
            
            <tag> darknet </tag>
            
            <tag> detection </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>正确选择ML算法</title>
      <link href="/ml/mlcomparsion/"/>
      <url>/ml/mlcomparsion/</url>
      
        <content type="html"><![CDATA[<p>本文教你如何选择合适自己的机器学习算法。</p><p><img src="20.png" alt></p><h1 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h1><h2 id="逻辑斯蒂回归（Logistic-regression）"><a href="#逻辑斯蒂回归（Logistic-regression）" class="headerlink" title="逻辑斯蒂回归（Logistic regression）"></a>逻辑斯蒂回归（Logistic regression）</h2><blockquote><p>属于判别式模型，有很多正则化模型的方法（L0，L1，L2，etc），而且你不必像在用朴素贝叶斯那样担心你的特征是否相关。与决策树与SVM机相比，你还会得到一个不错的概率解释，你甚至可以轻松地利用新数据来更新模型（使用在线梯度下降算法，onlinegradientdescent）。如果你需要一个概率架构（比如，简单地调节分类阈值，指明不确定性，或者是要获得置信区间），或者你希望以后将更多的训练数据快速整合到模型中去，那么使用它吧。<br><img src="1.png" alt></p><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ul><li>实现简单，广泛的应用于工业问题上；</li><li>分类时计算量非常小，速度很快，存储资源低；</li><li>便利的观测样本概率分数；</li><li>对逻辑回归而言，多重共线性并不是问题，它可以结合L2正则化来解决该问题；</li></ul><p><strong>表现最好：当特征没有相关性，最终分类结果是线性的，且特征维度远小于数据量的时候效果好。</strong></p><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul><li>当特征空间很大时，逻辑回归的性能不是很好；</li><li>容易欠拟合，一般准确度不太高</li><li>不能很好地处理大量多类特征或变量；</li><li>只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分；</li><li>对于非线性特征，需要进行转换；</li></ul><p><strong>表现最差：当特征相关性比较强时，表现会很差。</strong></p><h3 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h3><p><a href="http://www.cnblogs.com/chenice/p/7202045.html" target="_blank" rel="noopener">机器学习之良/恶性乳腺癌肿瘤预测</a><br><a href="https://zhuanlan.zhihu.com/p/25327755" target="_blank" rel="noopener">机器学习算法集锦：从贝叶斯到深度学习及各自优缺点</a></p></blockquote><h2 id="朴素贝叶斯（Naive-Bayes）"><a href="#朴素贝叶斯（Naive-Bayes）" class="headerlink" title="朴素贝叶斯（Naive Bayes）"></a>朴素贝叶斯（Naive Bayes）</h2><blockquote><p>朴素贝叶斯属于生成式模型（关于生成模型和判别式模型，主要还是在于是否是要求联合分布），非常简单，你只是做了一堆计数。如果注有条件独立性假设（一个比较严格的条件），朴素贝叶斯分类器的收敛速度将快于判别模型，如逻辑回归，所以你只需要较少的训练数据即可。即使NB条件独立假设不成立，NB分类器在实践中仍然表现的很出色。它的主要缺点是它不能学习特征间的相互作用，用mRMR中R来讲，就是特征冗余。引用一个比较经典的例子，比如，虽然你喜欢BradPitt和TomCruise的电影，但是它不能学习出你不喜欢他们在一起演的电影。<br><img src="2.png" alt></p><h3 id="优点-1"><a href="#优点-1" class="headerlink" title="优点"></a>优点</h3><ul><li>朴素贝叶斯模型发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率。</li><li>对小规模的数据表现很好，能个处理多分类任务，适合增量式训练。</li><li>对缺失数据不太敏感，算法也比较简单，常用于文本分类。</li></ul><h3 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h3><ul><li>需要计算先验概率；</li><li>分类决策存在错误率；</li><li>对输入数据的表达形式很敏感。</li></ul><h3 id="链接-1"><a href="#链接-1" class="headerlink" title="链接"></a>链接</h3><p><a href="https://scikit-learn.org/stable/modules/naive_bayes.html" target="_blank" rel="noopener">朴素贝叶斯的sklearn实现</a></p></blockquote><h2 id="最近邻算法（KNN）"><a href="#最近邻算法（KNN）" class="headerlink" title="最近邻算法（KNN）"></a>最近邻算法（KNN）</h2><blockquote><p>其主要过程为：</p><ol><li>计算训练样本和测试样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）；</li><li>对上面所有的距离值进行排序；</li><li>选前k个最小距离的样本；</li><li>根据这k个样本的标签进行投票，得到最后的分类类别；<br>如何选择一个最佳的K值，这取决于数据。一般情况下，在分类时较大的K值能够减小噪声的影响。但会使类别之间的界限变得模糊。一个较好的K值可通过各种启发式技术来获取，比如，交叉验证。另外噪声和非相关性特征向量的存在会使K近邻算法的准确性减小。<br>近邻算法具有较强的一致性结果。随着数据趋于无限，算法保证错误率不会超过贝叶斯算法错误率的两倍。对于一些好的K值，K近邻保证错误率不会超过贝叶斯理论误差率。<br><img src="3.png" alt></li></ol><h3 id="优点-2"><a href="#优点-2" class="headerlink" title="优点"></a>优点</h3><ul><li>理论成熟，思想简单，既可以用来做分类也可以用来做回归；</li><li>可用于非线性分类；</li><li>训练时间复杂度为O(n)；</li><li>对数据没有假设，准确度高，对outlier不敏感</li></ul><h3 id="缺点-2"><a href="#缺点-2" class="headerlink" title="缺点"></a>缺点</h3><ul><li>计算量大；</li><li>样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）；</li><li>需要大量的内存；</li></ul></blockquote><h2 id="支持向量机（SVM）"><a href="#支持向量机（SVM）" class="headerlink" title="支持向量机（SVM）"></a>支持向量机（SVM）</h2><blockquote><p>支持向量机使用一个名为核函数的技巧，来将非线性问题变换为线性问题，其本质是计算两个观测数据的距离。支持向量机算法所寻找的是能够最大化样本间隔的决策边界，因此又被称为大间距分类器。<br>举例来说，使用线性核函数的支持向量机类似于逻辑回归，但更具稳健性。因而在实践中，支持向量机最大用处是用非线性核函数来对非线性决策边界进行建模。<br><img src="4.png" alt></p><h3 id="优点-3"><a href="#优点-3" class="headerlink" title="优点"></a>优点</h3><ul><li>支持向量机能对非线性决策边界建模，又有许多可选的核函数。在面对过拟合时，支持向量机有着极强的稳健性，尤其是在高维空间中。</li></ul><h3 id="缺点-3"><a href="#缺点-3" class="headerlink" title="缺点"></a>缺点</h3><ul><li>不过，支持向量机是内存密集型算法，选择正确的核函数就需要相当的j技巧，不太适用较大的数据集。在当前的业界应用中，随机森林的表现往往要优于支持向量机。</li></ul><h3 id="链接-2"><a href="#链接-2" class="headerlink" title="链接"></a>链接</h3><p><a href="https://scikit-learn.org/stable/modules/svm.html#classification" target="_blank" rel="noopener">svm的sklearn实现</a></p></blockquote><h2 id="决策树（Decision-tree）"><a href="#决策树（Decision-tree）" class="headerlink" title="决策树（Decision tree）"></a>决策树（Decision tree）</h2><blockquote><p>易于解释。它可以毫无压力地处理特征间的交互关系并且是非参数化的，因此你不必担心异常值或者数据是否线性可分（举个例子，决策树能轻松处理好类别A在某个特征维度x的末端，类别B在中间，然后类别A又出现在特征维度x前端的情况）。它的缺点之一就是不支持在线学习，于是在新样本到来后，决策树需要全部重建。另一个缺点就是容易出现过拟合，但这也就是诸如随机森林RF（或提升树boostedtree）之类的集成方法的切入点。另外，随机森林经常是很多分类问题的赢家（通常比支持向量机好上那么一丁点），它训练快速并且可调，同时你无须担心要像支持向量机那样调一大堆参数，所以在以前都一直很受欢迎。<br>决策树中很重要的一点就是选择一个属性进行分枝，因此要注意一下信息增益的计算公式，并深入理解它。<br><img src="5.png" alt></p><h3 id="优点-4"><a href="#优点-4" class="headerlink" title="优点"></a>优点</h3><ul><li>计算简单，易于理解，可解释性强；</li><li>比较适合处理有缺失属性的样本；</li><li>能够处理不相关的特征；</li><li>在相对短的时间内能够对大型数据源做出可行且效果良好的结果。</li></ul><h3 id="缺点-4"><a href="#缺点-4" class="headerlink" title="缺点"></a>缺点</h3><ul><li>容易发生过拟合（随机森林可以很大程度上减少过拟合）；</li><li>忽略了数据之间的相关性；</li><li>对于那些各类别样本数量不一致的数据，在决策树当中,信息增益的结果偏向于那些具有更多数值的特征（只要是使用了信息增益，都有这个缺点，如RF）。</li></ul></blockquote><h2 id="随机森林（Random-Forests）"><a href="#随机森林（Random-Forests）" class="headerlink" title="随机森林（Random Forests）"></a>随机森林（Random Forests）</h2><blockquote><p>集成的方法，如随机森林（RF）或梯度提升树（GBM），则能结合许多独立训练树的预测。我们在这里不会详述其中的机制，但在实践中，随机森林一般都有很出色的表现，梯度提升树则较难调参，但往往能有更高的性能上限。<br><img src="6.png" alt></p><h3 id="优点-5"><a href="#优点-5" class="headerlink" title="优点"></a>优点</h3><ul><li>在数据集上表现良好，两个随机性的引入，使得随机森林不容易陷入过拟合。</li><li>在当前的很多数据集上，相对其他算法有着很大的优势，两个随机性的引入，使得随机森林具有很好的抗噪声能力。</li><li>它能够处理很高维度（feature很多）的数据，并且不用做特征选择，对数据集的适应能力强：既能处理离散型数据，也能处理连续型数据，数据集无需规范化。</li><li>在创建随机森林的时候，对generlization error使用的是无偏估计。</li><li>训练速度快，可以得到变量重要性排序。</li><li>在训练过程中，能够检测到feature间的互相影响。</li><li>容易做成并行化方法。</li><li>实现比较简单</li></ul><h3 id="缺点-5"><a href="#缺点-5" class="headerlink" title="缺点"></a>缺点</h3><ul><li>由于无约束，单棵树容易过拟合，这是因为单棵树可保留分支直至记住训练的数据。不够，集成方法可以弱化这一缺点。</li></ul><h3 id="链接-3"><a href="#链接-3" class="headerlink" title="链接"></a>链接</h3><p><a href="https://scikit-learn.org/stable/modules/ensemble.html#random-forests" target="_blank" rel="noopener">随机森林的实现</a></p></blockquote><h2 id="深度学习（DL）"><a href="#深度学习（DL）" class="headerlink" title="深度学习（DL）"></a>深度学习（DL）</h2><blockquote><p>深度学习是指能够学习极端复杂模式的多层神经网络。它们在输入层和输出层之间使用隐藏层来对数据的中间表征建模，这一点是其他算法很难做到的。<br>深度学习还有几个重要的机制，如卷积、漏失等，这使该算法可以有效学习高维数据。然而，相对于其他算法，深度学习需要更多的数据来进行训练，因为该模型需要估算更大数量级的参数。<br><img src="7.png" alt></p><h3 id="优点-6"><a href="#优点-6" class="headerlink" title="优点"></a>优点</h3><p>深度学习是当前特定领域的最先进技术，如计算机视觉与语音识别。深度神经网络在图像、音频和文本数据上表现优异，也很容易通过反向传播算法来更新数据模型。它们的架构（即层级的数量和结构）能适用于多种问题，同时隐藏层还能降低算法对特征工程的依赖。</p><h3 id="缺点-6"><a href="#缺点-6" class="headerlink" title="缺点"></a>缺点</h3><p>深度学习算法往往不适合用于通用目的，因为它们需要大量的数据。事实上，对于经典的机器学习问题，深度学习的表现并不比集成方法好。此外，由于训练所需的密集型计算，它们需要更多的专门知识才能进行调参（如设定架构与超参数）。</p><h3 id="链接-4"><a href="#链接-4" class="headerlink" title="链接"></a>链接</h3><p><a href="https://keras.io/" target="_blank" rel="noopener">Keras深度学习</a></p></blockquote><hr><h1 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h1><p><img src="11.png" alt></p><h2 id="K均值（K-means）"><a href="#K均值（K-means）" class="headerlink" title="K均值（K-means）"></a>K均值（K-means）</h2><blockquote><p>K 均值是基于样本点间的几何距离来度量聚类的通用目的算法。由于集群围绕在聚类中心，结果会接近于球状并具有相似的大小。<br>我们之所以推荐该算法给初学者，是因为它不仅足够简单，而且足够灵活，对于大多数问题都能给出合理的结果。<br><img src="8.png" alt></p><h3 id="优点-7"><a href="#优点-7" class="headerlink" title="优点"></a>优点</h3><ul><li>K均值是最为流行的聚类算法，因为它足够快速、足够简单，如果你的预处理数据和特征工程都做得十分有效，那它将具备令人惊叹的灵活性。</li></ul><h3 id="缺点-7"><a href="#缺点-7" class="headerlink" title="缺点"></a>缺点</h3><ul><li>该算法需要指定集群的数量，而 K 值的选择通常都不是那么容易确定的。另外，如果训练数据中的真实集群并不是类球状的，那么 K 均值聚类会得出一些比较差的集群。</li></ul><h3 id="链接-5"><a href="#链接-5" class="headerlink" title="链接"></a>链接</h3><p><a href="https://scikit-learn.org/stable/modules/clustering.html#k-means" target="_blank" rel="noopener">K均值的sklearn实现</a></p></blockquote><h2 id="仿射传播（Affinity-Propagation）"><a href="#仿射传播（Affinity-Propagation）" class="headerlink" title="仿射传播（Affinity Propagation）"></a>仿射传播（Affinity Propagation）</h2><blockquote><p>仿射传播是一种相对较新的聚类算法，它基于两个样本点之间的图形距离来确定集群，其结果倾向于更小且大小不等的集群。<br><img src="9.png" alt></p><h3 id="优点-8"><a href="#优点-8" class="headerlink" title="优点"></a>优点</h3><ul><li>仿射传播不需要指出明确的集群数量，但需要指定“sample preference”和“damping”等超参数。</li></ul><h3 id="缺点-8"><a href="#缺点-8" class="headerlink" title="缺点"></a>缺点</h3><ul><li>仿射传播的主要缺点是训练速度较慢，且需要大量内存，因而难于扩展到大数据集。此外，该算法同样在假定潜在的集群要接近于球状。</li></ul><h3 id="链接-6"><a href="#链接-6" class="headerlink" title="链接"></a>链接</h3><p><a href="https://scikit-learn.org/stable/modules/clustering.html#affinity-propagation" target="_blank" rel="noopener">仿射传播的sklearn实现</a></p></blockquote><h2 id="层次聚类（Hierarchical-clustering）"><a href="#层次聚类（Hierarchical-clustering）" class="headerlink" title="层次聚类（Hierarchical clustering）"></a>层次聚类（Hierarchical clustering）</h2><blockquote><p>其算法基于以下概念来实现：</p><ol><li>每一个集群都从一个数据点开始；</li><li>每一个集群都可基于相同的标准进行合并；</li><li>重复这一过程，直至你仅剩下一个集群，这就获得了集群的层次结构。<br><img src="10.png" alt></li></ol><h3 id="优点-9"><a href="#优点-9" class="headerlink" title="优点"></a>优点</h3><ul><li>层次聚类的最主要优点，是集群不再假定为类球形。此外，它可以很容易扩展到大数据集。</li></ul><h3 id="缺点-9"><a href="#缺点-9" class="headerlink" title="缺点"></a>缺点</h3><ul><li>类似于 K 均值，该算法需要选定集群的数量，即算法完成后所要保留的层次。</li></ul></blockquote><h2 id="DBSCAN"><a href="#DBSCAN" class="headerlink" title="DBSCAN"></a>DBSCAN</h2><blockquote><p>DBSCAN 是一种基于密度的聚类算法，它将样本点的密集区域组成集群；其最新进展是HDBSCAN，它允许集群的密度可变。<br><img src="12.png" alt></p><h3 id="优点-10"><a href="#优点-10" class="headerlink" title="优点"></a>优点</h3><ul><li>DBSCAN 不需要假定类球形集群，其性能可以扩展。此外，它不需要每个点都被分配到集群中，这就降低了集群的噪音。</li></ul><h3 id="缺点-10"><a href="#缺点-10" class="headerlink" title="缺点"></a>缺点</h3><ul><li>用户必须要调整“epsilon”和“min_sample”这两个超参数来定义集群密度。DBSCAN 对此非常敏感。</li></ul><h3 id="链接-7"><a href="#链接-7" class="headerlink" title="链接"></a>链接</h3><p><a href="https://scikit-learn.org/stable/modules/clustering.html#dbscan" target="_blank" rel="noopener">DBSCAN的sklearn实现</a></p></blockquote><hr><h1 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h1><h2 id="线性回归（Linear-regression）"><a href="#线性回归（Linear-regression）" class="headerlink" title="线性回归（Linear regression）"></a>线性回归（Linear regression）</h2><blockquote><p>线性回归是回归任务最常用的算法。它最简的形式，是用一个连续的超平面来拟合数据集（比如，当你仅有两个变量时就用一条直线）。如果数据集内的变量存在线性关系，拟合程度就相当高。<br>在实践中，简单线性回归通常会被其正则化形式（LASSO、Ridge 及弹性网络）所取代。正则化是对过多回归系数所采取的一种避免过拟合的惩罚技巧，同时，惩罚的强度需要被平衡好。<br><img src="13.png" alt></p><h3 id="优点-11"><a href="#优点-11" class="headerlink" title="优点"></a>优点</h3><ul><li>线性回归的理解和解释都非常直观，还能通过正则化来避免过拟合。此外，线性模型很容易通过随机梯度下降来更新数据模型。</li></ul><h3 id="缺点-11"><a href="#缺点-11" class="headerlink" title="缺点"></a>缺点</h3><ul><li>线性回归在处理非线性关系时非常糟糕，在识别复杂的模式上也不够灵活，而添加正确的相互作用项或多项式又极为棘手且耗时。</li></ul><h3 id="链接-8"><a href="#链接-8" class="headerlink" title="链接"></a>链接</h3><p><a href="https://scikit-learn.org/stable/modules/linear_model.html" target="_blank" rel="noopener">sklearn的实现方法</a></p></blockquote><hr><h1 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h1><blockquote><p><strong>特征选取</strong>与<strong>特征提取</strong>的关键区别在于：特征选取是从原特征集中选取一个子特征集，而特称提取则是在原特征集的基础上重新构造出一些(一个或多个)全新的特征。</p></blockquote><h2 id="方差阈值（Variance-Threshold）"><a href="#方差阈值（Variance-Threshold）" class="headerlink" title="方差阈值（Variance Threshold）"></a>方差阈值（Variance Threshold）</h2><blockquote><ul><li>方差阈值会摒弃掉观测样本那些观测值改变较小的特征(即，它们的方差小于某个设定的阈值)。这样的特征的价值极小。</li><li>举例来说，如果你有一份公共健康数据，其中96%的人都是35岁的男性，那么去掉“年龄”和“性别”的特征也不会损失重要信息。</li><li>由于方差阈值依赖于特征值的数量级，你应该对特征值先做归一化处理。<br><img src="14.png" alt></li></ul><h3 id="优点-12"><a href="#优点-12" class="headerlink" title="优点"></a>优点</h3><p>使用方差阈值方式进行数据降维只需一个非常可靠的直觉：特征值不怎么改变的特征，不会带来什么有用的信息。这是在你建模初期进行数据降维相对安全的一种方式。</p><h3 id="缺点-12"><a href="#缺点-12" class="headerlink" title="缺点"></a>缺点</h3><p>如果你正在解决的问题并不需要进行数据降维，即便使用了方差阈值也几乎没有什么作用。此外，你需要手工设置、调整方差阈值，这个过程相当具有技术含量。我们建议从一个保守(也就是，较低)的阈值开始。</p><h3 id="链接-9"><a href="#链接-9" class="headerlink" title="链接"></a>链接</h3><p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html" target="_blank" rel="noopener">方差阈值的sklearn实现</a></p></blockquote><h2 id="相关性阈值"><a href="#相关性阈值" class="headerlink" title="相关性阈值"></a>相关性阈值</h2><blockquote><ul><li>相关性阈值会去掉那些高度相关的特征(亦即，这些特征的特征值变化与其他特征非常相似)。它们提供的是冗余信息。</li><li>举例来说，如果你有一个房地产数据，其中两个特征分别是“房屋面积（单位：平方英尺）”和“房屋面积（单位：平方米）”，那么，你就可以去掉其中的任何一个（这非常安全，也不会给你的模型带来任何负面影响）。</li><li>问题是，你该去掉哪一个特征呢？首先，你应该计算所有特征对的相关系数。而后，如果某个特征对的相关系数大于设定的阈值，那你就可以去掉其中平均绝对相关系数较大的那一个。<br><img src="15.png" alt></li></ul><h3 id="优点-13"><a href="#优点-13" class="headerlink" title="优点"></a>优点</h3><p>使用相关性阈值同样只需一个可靠的直觉：相似的特征提供了冗余的信息。对于某些含有强相关性特征较多的数据集，有些算法的稳健性并不好，因此，去掉它们可以提升整个模型的性能(计算速度、模型准确度、模型稳健性，等等)。</p><h3 id="缺点-13"><a href="#缺点-13" class="headerlink" title="缺点"></a>缺点</h3><p>同样，你还是必须手动去设置、调整相关性阈值，这同样是个棘手且复杂的过程。此外，如果你设置的阈值过低，那么你将会丢失掉一些有用的信息。无论在什么时候，我们都更倾向于使用那些内置了特征选取的算法。对于没有内置特征提取的算法，主成分分析是一个很好的备用方案。</p><h3 id="链接-10"><a href="#链接-10" class="headerlink" title="链接"></a>链接</h3><p><a href="https://gist.github.com/Swarchal/881976176aaeb21e8e8df486903e99d6" target="_blank" rel="noopener">相关性阈值的py实现</a></p></blockquote><h2 id="遗传算法（GA）"><a href="#遗传算法（GA）" class="headerlink" title="遗传算法（GA）"></a>遗传算法（GA）</h2><blockquote><p>遗传算法是可用于不同任务的一大类算法的统称。它们受进化生物学与自然选择的启发，结合变异与交叉，在解空间内进行高效的遍历搜索。<br>在机器学习领域，遗传算法主要有两大用处。</p><ul><li>用于最优化，比如去找神经网络的最佳权重。</li><li>是用于监督式特征提取。<br>这一用例中，“基因”表示单个特征，同时“有机体”表示候选特征集。“种群体”内的每一个有机体都会基于其适应性进行评分，正如在测试数据集上进行模型性能测试。最能适应环境的有机体将会生存下来，并不断繁衍，一直迭代，直至最终收敛于某个最优的解决方案。<br><img src="16.png" alt></li></ul><h3 id="优点-14"><a href="#优点-14" class="headerlink" title="优点"></a>优点</h3><p>在穷举搜索不可行的情况下，对高维数据集使用遗传算法会相当有效。当你的算法需要预处理数据却没有内置的特征选取机制(如最近邻分类算法），而你又必须保留最原始的特征(也就是不能用任何主成分分析算法)，遗传算法就成了你最好的选择。这一情况在要求透明、可解释方案的商业环境下时有发生。</p><h3 id="缺点-14"><a href="#缺点-14" class="headerlink" title="缺点"></a>缺点</h3><p>遗传算法为你解决方案的实施带来了更高的复杂度，而多数情况下它们都是不必要的麻烦。如果可能的话，主成分分析或其它内置特征选取的算法将会更加高效和简洁。</p><h3 id="链接-11"><a href="#链接-11" class="headerlink" title="链接"></a>链接</h3><p><a href="https://pypi.org/project/deap/" target="_blank" rel="noopener">遗传算法的py实现</a></p></blockquote><h2 id="主成分分析（PCA）"><a href="#主成分分析（PCA）" class="headerlink" title="主成分分析（PCA）"></a>主成分分析（PCA）</h2><blockquote><p>主成分分析是一个非监督式算法，它用来创造原始特征的线性组合。新创造出来的特征他们之间都是正交的，也就是没有关联性。具体来说，这些新特征是按它们本身变化程度的大小来进行排列的。第一个主成分代表了你的数据集中变化最为剧烈的特征，第二个主成分代表了变化程度排在第二位的特征，以此类推。<br>因此，你可以通过限制使用主成分的个数来达到数据降维的目的。例如，你可以仅采用能使累积可解释方差为90%的主成分数量。<br>你需要在使用主成分分析之前，对数据进行归一化处理。否则，原始数据中特征值数量级最大的那个特征将会主导你新创造出来的主成分特征。<br><img src="17.png" alt></p><h3 id="优点-15"><a href="#优点-15" class="headerlink" title="优点"></a>优点</h3><p>主成分分析是一项多用途技术，实用效果非常好。它部署起来快速、简单，也就是说，你可以很方便地测试算法性能，无论使用还是不使用主成分分析。此外，主成分分析还有好几种变体和扩展(如：核主成分分析(kernel PCA)，稀疏主成分分析(sparse PCA))，用以解决特定的问题。</p><h3 id="缺点-15"><a href="#缺点-15" class="headerlink" title="缺点"></a>缺点</h3><p>新创造出来的主成分并不具备可解释性，因而在某些情况下，新特征与应用实际场景之间很难建立起联系。此外，你仍然需要手动设置、调整累积可解释方差的阈值。</p><h3 id="链接-12"><a href="#链接-12" class="headerlink" title="链接"></a>链接</h3><p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" target="_blank" rel="noopener">PCA的sklearn实现</a></p></blockquote><h2 id="线性判别分析（LDA）"><a href="#线性判别分析（LDA）" class="headerlink" title="线性判别分析（LDA）"></a>线性判别分析（LDA）</h2><blockquote><ul><li>线性判别分析不是隐含狄利克雷分布，它同样用来构造原始特征集的线性组合。但与主成分分析不同，线性判别分析不会最大化可解释方差，而是最大化类别间的分离程度。</li><li>因此，线性判别分析是一种监督式学习方式，它必须使用有标记的数据集。那么，线性判别分析与主成分分析，到底哪种方法更好呢？这要视具体的情况而定，“没有免费的午餐”原理在这里同样适用。</li><li>线性判别分析同样依赖于特征值的数量级，你同样需要先对特征值做归一化处理。<br><img src="18.png" alt></li></ul><h3 id="优点-16"><a href="#优点-16" class="headerlink" title="优点"></a>优点</h3><p>线性判别分析是一种监督式学习，基于这种方式获取到的特征可以(但并不总是能)提升模型性能。此外，线性判别分析还有一些变体(如二次线性判别分析)，可用来解决特定的问题。</p><h3 id="缺点-16"><a href="#缺点-16" class="headerlink" title="缺点"></a>缺点</h3><p>与主成分分析一样，新创造出来的特征不具有可解释性。而且，你同样要手动设置、调整需要保留的特征数量。线性判别分析需要已经标记好的数据，因此，这也让它更加接地气儿。</p><h3 id="链接-13"><a href="#链接-13" class="headerlink" title="链接"></a>链接</h3><p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis" target="_blank" rel="noopener">LDA的sklearn实现</a><br><a href="https://www.choyang.me/zh/post/liner-discriminant-analysis/" target="_blank" rel="noopener">线性判别分析LDA</a></p></blockquote><h2 id="自编码机"><a href="#自编码机" class="headerlink" title="自编码机"></a>自编码机</h2><blockquote><ul><li>自编码机是一种人工神经网络，它是用来重新构建原始输入的。例如，图像自编码机是训练来重新表征原始数据的，而非用以区分图片里面的小猫、小狗。</li><li>但这有用吗？这里的关键，是在隐含层搭建比输入层和输出层更少数量的神经元。这样，隐含层就会不断学习如何用更少的特征来表征原始图像。</li><li>因为是用输入图像来作为目标输出，自编码机被视为无监督学习。它们可被直接使用（如：图像压缩）或按顺序堆叠使用（如：深度学习）。<br><img src="19.png" alt></li></ul><h3 id="优点-17"><a href="#优点-17" class="headerlink" title="优点"></a>优点</h3><p>自编码机是人工神经网络中的一种，这表示它们对某些特定类型的数据表现会非常好，比如图像和语音数据。</p><h3 id="缺点-17"><a href="#缺点-17" class="headerlink" title="缺点"></a>缺点</h3><p>自编码机是一种人工神经网络。这就是说，它们的优化需要更多的数据来进行训练。它们并不能作为一般意义上的数据降维算法来用。</p><h3 id="链接-14"><a href="#链接-14" class="headerlink" title="链接"></a>链接</h3><p><a href="https://ynuwm.github.io/2017/05/14/%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8AutoEncoder/" target="_blank" rel="noopener">自编码器AutoEncoder</a></p></blockquote><hr><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><blockquote><ol><li><a href="https://zhuanlan.zhihu.com/p/31711537" target="_blank" rel="noopener">机器学习算法汇总及选择</a></li></ol></blockquote>]]></content>
      
      
      <categories>
          
          <category> ml </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ml </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux-入门速成</title>
      <link href="/linux/linux-quickstart/"/>
      <url>/linux/linux-quickstart/</url>
      
        <content type="html"><![CDATA[<p>本文实用地介绍了 Linux 指令的用法，和 Linux 系统的相关知识。</p><p>（大多内容选自 <a href="http://billie66.github.io/TLCL/book/index.html" target="_blank" rel="noopener">The Linux Command Line 的中文版</a> ）</p><p><img src="1.png" alt></p><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><h2 id="一个故事"><a href="#一个故事" class="headerlink" title="一个故事"></a>一个故事</h2><blockquote><p>我想给大家讲个故事。<br>故事内容不是 Linus Torvalds 在1991年怎样写了 Linux 内核的第一个版本， 因为这些内容你可以在许多 Linux 书籍中读到。我也不是来告诉你，更早之前，Richard Stallman 是如何开始 GNU 项目，设计了一个免费的类Unix 的操作系统。那也是一个很有意义的故事， 但大多数 Linux 书籍也讲到了它。<br>我想告诉大家一个你如何才能夺回计算机管理权的故事。<br>在20世纪70年代末，我刚开始和计算机打交道时，正进行着一场革命，那时的我还是一名大学生。 微处理器的发明，使普通老百姓（就如你和我）真正拥有一台计算机成为可能。今天， 人们难以想象，只有大企业和强大的政府才能够拥有计算机的世界，是怎样的一个世界。 简单说，你做不了多少事情。<br>今天，世界已经截然不同了。计算机遍布各个领域，从小手表到大型数据中心，及大小介于它们之间的每件东西。 除了随处可见的计算机之外，我们还有一个无处不在的连接所有计算机的网络。这已经开创了一个奇妙的， 个人授权和创作自由的新时代，但是在过去的二三十年里，正在发生另一些事情。一个大公司不断地把它的 管理权强加到世界上绝大多数的计算机上，并且决定你对计算机的操作权力。幸运地是，来自世界各地的人们， 正积极努力地做些事情来改变这种境况。通过编写自己的软件，他们一直在为维护电脑的管理权而战斗着。 他们建设着 Linux。<br>一提到 Linux，许多人都会说到“自由”，但我不认为他们都知道“自由”的真正涵义。“自由”是一种权力， 它决定你的计算机能做什么，同时能够拥有这种“自由”的唯一方式就是知道计算机正在做什么。 “自由”是指一台没有任何秘密的计算机，你可以从它那里了解一切，只要你用心的去寻找。</p></blockquote><h2 id="为什么使用命令行"><a href="#为什么使用命令行" class="headerlink" title="为什么使用命令行"></a>为什么使用命令行</h2><blockquote><p>你是否注意到，在电影中一个“超级黑客”坐在电脑前，从不摸一下鼠标， 就能够在30秒内侵入到超安全的军用计算机中。这是因为电影制片人意识到， 作为人类，本能地知道让计算机圆满完成工作的唯一途径，是用键盘来操纵计算机。<br>现在，大多数的计算机用户只是熟悉图形用户界面（GUI），并且产品供应商和此领域的学者会灌输给用户这样的思想， 命令行界面（CLI）是过去使用的一种很恐怖的东西。这就很不幸，因为一个好的命令行界面， 是用来和计算机进行交流沟通的非常有效的方式，正像人类社会使用文字互通信息一样。人们说，“图形用户界面让简单的任务更容易完成， 而命令行界面使完成复杂的任务成为可能”，到现在这句话仍然很正确。<br>因为 Linux 是以 Unix 家族的操作系统为模型写成的，所以它分享了 Unix 丰富的命令行工具。 Unix 在20世纪80年代初显赫一时(虽然，开发它在更早之前），结果，在普遍地使用图形界面之前， 开发了一种广泛的命令行界面。事实上，很多人选择 Linux（而不是其他的系统，比如说 Windows NT）是因为其可以使“完成复杂的任务成为可能”的强大的命令行界面。</p></blockquote><h2 id="Linux-版本选择"><a href="#Linux-版本选择" class="headerlink" title="Linux 版本选择"></a>Linux 版本选择</h2><blockquote><ol><li>适合初学者的最佳Linux发行版：Linux Mint</li><li>老旧硬件的最佳Linux发行版：Ubuntu MATE</li><li>安全行业的最佳Linux发行版：Kali Linux</li><li>专属游戏的Linux发行版：Steam OS</li><li>用于编程的Linux发行版：Debian</li><li>美丽的Linux发行版：elementary OS</li><li>儿童专属Linux发行版：Ubermix</li><li>隐私和匿名的Linux发行版：Tails</li><li>Linux服务器发行版：CentOS</li><li>强大PC和笔记本电脑推荐的Linux发行版：Ubuntu</li></ol></blockquote><h1 id="学习-shell"><a href="#学习-shell" class="headerlink" title="学习 shell"></a>学习 shell</h1><h2 id="什么是-shell"><a href="#什么是-shell" class="headerlink" title="什么是 shell"></a>什么是 shell</h2><blockquote><p>一说到命令行，我们真正指的是 shell。shell 就是一个程序，它接受从键盘输入的命令， 然后把命令传递给操作系统去执行。几乎所有的 Linux 发行版都提供一个名为 bash 的 来自 GNU 项目的 shell 程序。“bash” 是 “Bourne Again SHell” 的首字母缩写， 所指的是这样一个事实，bash 是最初 Unix 上由 Steve Bourne 写成 shell 程序 sh 的增强版。</p><p><code>[root@izwz9biz2m4sd3tu00600pz ~]#</code>这叫做 shell 提示符，无论何时当 shell 准备好了去接受输入时，它就会出现。然而， 它可能会以各种各样的面孔显示，这则取决于不同的 Linux 发行版， 它通常包括你的用户名@主机名，紧接着当前工作目录（稍后会有更多介绍）和一个美元符号。<br>如果提示符的最后一个字符是“#”, 而不是“$”, 那么这个终端会话就有超级用户权限。 这意味着，我们或者是以 root 用户的身份登录，或者是我们选择的终端仿真器提供超级用户（管理员）权限。</p><p>下面尝试使用下面指令来体会下 Linux 操作系统吧 (≧∀≦)ゞ</p><p>显示时间日期</p></blockquote><pre><code>[root@izwz9biz2m4sd3tu00600pz ~]# dateFri Mar  1 10:44:55 CST 2019</code></pre><blockquote><p>显示当前月份的日历</p></blockquote><pre><code>[root@izwz9biz2m4sd3tu00600pz ~]# cal     March 2019Su Mo Tu We Th Fr Sa                1  2 3  4  5  6  7  8  910 11 12 13 14 15 1617 18 19 20 21 22 2324 25 26 27 28 29 3031</code></pre><blockquote><p>查看磁盘剩余空间</p></blockquote><pre><code>[root@izwz9biz2m4sd3tu00600pz ~]# dfFilesystem     1K-blocks    Used Available Use% Mounted on/dev/vda1       41151808 4606104  34432272  12% /devtmpfs          931516       0    931516   0% /devtmpfs             941860       0    941860   0% /dev/shmtmpfs             941860   16708    925152   2% /runtmpfs             941860       0    941860   0% /sys/fs/cgrouptmpfs             188376       0    188376   0% /run/user/0</code></pre><blockquote><p>显示空闲内存</p></blockquote><pre><code>[root@izwz9biz2m4sd3tu00600pz ~]# free              total        used        free      shared  buff/cache   availableMem:        1883724      128840      470492       16712     1284392     1537728Swap:             0           0           0</code></pre><blockquote><p>结束终端会话（退出）</p></blockquote><pre><code>[root@izwz9biz2m4sd3tu00600pz ~]# exit</code></pre><h2 id="文件系统中跳转"><a href="#文件系统中跳转" class="headerlink" title="文件系统中跳转"></a>文件系统中跳转</h2><blockquote><p>下面是常用的指令</p></blockquote><pre><code>pwd  # 显示当前的工作目录（print working directory的缩写）cd   # 切换目录ls   # 列出目录内容</code></pre><blockquote><p>当我们首次登录系统（或者启动终端仿真器会话）后，当前工作目录是我们的家目录。 每个用户都有他自己的家目录，当用户以普通用户的身份操控系统时，家目录是唯一允许用户写入文件的地方。</p></blockquote><h3 id="cd-指令"><a href="#cd-指令" class="headerlink" title="cd 指令"></a>cd 指令</h3><blockquote><p>其他有用的 cd 指令：</p></blockquote><pre><code>cd -        # 切换到先前的工作目录cd          # 切换到自己的家目录（等同于&quot;cd ~&quot;）cd ~karbo    # 切换到karbo的家目录</code></pre><h3 id="ls-指令"><a href="#ls-指令" class="headerlink" title="ls 指令"></a>ls 指令</h3><blockquote><p>其他有用的 ls 指令：</p></blockquote><pre><code>ls -a                          # 显示所有文件（包括以”.“开头的隐藏文件）ls -l                          # 显示文件细节（以长模式输出）ls -lt                         # 显示文件细节并按照时间顺序排序（时间正序指最近修改的文件是第一行）ls &lt;想查看的目录&gt;                # 显示 &lt;想查看的目录&gt; 下的文件ls &lt;想查看的目录1&gt; &lt;想查看的目录2&gt; # 显示 &lt;想查看的目录1&gt; 和 &lt;想查看的目录2&gt; 下的文件</code></pre><blockquote><p>ls 命令选项：</p></blockquote><div class="table-container"><table><thead><tr><th>选项</th><th>长选项</th><th>描述</th></tr></thead><tbody><tr><td>-a</td><td>—all</td><td>列出所有文件，甚至包括文件名以圆点开头的默认会被隐藏的隐藏文件。</td></tr><tr><td>-d</td><td>—directory</td><td>通常，如果指定了目录名，ls 命令会列出这个目录中的内容，而不是目录本身。 把这个选项与 -l 选项结合使用，可以看到所指定目录的详细信息，而不是目录中的内容。</td></tr><tr><td>-F</td><td>—classify</td><td>这个选项会在每个所列出的名字后面加上一个指示符。例如，如果名字是 目录名，则会加上一个’/‘字符。</td></tr><tr><td>-h</td><td>—human-readable</td><td>当以长格式列出时，以人们可读的格式，而不是以字节数来显示文件的大小。</td></tr><tr><td>-l</td><td></td><td>以长格式显示结果。</td></tr><tr><td>-r</td><td>—reverse</td><td>以相反的顺序来显示结果。通常，ls 命令的输出结果按照字母升序排列。</td></tr><tr><td>-S</td><td></td><td>命令输出结果按照文件大小来排序。</td></tr><tr><td>-t</td><td></td><td>按照修改时间来排序。</td></tr></tbody></table></div><blockquote><p>ls 长格式列表的字段（以<code>-rw-r--r-- 1 root root   32059 2007-04-03 11:05 oo-cd-cover.odf</code>为例）</p></blockquote><div class="table-container"><table><thead><tr><th>字段</th><th>含义</th></tr></thead><tbody><tr><td>-rw-r—r—</td><td>对于文件的访问权限。第一个字符指明文件类型。在不同类型之间， 开头的“－”说明是一个普通文件，“d”表明是一个目录。其后三个字符是文件所有者的 访问权限，再其后的三个字符是文件所属组中成员的访问权限，最后三个字符是其他所 有人的访问权限。这个字段的完整含义将在第十章讨论。</td></tr><tr><td>1</td><td>文件的硬链接数目。参考随后讨论的关于链接的内容。</td></tr><tr><td>root</td><td>文件所有者的用户名。</td></tr><tr><td>root</td><td>文件所属用户组的名字。</td></tr><tr><td>32059</td><td>以字节数表示的文件大小。</td></tr><tr><td>2007-04-03 11:05</td><td>上次修改文件的时间和日期。</td></tr><tr><td>oo-cd-cover.odf</td><td>文件名。</td></tr></tbody></table></div><blockquote><p>注意事项：</p><ol><li>文件名和命令名是<strong>大小写敏感</strong>的。文件名 “File1” 和 “file1” 是指两个不同的文件名。</li><li>Linux <strong>没有“文件扩展名”的概念</strong>，不像其它一些系统。可以用你喜欢的任何名字 来给文件起名。文件内容或用途由其它方法来决定。虽然类 Unix 的操作系统， 不用文件扩展名来决定文件的内容或用途，但是有些应用程序会。</li><li>虽然 Linux 支持长文件名，文件名可能包含空格，标点符号，但标点符号仅限 使用 “.”，“－”，下划线。最重要的是，不要在文件名中使用空格。如果你想表示词与词间的空格，<strong>用下划线字符来代替</strong>。有些时候，你会感激自己这样做。</li></ol></blockquote><h2 id="探究操作系统"><a href="#探究操作系统" class="headerlink" title="探究操作系统"></a>探究操作系统</h2><blockquote><p>既然我们已经知道了如何在文件系统中跳转，是时候开始 Linux 操作系统之旅了。然而在开始之前，我们先学习一些对研究 Linux 系统有帮助的命令。</p></blockquote><pre><code>file &lt;文件名&gt;  # 确定文件类型less &lt;文件名&gt;  # 浏览文件内容</code></pre><blockquote><p>在进行进一步探究之前，有必要介绍下命令的组成格式。<br>大多数命令看起来像这样：<code>command -options arguments</code><br>大多数命令使用的选项，是由一个中划线加上一个字符组成，例如，“-l”。<br>但是许多命令，包括来自于 GNU 项目的命令，也支持长选项，长选项由两个中划线加上一个字组成。当然， 许多命令也允许把多个短选项串在一起使用。以这个例子<code>ls -lt --reverse</code>为例，代表列出当前目录下的文件详细信息，并按照时间逆序排序。</p></blockquote><h3 id="file-指令"><a href="#file-指令" class="headerlink" title="file 指令"></a>file 指令</h3><blockquote><p>当调用 file 命令后，file 命令会打印出文件内容的简单描述。我们将用 file 命令来确定文件的类型。<br>例如我们想要知道 l2tp.log 文件是用来干什么的，就这样做：</p></blockquote><pre><code>[root@izwz9biz2m4sd3tu00600pz ~]# file l2tp.logl2tp.log: UTF-8 Unicode text, with escape sequences</code></pre><blockquote><p>有许多种类型的文件。事实上，在类 Unix 操作系统中比如说 Linux 中，有个普遍的观念就是“一切皆文件”。 随着课程的进行，我们将会明白这句话是多么的正确。</p></blockquote><h3 id="less-指令"><a href="#less-指令" class="headerlink" title="less 指令"></a>less 指令</h3><blockquote><p>less 命令是一个用来浏览文本文件的程序。纵观 Linux 系统，有许多人类可读的文本文件。less 程序为我们检查文本文件提供了方便。<br>一旦运行起来，less 程序允许你前后滚动文件。例如，要查看一个定义了系统中全部用户身份的文件，输入以下命令：</p></blockquote><pre><code>[root@izwz9biz2m4sd3tu00600pz ~]# less /etc/passwd</code></pre><blockquote><p>一旦 less 程序运行起来，我们就能浏览文件内容了。如果文件内容多于一页，那么我们可以按上、下键滚动文件。按下“q”键， 退出 less 程序。<br>下表列出了 less 程序最常使用的键盘命令。</p></blockquote><div class="table-container"><table><thead><tr><th>命令</th><th>行为</th></tr></thead><tbody><tr><td>Page UP or b</td><td>向上翻滚一页</td></tr><tr><td>Page Down or space</td><td>向下翻滚一页</td></tr><tr><td>UP Arrow</td><td>向上翻滚一行</td></tr><tr><td>Down Arrow</td><td>向下翻滚一行</td></tr><tr><td>G</td><td>移动到最后一行</td></tr><tr><td>1G or g</td><td>移动到开头一行</td></tr><tr><td>/charaters</td><td>向前查找指定的字符串</td></tr><tr><td>n</td><td>向前查找下一个出现的字符串，这个字符串是之前所指定查找的</td></tr><tr><td>h</td><td>显示帮助屏幕</td></tr><tr><td>q</td><td>退出 less 程序</td></tr></tbody></table></div><blockquote><p>less 程序是早期 Unix 程序 more 的改进版。“less” 这个名字，对习语 “less is more” 开了个玩笑， 这个习语是现代主义建筑师和设计者的座右铭。<br>less 属于”页面调度器”类程序，这些程序允许以逐页方式轻松浏览长文本文档。 more 程序只能向前翻页，而 less 程序允许前后翻页，此外还有很多其它的特性。</p></blockquote><h3 id="符号链接-软链接"><a href="#符号链接-软链接" class="headerlink" title="符号链接(软链接)"></a>符号链接(软链接)</h3><blockquote><p>在我们到处查看时，我们可能会看到一个目录，列出像这样的一条信息：<br><code>lrwxrwxrwx 1 root root 11 2007-08-11 07:34 libc.so.6 -&gt; libc-2.6.so</code><br>注意看，为何这条信息第一个字符是“l”，并且有两个文件名呢？ 这是一个特殊文件，叫做符号链接（也称为软链接或者 symlink ）。 在大多数“类 Unix” 系统中， 有可能一个文件被多个文件名所指向。虽然这种特性的意义并不明显，但它真的很有用。<br>描绘一下这样的情景：一个程序要求使用某个包含在名为“foo”文件中的共享资源，但是“foo”经常改变版本号。 这样，在文件名中包含版本号，会是一个好主意，因此管理员或者其它相关方，会知道安装了哪个“foo”版本。 这会导致另一个问题。如果我们更改了共享资源的名字，那么我们必须跟踪每个可能使用了 这个共享资源的程序，当每次这个资源的新版本被安装后，都要让使用了它的程序去寻找新的资源名。 这听起来很没趣。<br>这就是符号链接存在至今的原因。比方说，我们安装了文件 “foo” 的 2.6 版本，它的 文件名是 “foo-2.6”，然后创建了叫做 “foo” 的符号链接，这个符号链接指向 “foo-2.6”。 这意味着，当一个程序打开文件 “foo” 时，它实际上是打开文件 “foo-2.6”。 现在，每个人都很高兴。依赖于 “foo” 文件的程序能找到这个文件，并且我们能知道安装了哪个文件版本。 当升级到 “foo-2.7” 版本的时候，仅添加这个文件到文件系统中，删除符号链接 “foo”， 创建一个指向新版本的符号链接。这不仅解决了版本升级问题，而且还允许在系统中保存两个不同的文件版本。 假想 “foo-2.7” 有个错误（该死的开发者！），那我们得回到原来的版本。 一样的操作，我们只需要删除指向新版本的符号链接，然后创建指向旧版本的符号链接就可以了。</p></blockquote><h3 id="硬链接"><a href="#硬链接" class="headerlink" title="硬链接"></a>硬链接</h3><blockquote><p>讨论到链接问题，我们需要提一下，还有一种链接类型，叫做硬链接。硬链接同样允许文件有多个名字， 但是硬链接以不同的方法来创建多个文件名。</p></blockquote><h3 id="Linux-系统中的目录"><a href="#Linux-系统中的目录" class="headerlink" title="Linux 系统中的目录"></a>Linux 系统中的目录</h3><div class="table-container"><table><thead><tr><th>目录</th><th>评论</th></tr></thead><tbody><tr><td>/</td><td>根目录，万物起源。</td></tr><tr><td>/bin</td><td>包含系统启动和运行所必须的二进制程序。</td></tr><tr><td>/boot</td><td>包含 Linux 内核、初始 RAM 磁盘映像（用于启动时所需的驱动）和 启动加载程序。有趣的文件：/boot/grub/grub.conf or menu.lst， 被用来配置启动加载程序。/boot/vmlinuz，Linux 内核。</td></tr><tr><td>/dev</td><td>这是一个包含设备结点的特殊目录。“一切都是文件”，也适用于设备。 在这个目录里，内核维护着所有设备的列表。</td></tr><tr><td>/etc</td><td>这个目录包含所有系统层面的配置文件。它也包含一系列的 shell 脚本， 在系统启动时，这些脚本会开启每个系统服务。这个目录中的任何文件应该是可读的文本文件。有趣的文件：虽然/etc 目录中的任何文件都有趣，但这里只列出了一些我一直喜欢的文件：/etc/crontab， 定义自动运行的任务。/etc/fstab，包含存储设备的列表，以及与他们相关的挂载点。/etc/passwd，包含用户帐号列表。</td></tr><tr><td>/home</td><td>在通常的配置环境下，系统会在/home 下，给每个用户分配一个目录。普通用户只能在自己的目录下写文件。这个限制保护系统免受错误的用户活动破坏。</td></tr><tr><td>/lib</td><td>包含核心系统程序所使用的共享库文件。这些文件与 Windows 中的动态链接库相似。</td></tr><tr><td>/lost+found</td><td>每个使用 Linux 文件系统的格式化分区或设备，例如 ext3文件系统， 都会有这个目录。当部分恢复一个损坏的文件系统时，会用到这个目录。这个目录应该是空的，除非文件系统 真正的损坏了。</td></tr><tr><td>/media</td><td>在现在的 Linux 系统中，/media 目录会包含可移动介质的挂载点， 例如 USB 驱动器，CD-ROMs 等等。这些介质连接到计算机之后，会自动地挂载到这个目录结点下。</td></tr><tr><td>/mnt</td><td>在早些的 Linux 系统中，/mnt 目录包含可移动介质的挂载点。</td></tr><tr><td>/opt</td><td>这个/opt 目录被用来安装“可选的”软件。这个主要用来存储可能 安装在系统中的商业软件产品。</td></tr><tr><td>/proc</td><td>这个/proc 目录很特殊。从存储在硬盘上的文件的意义上说，它不是真正的文件系统。 相反，它是一个由 Linux 内核维护的虚拟文件系统。它所包含的文件是内核的窥视孔。这些文件是可读的， 它们会告诉你内核是怎样监管计算机的。</td></tr><tr><td>/root</td><td>root 帐户的家目录。</td></tr><tr><td>/sbin</td><td>这个目录包含“系统”二进制文件。它们是完成重大系统任务的程序，通常为超级用户保留。</td></tr><tr><td>/tmp</td><td>这个/tmp 目录，是用来存储由各种程序创建的临时文件的地方。一些配置导致系统每次 重新启动时，都会清空这个目录。</td></tr><tr><td>/usr</td><td>在 Linux 系统中，/usr 目录可能是最大的一个。它包含普通用户所需要的所有程序和文件。</td></tr><tr><td>/usr/bin</td><td>/usr/bin 目录包含系统安装的可执行程序。通常，这个目录会包含许多程序。</td></tr><tr><td>/usr/lib</td><td>包含由/usr/bin 目录中的程序所用的共享库。</td></tr><tr><td>/usr/local</td><td>这个/usr/local 目录，是非系统发行版自带程序的安装目录。 通常，由源码编译的程序会安装在/usr/local/bin 目录下。新安装的 Linux 系统中会存在这个目录， 并且在管理员安装程序之前，这个目录是空的。</td></tr><tr><td>/usr/sbin</td><td>包含许多系统管理程序。</td></tr><tr><td>/usr/share</td><td>/usr/share 目录包含许多由/usr/bin 目录中的程序使用的共享数据。 其中包括像默认的配置文件、图标、桌面背景、音频文件等等。</td></tr><tr><td>/usr/share/doc</td><td>大多数安装在系统中的软件包会包含一些文档。在/usr/share/doc 目录下， 我们可以找到按照软件包分类的文档。</td></tr><tr><td>/var</td><td>除了/tmp 和/home 目录之外，相对来说，目前我们看到的目录是静态的，这是说， 它们的内容不会改变。/var 目录存放的是动态文件。各种数据库，假脱机文件， 用户邮件等等，都位于在这里。</td></tr><tr><td>/var/log</td><td>这个/var/log 目录包含日志文件、各种系统活动的记录。这些文件非常重要，并且 应该时时监测它们。其中最重要的一个文件是/var/log/messages。注意，为了系统安全，在一些系统中， 你必须是超级用户才能查看这些日志文件。</td></tr></tbody></table></div><h2 id="操作文件和目录"><a href="#操作文件和目录" class="headerlink" title="操作文件和目录"></a>操作文件和目录</h2><blockquote><p>此时此刻，我们已经准备好了做些真正的工作！这一章节将会介绍以下命令：</p></blockquote><pre><code>cp &lt;src&gt; &lt;dst&gt; # 复制文件和目录mv &lt;src&gt; &lt;dst&gt; # 移动/重命名文件和目录mkdir &lt;folder_name&gt; # 创建目录rm &lt;dir/file&gt; # 删除文件和目录ln # 创建硬链接和符号链接</code></pre><blockquote><p>这五个命令属于最常使用的 Linux 命令之列。它们用来操作文件和目录。<br>现在，坦诚地说，用图形文件管理器来完成一些由这些命令执行的任务会更容易些。使用文件管理器， 我们可以把文件从一个目录拖放到另一个目录、剪贴和粘贴文件、删除文件等等。那么， 为什么还使用早期的命令行程序呢？<br>答案是命令行程序，功能强大灵活。虽然图形文件管理器能轻松地实现简单的文件操作，但是对于 复杂的文件操作任务，则使用命令行程序比较容易完成。例如，怎样拷贝一个目录下所有的HTML文件 ——这些文件在目标目录不存在或者版本比目标目录的文件更新——到目标目录呢？ 要完成这个任务，使用文件管理器相当难，使用命令行相当容易：<code>cp -u *.html destination</code></p></blockquote><h3 id="通配符"><a href="#通配符" class="headerlink" title="通配符"></a>通配符</h3><blockquote><p>在开始使用命令之前，我们需要介绍一个使命令行如此强大的 shell 特性。因为 shell 频繁地使用 文件名，shell 提供了特殊字符来帮助你快速指定一组文件名。这些特殊字符叫做通配符。 使用通配符（也以文件名代换著称）允许你依据字符的组合模式来选择文件名。下表列出这些通配符 以及它们所选择的对象：</p></blockquote><div class="table-container"><table><thead><tr><th>通配符</th><th>意义</th></tr></thead><tbody><tr><td>*</td><td>匹配任意多个字符（包括零个或一个）</td></tr><tr><td>?</td><td>匹配任意一个字符（不包括零个）</td></tr><tr><td>[characters]</td><td>匹配任意一个属于字符集中的字符</td></tr><tr><td>[!characters]</td><td>匹配任意一个不是字符集中的字符</td></tr><tr><td>[[:class:]]</td><td>匹配任意一个属于指定字符类中的字符</td></tr></tbody></table></div><blockquote><p>下表列出了最常使用的字符类：</p></blockquote><div class="table-container"><table><thead><tr><th>字符类</th><th>意义</th></tr></thead><tbody><tr><td>[:alnum:]</td><td>匹配任意一个字母或数字</td></tr><tr><td>[:alpha:]</td><td>匹配任意一个字母</td></tr><tr><td>[:digit:]</td><td>匹配任意一个数字</td></tr><tr><td>[:lower:]</td><td>匹配任意一个小写字母</td></tr><tr><td>[:upper:]</td><td>匹配任意一个大写字母</td></tr></tbody></table></div><blockquote><p>借助通配符，为文件名构建非常复杂的选择标准成为可能。下面是一些类型匹配的范例:</p></blockquote><div class="table-container"><table><thead><tr><th>模式</th><th>匹配对象</th></tr></thead><tbody><tr><td>*</td><td>所有文件</td></tr><tr><td>g*</td><td>文件名以“g”开头的文件</td></tr><tr><td>b*.txt</td><td>以”b”开头，中间有零个或任意多个字符，并以”.txt”结尾的文件</td></tr><tr><td>Data???</td><td>以“Data”开头，其后紧接着3个字符的文件</td></tr><tr><td>[abc]*</td><td>文件名以”a”,”b”,或”c”开头的文件</td></tr><tr><td>[[:upper:]]*</td><td>以大写字母开头的文件</td></tr><tr><td>[![:digit:]]*</td><td>不以数字开头的文件</td></tr><tr><td>*[[:lower:]123]</td><td>文件名以小写字母结尾，或以 “1”，“2”，或 “3” 结尾的文件</td></tr></tbody></table></div><h3 id="mkdir-指令"><a href="#mkdir-指令" class="headerlink" title="mkdir 指令"></a>mkdir 指令</h3><blockquote><p>mkdir 命令是用来创建目录的。它这样工作：</p></blockquote><pre><code>mkdir directory...</code></pre><blockquote><p>注意表示法: 在描述一个命令时（如上所示），当有三个圆点跟在一个命令的参数后面， 这意味着那个参数可以重复，就像这样：<code>mkdir dir1 dir2 dir3</code>会创建三个目录，名为 dir1, dir2, dir3。</p></blockquote><h3 id="cp-指令"><a href="#cp-指令" class="headerlink" title="cp 指令"></a>cp 指令</h3><blockquote><p>cp 命令，复制文件或者目录。它有两种使用方法：</p></blockquote><pre><code>cp item1 item2 # 用法一</code></pre><blockquote><p>用法一：复制<code>item1</code>到<code>item2</code>，其中<code>item</code>是文件或者是目录</p></blockquote><pre><code>cp item... directory # 用法二</code></pre><blockquote><p>用法二：复制多个<code>item</code>（文件或目录）到同一个目录<code>directory</code>下。</p><p>这里列举了 cp 命令一些有用的选项（短选项和等效的长选项）：</p></blockquote><div class="table-container"><table><thead><tr><th>选项</th><th>意义</th></tr></thead><tbody><tr><td>-a, —archive</td><td>复制文件和目录，以及它们的属性，包括所有权和权限。 通常，副本具有用户所操作文件的默认属性。</td></tr><tr><td>-i, —interactive</td><td>在重写已存在文件之前，提示用户确认。如果这个选项不指定， cp 命令会默认重写文件。</td></tr><tr><td>-r, —recursive</td><td>递归地复制目录及目录中的内容。当复制目录时， 需要这个选项（或者-a 选项）。</td></tr><tr><td>-u, —update</td><td>当把文件从一个目录复制到另一个目录时，仅复制 目标目录中不存在的文件，或者是文件内容新于目标目录中已经存在的文件。</td></tr><tr><td>-v, —verbose</td><td>显示翔实的命令操作信息</td></tr></tbody></table></div><blockquote><p>应用例子：</p></blockquote><div class="table-container"><table><thead><tr><th>命令</th><th>运行结果</th></tr></thead><tbody><tr><td>cp file1 file2</td><td>复制文件 file1 内容到文件 file2。如果 file2 已经存在， file2 的内容会被 file1 的内容重写。如果 file2 不存在，则会创建 file2。</td></tr><tr><td>cp -i file1 file2</td><td>这条命令和上面的命令一样，除了如果文件 file2 存在的话，在文件 file2 被重写之前， 会提示用户确认信息。</td></tr><tr><td>cp file1 file2 dir1</td><td>复制文件 file1 和文件 file2 到目录 dir1。目录 dir1 必须存在。</td></tr><tr><td>cp dir1/* dir2</td><td>使用一个通配符，在目录 dir1 中的所有文件都被复制到目录 dir2 中。 dir2 必须已经存在。</td></tr><tr><td>cp -r dir1 dir2</td><td>复制目录 dir1 中的内容到目录 dir2。如果目录 dir2 不存在， 创建目录 dir2，操作完成后，目录 dir2 中的内容和 dir1 中的一样。 如果目录 dir2 存在，则目录 dir1 (和目录中的内容)将会被复制到 dir2 中。</td></tr></tbody></table></div><h3 id="mv-指令"><a href="#mv-指令" class="headerlink" title="mv 指令"></a>mv 指令</h3><blockquote><p>mv 命令可以执行文件移动和文件命名任务，这依赖于你怎样使用它。任何一种 情况下，完成操作之后，原来的文件名不再存在。mv 使用方法与 cp 很相像：</p></blockquote><pre><code>mv item1 item2 # 方式一</code></pre><blockquote><p>方式一：把文件或目录 “item1” 移动或重命名为 “item2”, 或者：</p></blockquote><pre><code>mv item... directory # 方式二</code></pre><blockquote><p>方式二：把一个或多个条目从一个目录移动到另一个目录中。</p><p>mv 与 cp 共享了很多一样的选项：</p></blockquote><div class="table-container"><table><thead><tr><th>选项</th><th>意义</th></tr></thead><tbody><tr><td>-i —interactive</td><td>在重写一个已经存在的文件之前，提示用户确认信息。 <strong>如果不指定这个选项，mv 命令会默认重写文件内容。</strong></td></tr><tr><td>-u —update</td><td>当把文件从一个目录移动另一个目录时，只是移动不存在的文件， 或者文件内容新于目标目录相对应文件的内容。</td></tr><tr><td>-v —verbose</td><td>当操作 mv 命令时，显示翔实的操作信息。</td></tr></tbody></table></div><blockquote><p>操作实例：</p></blockquote><div class="table-container"><table><thead><tr><th>mv file1 file2</th><th>移动 file1 到 file2。 <strong>如果 file2 存在，它的内容会被 file1 的内容重写。如果 file2 不存在，则创建 file2。这两种情况下，file1 都不再存在。</strong></th></tr></thead><tbody><tr><td>mv -i file1 file2</td><td>除了如果 file2 存在的话，在 file2 被重写之前，用户会得到 提示信息外，这个和上面的选项一样。</td></tr><tr><td>mv file1 file2 dir1</td><td>移动 file1 和 file2 到目录 dir1 中。dir1 必须已经存在。</td></tr><tr><td>mv dir1 dir2</td><td>如果目录 dir2 不存在，创建目录 dir2，并且移动目录 dir1 的内容到 目录 dir2 中，同时删除目录 dir1。如果目录 dir2 存在，移动目录 dir1（及它的内容）到目录 dir2。</td></tr></tbody></table></div><h3 id="rm-指令"><a href="#rm-指令" class="headerlink" title="rm 指令"></a>rm 指令</h3><blockquote><p>rm 命令用来移除（删除）文件和目录：</p></blockquote><pre><code>rm item...</code></pre><blockquote><p>“item”代表一个或多个文件或目录。</p><p>下表是一些普遍使用的 rm 选项：</p></blockquote><div class="table-container"><table><thead><tr><th>选项</th><th>意义</th></tr></thead><tbody><tr><td>-i, —interactive</td><td>在删除已存在的文件前，提示用户确认信息。 <strong>如果不指定这个选项，rm 会默默地删除文件</strong></td></tr><tr><td>-r, —recursive</td><td>递归地删除文件，这意味着，如果要删除一个目录，而此目录 又包含子目录，那么子目录也会被删除。要删除一个目录，必须指定这个选项。</td></tr><tr><td>-f, —force</td><td>忽视不存在的文件，不显示提示信息。这选项覆盖了“—interactive”选项。</td></tr><tr><td>-v, —verbose</td><td>在执行 rm 命令时，显示翔实的操作信息。</td></tr></tbody></table></div><blockquote><p>操作实例：</p></blockquote><div class="table-container"><table><thead><tr><th>命令</th><th>运行结果</th></tr></thead><tbody><tr><td>rm file1</td><td>默默地删除文件</td></tr><tr><td>rm -i file1</td><td>除了在删除文件之前，提示用户确认信息之外，和上面的命令作用一样。</td></tr><tr><td>rm -r file1 dir1</td><td>删除文件 file1, 目录 dir1，及 dir1 中的内容。</td></tr><tr><td>rm -rf file1 dir1</td><td>同上，除了如果文件 file1，或目录 dir1 不存在的话，rm 仍会继续执行。</td></tr></tbody></table></div><blockquote><p><strong>小心 rm!</strong><br>类 Unix 的操作系统，比如说 Linux，没有复原命令。一旦你用 rm 删除了一些东西， 它就消失了。Linux 假定你很聪明，你知道你在做什么。<br>尤其要小心通配符。思考一下这个经典的例子。假如说，你只想删除一个目录中的 HTML 文件。输入：<code>rm *.html</code><br>这是正确的，如果你不小心在 “<em>” 和 “.html” 之间多输入了一个空格，就像这样：`rm </em> .html`<br>这个 rm 命令会删除目录中的所有文件，还会抱怨没有文件叫做 “.html”。<br><strong>[小贴士]</strong> 当你使用带有通配符的rm命令时（除了仔细检查输入的内容外）， 先用 ls 命令来测试通配符。这会让你看到将要被删除的文件是什么。然后按下上箭头按键，重新调用 刚刚执行的命令，用 rm 替换 ls。</p></blockquote><h3 id="ln-指令"><a href="#ln-指令" class="headerlink" title="ln 指令"></a>ln 指令</h3><blockquote><p>ln 命令既可创建硬链接，也可以创建符号链接。</p></blockquote><pre><code>ln file link # 创建硬链接ln -s item link # 创建符号链接，”item” 可以是一个文件或是一个目录。</code></pre><blockquote><p>与更加现代的符号链接相比，硬链接是最初 Unix 创建链接的方式。每个文件默认会有一个硬链接， 这个硬链接给予文件名字。我们每创建一个硬链接，就为一个文件创建了一个额外的目录项。 硬链接有两个重要局限性：</p><ol><li>一个硬链接不能关联它所在文件系统之外的文件。这是说一个链接不能关联与链接本身不在同一个磁盘分区上的文件。</li><li>一个硬链接不能关联一个目录。<br>一个硬链接和文件本身没有什么区别。不像符号链接，当你列出一个包含硬链接的目录内容时，你会看到没有特殊的链接指示说明。当一个硬链接被删除时，这个链接 被删除，但是文件本身的内容仍然存在（这是说，它所占的磁盘空间不会被重新分配）， 直到所有关联这个文件的链接都删除掉。知道硬链接很重要，因为你可能有时 会遇到它们，但现在实际中更喜欢使用符号链接，下一步我们会讨论符号链接。</li></ol><p>创建符号链接是为了克服硬链接的局限性。符号链接生效，是通过创建一个 特殊类型的文件，这个文件包含一个关联文件或目录的文本指针。在这一方面， 它们和 Windows 的快捷方式差不多，当然，符号链接早于 Windows 的快捷方式 很多年 ;-)<br>一个符号链接指向一个文件，而且这个符号链接本身与其它的符号链接几乎没有区别。 例如，如果你往一个符号链接里面写入东西，那么相关联的文件也被写入。然而， 当你删除一个符号链接时，只有这个链接被删除，而不是文件自身。如果先于符号链接 删除文件，这个链接仍然存在，但是不指向任何东西。在这种情况下，这个链接被称为 坏链接。在许多实现中，ls 命令会以不同的颜色展示坏链接，比如说红色，来显示它们 的存在。</p><p>关于链接的概念，看起来很迷惑，但不要胆怯。我们将要试着练习 这些命令，希望，它变得清晰起来。</p></blockquote><h3 id="创建游戏场（实战演习）"><a href="#创建游戏场（实战演习）" class="headerlink" title="创建游戏场（实战演习）"></a>创建游戏场（实战演习）</h3><blockquote><p>下面我们将要做些真正的文件操作，让我们先建立一个安全地带， 来玩一下文件操作命令。首先，我们需要一个工作目录。在我们的 家目录下创建一个叫做“playground”的目录。</p></blockquote><pre><code>[root@izwz9biz2m4sd3tu00600pz ~]# cd[root@izwz9biz2m4sd3tu00600pz ~]# mkdir playground[root@izwz9biz2m4sd3tu00600pz ~]# cd playground[root@izwz9biz2m4sd3tu00600pz playground]# mkdir dir1 dir2[root@izwz9biz2m4sd3tu00600pz playground]# cp /etc/passwd .[root@izwz9biz2m4sd3tu00600pz playground]# ls -ltotal 12drwxr-xr-x 2 root root 4096 Mar  8 23:19 dir1drwxr-xr-x 2 root root 4096 Mar  8 23:19 dir2-rw-r--r-- 1 root root 1320 Mar  8 23:20 passwd[root@izwz9biz2m4sd3tu00600pz playground]# mv fun dir1[root@izwz9biz2m4sd3tu00600pz playground]# ls dir1fun[root@izwz9biz2m4sd3tu00600pz playground]# mv dir1/fun .[root@izwz9biz2m4sd3tu00600pz playground]# lsdir1  dir2  fun</code></pre><blockquote><p>现在，我们试着创建链接。首先是硬链接。我们创建一些关联我们数据文件的链接：</p></blockquote><pre><code>[root@izwz9biz2m4sd3tu00600pz playground]# ln fun fun-hard[root@izwz9biz2m4sd3tu00600pz playground]# ln fun dir1/fun-hard[root@izwz9biz2m4sd3tu00600pz playground]# ln fun dir2/fun-hard[root@izwz9biz2m4sd3tu00600pz playground]# ls -ltotal 16drwxr-xr-x 2 root root 4096 Mar  8 23:26 dir1drwxr-xr-x 2 root root 4096 Mar  8 23:26 dir2-rw-r--r-- 4 root root 1320 Mar  8 23:21 fun-rw-r--r-- 4 root root 1320 Mar  8 23:21 fun-hard</code></pre><blockquote><p>注意到一件事，列表中，文件 fun 和 fun-hard 的第二个字段是”4”，这个数字 是文件”fun”的硬链接数目。你要记得一个文件至少有一个硬链接，因为文件 名就是由链接创建的。那么，我们怎样知道实际上 fun 和 fun-hard 是同一个文件呢？ 在这个例子里，ls 不是很有用。虽然我们能够看到 fun 和 fun-hard 文件大小一样 （第五字段），但我们的列表没有提供可靠的信息来确定（这两个文件一样）。 为了解决这个问题，我们更深入的研究一下。<br>当考虑到硬链接的时候，我们可以假设文件由两部分组成：包含文件内容的数据部分和持有文件名的名字部分 ，这将有助于我们理解这个概念。当我们创建文件硬链接的时候，实际上是为文件创建了额外的名字部分， 并且这些名字都关联到相同的数据部分。这时系统会分配一连串的磁盘块给所谓的索引节点，然后索引节点与文 件名字部分相关联。因此每一个硬链接都关系到一个具体的包含文件内容的索引节点。<br>ls 命令有一种方法，来展示（文件索引节点）的信息。在命令中加上”-i”选项：</p></blockquote><pre><code>[root@izwz9biz2m4sd3tu00600pz playground]# ls -litotal 16131435 drwxr-xr-x 2 root root 4096 Mar  8 23:26 dir1131436 drwxr-xr-x 2 root root 4096 Mar  8 23:26 dir2131437 -rw-r--r-- 4 root root 1320 Mar  8 23:21 fun131437 -rw-r--r-- 4 root root 1320 Mar  8 23:21 fun-hard</code></pre><blockquote><p>在这个版本的列表中，第一字段表示文件索引节点号，正如我们所见到的， fun 和 fun-hard 共享一样的索引节点号，这就证实这两个文件是同一个文件。</p><p>建立符号链接的目的是为了克服硬链接的两个缺点：硬链接不能跨越物理设备， 硬链接不能关联目录，只能是文件。符号链接是文件的特殊类型，它包含一个指向 目标文件或目录的文本指针。<br>符号链接的建立过程相似于创建硬链接：</p></blockquote><pre><code>[root@izwz9biz2m4sd3tu00600pz playground]# ln -s fun fun-sym[root@izwz9biz2m4sd3tu00600pz playground]# ln -s ../fun dir1/fun-sym[root@izwz9biz2m4sd3tu00600pz playground]# ln -s ../fun dir2/fun-sym</code></pre><blockquote><p>第一个例子相当直接，在 ln 命令中，简单地加上”-s”选项就可以创建一个符号链接， 而不是一个硬链接。下面两个例子又是怎样呢？ 记住，当我们创建一个符号链接 的时候，会建立一个目标文件在哪里和符号链接有关联的文本描述。如果我们看看 ls 命令的输出结果，比较容易理解。</p></blockquote><pre><code>[root@izwz9biz2m4sd3tu00600pz playground]# ls -l dir1total 4-rw-r--r-- 4 root root 1320 Mar  8 23:21 fun-hardlrwxrwxrwx 1 root root    6 Mar  8 23:34 fun-sym -&gt; ../fun</code></pre><blockquote><p>目录 dir1 中，fun-sym 的列表说明了它是一个符号链接，通过在第一字段中的首字符”l” 可知，并且它还指向”../fun”，也是正确的。相对于 fun-sym 的存储位置，fun 在它的 上一个目录。同时注意，符号链接文件的长度是6，这是字符串”../fun”所包含的字符数， 而不是符号链接所指向的文件长度。<br>当建立符号链接时，你既可以使用绝对路径名：<code>ln -s /home/me/playground/fun dir1/fun-sym</code>，也可用相对路径名，正如前面例题所展示的。使用相对路径名更令人满意， 因为它允许一个包含符号链接的目录重命名或移动，而不会破坏链接。<br>除了普通文件，符号链接也能关联目录：</p></blockquote><pre><code>[root@izwz9biz2m4sd3tu00600pz playground]# ln -s dir1 dir1-sym[root@izwz9biz2m4sd3tu00600pz playground]# ls -ltotal 16drwxr-xr-x 2 root root 4096 Mar  8 23:34 dir1lrwxrwxrwx 1 root root    4 Mar  8 23:40 dir1-sym -&gt; dir1drwxr-xr-x 2 root root 4096 Mar  8 23:34 dir2-rw-r--r-- 4 root root 1320 Mar  8 23:21 fun-rw-r--r-- 4 root root 1320 Mar  8 23:21 fun-hardlrwxrwxrwx 1 root root    3 Mar  8 23:34 fun-sym -&gt; fun</code></pre><blockquote><p>正如我们之前讨论的，rm 命令被用来删除文件和目录。我们将要使用它 来清理一下我们的游戏场。首先，删除一个硬链接：</p></blockquote><pre><code>[root@izwz9biz2m4sd3tu00600pz playground]# rm fun-hardrm: remove regular file ‘fun-hard’? y[root@izwz9biz2m4sd3tu00600pz playground]# ls -ltotal 12drwxr-xr-x 2 root root 4096 Mar  8 23:34 dir1lrwxrwxrwx 1 root root    4 Mar  8 23:40 dir1-sym -&gt; dir1drwxr-xr-x 2 root root 4096 Mar  8 23:34 dir2-rw-r--r-- 3 root root 1320 Mar  8 23:21 funlrwxrwxrwx 1 root root    3 Mar  8 23:34 fun-sym -&gt; fun[root@izwz9biz2m4sd3tu00600pz playground]# rm fun-sym dir1-symrm: remove symbolic link ‘fun-sym’? yrm: remove symbolic link ‘dir1-sym’? y[root@izwz9biz2m4sd3tu00600pz playground]# lsdir1  dir2  fun</code></pre><blockquote><p>对于符号链接，有一点值得记住，执行的大多数文件操作是针对链接的对象，而不是链接本身。 而 rm 命令是个特例。当你删除链接的时候，删除链接本身，而不是链接的对象。</p><p>最后，我们将删除我们的游戏场。为了完成这个工作，我们将返回到 我们的家目录，然后用 rm 命令加上选项(-r)，来删除目录 playground， 和目录下的所有内容，包括子目录：</p></blockquote><pre><code>[root@izwz9biz2m4sd3tu00600pz playground]# cd[root@izwz9biz2m4sd3tu00600pz ~]# rm -r  playgroundrm: descend into directory ‘playground’? yrm: descend into directory ‘playground/dir1’? yrm: remove symbolic link ‘playground/dir1/fun-sym’? yrm: remove regular file ‘playground/dir1/fun-hard’? yrm: remove directory ‘playground/dir1’? yrm: descend into directory ‘playground/dir2’? yrm: remove symbolic link ‘playground/dir2/fun-sym’? yrm: remove regular file ‘playground/dir2/fun-hard’? yrm: remove directory ‘playground/dir2’? yrm: remove regular file ‘playground/fun’? yrm: remove directory ‘playground’? y</code></pre><blockquote><p>在这一章中，我们已经研究了许多基础知识。我们得花费一些时间来全面地理解。 反复练习 playground 例题，直到你觉得它有意义。能够良好地理解基本文件操作 命令和通配符，非常重要。随意通过添加文件和目录来拓展 playground 练习， 使用通配符来为各种各样的操作命令指定文件。关于链接的概念，在刚开始接触 时会觉得有点迷惑，花些时间来学习它们是怎样工作的。它们能成为真正的救星。</p></blockquote><p><strong>TO BE CONTINUED…</strong></p>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> grammar </tag>
            
            <tag> linux </tag>
            
            <tag> programing </tag>
            
            <tag> os </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Markdown-快速入门</title>
      <link href="/markdown/markdown-beginner/"/>
      <url>/markdown/markdown-beginner/</url>
      
        <content type="html"><![CDATA[<p>Markdown 是什么？它是一门标记语言，它能实现与 Microsoft Word 文字排版相似的功能。而这篇文章正是用 Markdown 语言编写的，我们使用它来进行文字的排版。</p><p><img src="Markdown-mark.svg" alt></p><h1 id="行文结构"><a href="#行文结构" class="headerlink" title="行文结构"></a>行文结构</h1><h2 id="标题"><a href="#标题" class="headerlink" title="标题"></a>标题</h2><blockquote><p>使用井号 “#” 来调整标题的大小。实例代码如下：</p></blockquote><pre><code># 一级大标题## 二级标题### 三级标题#### 以此类推</code></pre><blockquote><p>效果如下：<br><img src="1.png" alt></p></blockquote><h2 id="分割线"><a href="#分割线" class="headerlink" title="分割线"></a>分割线</h2><blockquote><p>使用三个横线 “---” 来实现分割线，实例代码如下</p></blockquote><pre><code>---</code></pre><blockquote><p>效果如下：</p><hr></blockquote><h1 id="段落文字"><a href="#段落文字" class="headerlink" title="段落文字"></a>段落文字</h1><h2 id="粗体"><a href="#粗体" class="headerlink" title="粗体"></a>粗体</h2><blockquote><p>使用两个星号 “**” 包围住想要加粗的字体，代码如下：（其中的符号 “**” 也可以用 “__” 替代，注意不带空格）</p></blockquote><pre><code>**我是粗体**</code></pre><blockquote><p>效果如下：<br><strong>我是粗体</strong></p><h2 id="斜体"><a href="#斜体" class="headerlink" title="斜体"></a>斜体</h2><p>使用一个星号 “*” 包围住想要加粗的字体，代码如下：（其中的符号 “*” 也可以用 “_” 替代，注意不带空格）</p></blockquote><pre><code>*我是斜体*</code></pre><blockquote><p>效果如下：<br><em>我是斜体</em><br>如果想要粗斜体，则使用三个星号 “***” 包围即可。</p></blockquote><h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><blockquote><p>此处的引用指的是在段落的前方出现一条竖线，代码如下：</p></blockquote><pre><code> &gt; Markdown is a lightweight markup language with plain text formatting syntax. Its design allows it to be converted to many output formats, but the original tool by the same name only supports HTML.</code></pre><blockquote><p> 效果如下：</p><blockquote><p>Markdown is a lightweight markup language with plain text formatting syntax. Its design allows it to be converted to many output formats, but the original tool by the same name only supports HTML.</p></blockquote></blockquote><h2 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h2><blockquote><p>列表分为有序列表和无序列表，代码如下：（其中的符号 “-” 也可以用 “*” 或 “+” 替代）</p></blockquote><pre><code>- 无序列表A- 无序列表B1. 有序列表A2. 有序列表B</code></pre><blockquote><p>效果如下：</p><ul><li>无序列表A</li><li>无序列表B</li></ul><ol><li>有序列表A</li><li>有序列表B</li></ol></blockquote><h2 id="选择框"><a href="#选择框" class="headerlink" title="选择框"></a>选择框</h2><blockquote><p>选择框类似windows的勾选框框，代码如下：</p></blockquote><pre><code>- [ ] 不勾选- [x] 勾选</code></pre><blockquote><p>效果如下：</p><ul><li>[ ] 不勾选</li><li>[x] 勾选</li></ul></blockquote><h2 id="超链接"><a href="#超链接" class="headerlink" title="超链接"></a>超链接</h2><blockquote><p>代码如下：</p></blockquote><pre><code>[Karbo的博客](https://blog.karbo.online)</code></pre><blockquote><p>效果如下：<br><a href="https://blog.karbo.online">Karbo的博客</a></p></blockquote><h1 id="特殊部件"><a href="#特殊部件" class="headerlink" title="特殊部件"></a>特殊部件</h1><h2 id="代码段"><a href="#代码段" class="headerlink" title="代码段"></a>代码段</h2><blockquote><p>代码段一般支持高亮功能，用于显示代码，示例代码如下：</p></blockquote><p><img src="2.png" alt></p><blockquote><p>代码段被三个反引号包括（即数字1左边的那个符号），在其后可以指定编程语言的类型。<br>效果如下：</p></blockquote><pre class=" language-lang-c++"><code class="language-lang-c++">int main(int argc, char *argv[]){    if (argc == 2)    {        cout << argv[1] << endl;    }}</code></pre><h2 id="表格"><a href="#表格" class="headerlink" title="表格"></a>表格</h2><blockquote><p>第一列左对齐、第二列居中、第三列右对齐的表格，实现代码如下：</p></blockquote><pre><code>| 1    |  2   |    3 || :--- | :--: | ---: || 4    |  5   |    6 || 7    |  8   |    9 |</code></pre><blockquote><p>效果如下：</p></blockquote><div class="table-container"><table><thead><tr><th style="text-align:left">1</th><th style="text-align:center">2</th><th style="text-align:right">3</th></tr></thead><tbody><tr><td style="text-align:left">4</td><td style="text-align:center">5</td><td style="text-align:right">6</td></tr><tr><td style="text-align:left">7</td><td style="text-align:center">8</td><td style="text-align:right">9</td></tr></tbody></table></div><h2 id="图片"><a href="#图片" class="headerlink" title="图片"></a>图片</h2><blockquote><p>可以在 Markdown 中嵌入图片，示例代码如下：</p></blockquote><pre><code>![图片的描述](图片的路径 鼠标悬浮于图片时显示的文字)</code></pre><blockquote><ol><li>其中 “图片的描述” 为用来描述图片的关键词，可以不写。最初的本意是当图片因为某种原因不能被显示时而出现的替代文字，后来又被用于SEO，可以方便搜索引擎根据描述关键词搜索到图片。</li><li>其中 “图片的路径” 可以使用绝对路径、相对路径，或是图片网址。<br> 如果你的 Markdown 编辑器是 Typora，则可以通过在 Front Matter 中配置 typora-root-url 的值来指定寻找图片的根路径。</li><li>其中 “鼠标悬浮于图片时显示的文字” 为可选选项，可以不填。<br>除此之外，插入图片还有类似于 reference 的方式，示例代码如下：</li></ol></blockquote><pre><code>以下是我们的图片啦：![GitHub][github]以上为我们的图片了。[github]: https://avatars2.githubusercontent.com/u/3265208?v=3&amp;s=100 &quot;GitHub,Social Coding&quot;</code></pre><blockquote><p>上述代码则会在 “以下是我们的图片啦：” 和 “以上为我们的图片了。” 之间插入图片。<br>相当于为图片创建了一个id，叫 github，以后使用图片只需要<code>![][github]</code>这样就可以啦。而<code>[github]: https://avatars2.githubusercontent.com/u/3265208?v=3&amp;s=100 &quot;GitHub,Social Coding&quot;</code>这句话则不会显示，因为它是用来指明 github 是什么图片的语句。</p><p>效果如下：</p><p>以下是我们的图片啦：<br><img src="https://avatars2.githubusercontent.com/u/3265208?v=3&amp;s=100" alt="GitHub" title="GitHub,Social Coding"><br>以上为我们的图片了。</p></blockquote><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><blockquote><p>Markdown 还支持画流程图、时序图、甘特图等<br>因为不常用，这里就不一一列出使用方法了，网上有教程。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> markdown </category>
          
      </categories>
      
      
        <tags>
            
            <tag> grammar </tag>
            
            <tag> programing </tag>
            
            <tag> markdown </tag>
            
            <tag> writing </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数学资源</title>
      <link href="/math/mathlearn/"/>
      <url>/math/mathlearn/</url>
      
        <content type="html"><![CDATA[<p>本文搜集了一些网络上优秀的数学学习资源。</p><p><img src="1.jpg" alt></p><h1 id="3Blue1Brown"><a href="#3Blue1Brown" class="headerlink" title="3Blue1Brown"></a>3Blue1Brown</h1><blockquote><p>3blue1brown的创始人是毕业于斯坦福大学的Grant Sanderson，他用“可以看见”的动画方式，通俗易懂、深入浅出的解释了数学原理。这是一种“让你懂”的解释，而不是一种“仅他懂”的证明。3blue1brown是我见过的最好的数学科普网站，没有之一。</p></blockquote><p><img src="2.png" alt></p><p><img src="3.png" alt></p><blockquote><p>链接：</p><ul><li><a href="https://www.3blue1brown.com/" target="_blank" rel="noopener">3Blue1Brown（油管）</a></li><li><a href="https://space.bilibili.com/88461692/" target="_blank" rel="noopener">3Blue1Brown（B站）</a></li></ul></blockquote><h1 id="沉浸式线性代数"><a href="#沉浸式线性代数" class="headerlink" title="沉浸式线性代数"></a>沉浸式线性代数</h1><blockquote><p>本网站试图用交互式的动图来解释线性代数，一图胜千言。</p></blockquote><ul><li><a href="http://immersivemath.com/ila/index.html" target="_blank" rel="noopener">immersive linear algebra</a></li></ul><p><img src="4.png" alt></p><p><img src="5.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> math </category>
          
      </categories>
      
      
        <tags>
            
            <tag> math </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
