<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>3D分类笔记</title>
      <link href="/3d/3dclassificationnotes/"/>
      <url>/3d/3dclassificationnotes/</url>
      
        <content type="html"><![CDATA[<h1 id="3D分类笔记"><a href="#3D分类笔记" class="headerlink" title="3D分类笔记"></a>3D分类笔记</h1><p>下面的papers都是有关3D点云特征提取的，将提取到的特征用于分类或分割任务。</p><p>主要是围绕如何提取局部的几何关系特征而展开，给我的感觉就像是传统的机器学习算法（像SIFT那代）与深度学习相结合。</p><h2 id="PointNet-👍"><a href="#PointNet-👍" class="headerlink" title="PointNet 👍"></a>PointNet 👍</h2><blockquote><p>CVPR2017《PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation》</p><p>在线介绍：<a href="http://stanford.edu/~rqi/pointnet/" target="_blank" rel="noopener">http://stanford.edu/~rqi/pointnet/</a></p><p>paper下载：<a href="http://openaccess.thecvf.com/content_cvpr_2017/html/Qi_PointNet_Deep_Learning_CVPR_2017_paper.html" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2017/html/Qi_PointNet_Deep_Learning_CVPR_2017_paper.html</a></p><p>pytorch代码：<a href="https://github.com/fxia22/pointnet.pytorch" target="_blank" rel="noopener">https://github.com/fxia22/pointnet.pytorch</a></p></blockquote><h3 id="概括"><a href="#概括" class="headerlink" title="概括"></a>概括</h3><p>提出了一种“天然”适用于处理point cloud数据的Net。并且给出理论分析和较多实验。声称达到了state-of-the-art的效果：模型结构简单、精度高、计算复杂度低。</p><p>TODO IDEA：如果对称函数/max pooling换做了sort函数/top k函数（排序后返回前k大的值），那会怎么样？没有层次化特征提取的过程（PointNet++已解决）？</p><h3 id="难点"><a href="#难点" class="headerlink" title="难点"></a>难点</h3><p>最大的难点是如何处理point cloud数据的无序性，因为传统的操作都是有顺序依赖的（例如卷积，它是顺序相关的）。此外，点之间的空间相关性、变换（平移旋转等）的不变性也是难点所在。</p><p>传统的处理方法：将点云数据投影到二维平面然后使用图像的方法处理；将点云数据划分到有空间依赖关系的voxel；数据sort之后再输入；使用RNN尝试消除输入的顺序性。</p><h3 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h3><p>使用具有对称性质的函数（例如max函数），来实现处理点云的无序性。</p><p>形式化表述即：</p><script type="math/tex; mode=display">   f(\{x_1,...,x_n\}) \approx g(h(x_1),...,h(x_n))</script><p>   意思是说，先进行非线性变换$h(\cdot)$，再使用对称函数$g(\cdot)$，就能近似出某一个能实现无序性的函数$f(\cdot)$。且$h(\cdot)$决定了$f(\cdot)$，所以关键是如何学习$h(\cdot)$。</p><p>   其中函数$h(\cdot)$使用多层感知机自动学习出来，$g(\cdot)$是某一个对称函数（例如max函数），$x_i$是第$i$个点。</p><h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><ol><li><p>网络前端（实现变换不变性）</p><p>使用与输入数据相关的spatial transformer network（缩写stn），来生成一个仿射变换矩阵（它是方阵，例如一个object有2500个points且point的维度是3（x、y和z坐标构成3维）的点，那么丢进这个网络则会得到一个3x3大小的方阵）<br>然后将输入数据点施加这个仿射变换（乘以它），就得到预处理后的结果（虽然还是2500个维度是3的点）。可能可以认为这个仿射变换的作用是消除camera的摆放视角（看作是某一种变换）的影响？？</p><pre><code>input data size: [batchsize, 3, 2500]after many conv1d: [batchsize, 1024, 2500]after max function: [batchsize, 1024, 1]after view: [batchsize, 1024]after many fc layer: [batchsize, 9]after reshape to matrix: [batchsize, 3, 3]after add identity matrix: [batchsize, 3, 3] （加上3x3的单位矩阵，why？？可能类似于resnet的思想，或者是其他原因）</code></pre><p>且我们在loss上加了正则约束项$||I-AA^T||^2_F$，使得学习到的矩阵$A$尽可能地是正交矩阵。</p></li><li><p>网络中端（消除输入的顺序性）</p><p>然后按照下图来操作：</p><p><img src="http://stanford.edu/~rqi/pointnet/images/pointnet.jpg" alt></p><p>两处用到了stn网络。stn生成仿射矩阵后对每个points都施加相同的仿射变换，不同sample的仿射矩阵不一样，都需要用stn网络计算出来。至于为什么后面也有个stn，那就只能姑且认为是stn网络起到类似于batchnorm的作用了。。。</p><p>其中max函数对每个feature_dim（共1024个）扫视一遍，然后取max（例如2500个points则取2500个数值中最大的，然后形成相应feature_dim上的值）</p><p>上图中蓝色部分是分类网络，黄色部分是分割网络。分割网络需要cancat global features和local features。（但实际上local features并没有充分地提取局部特征，例如密度表征等，所以后来才提出了其改进版PointNet++，但速度更慢）</p></li><li><p>网络后端（分类or分割）</p><ul><li><p>分类就是正常的Classificaion网络了</p></li><li><p>分割则是继续使用conv1d，然后view得到<code>[batchsize, 2500, classnum]</code>，再view作<code>[batchsize*2500, classnum]</code>然后使用softmax，最后再view回来<code>[batchsize, 2500, classnum]</code>就好了</p></li></ul></li></ol><h2 id="PointNet"><a href="#PointNet" class="headerlink" title="PointNet++"></a>PointNet++</h2><blockquote><p>NIPS2017《PointNet++: Deep Hierarchical Feature Learning onPoint Sets in a Metric Space》</p><p>在线介绍：<a href="http://stanford.edu/~rqi/pointnet2/" target="_blank" rel="noopener">http://stanford.edu/~rqi/pointnet2/</a></p><p>paper下载：<a href="http://papers.nips.cc/paper/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space" target="_blank" rel="noopener">http://papers.nips.cc/paper/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space</a></p></blockquote><h3 id="概括-1"><a href="#概括-1" class="headerlink" title="概括"></a>概括</h3><p>是PointNet的升级版。针对PointNet对local features提取能力弱的缺点（例如不能应对points density的variation），提出使用层级结构来提取local features。简言之，就是模仿了CNN的分层特征提取，它将全体Points划分为许多local regions（类比于CNN中Kernel大小的概念），然后使用原始版本的PointNet提取regions里点的特征，并利用centroid取代regions的点，实现点数的减少，再多次这样，最后就能提取包含local features的features了。</p><p>结果：特征表达更有效、鲁棒、精度高、但是计算代价高且慢（比PointNet慢几倍）</p><p>其他：作者还发现在Non-Euclidean Metric Space（为geodesic distances）中模型的表现也很好，可以抵抗object的形变（例如不同姿态的猫）。作者可视化了the first level kernels，<a href="http://stanford.edu/~rqi/pointnet2/images/features.jpg" target="_blank" rel="noopener">戳图</a>。</p><p>TODO IDEA：可能不同的度量空间对性能也有影响？感觉划定local regions的方式过于粗暴和生硬，有待改进？</p><h3 id="难点-1"><a href="#难点-1" class="headerlink" title="难点"></a>难点</h3><p>如何有效地提取local features以抵抗density variation？（density variation：即点云的密度变化，稀疏的点云和密集的点云对模型精度的影响比较大，模型应该能对这种变化鲁棒才行）</p><h3 id="思想-1"><a href="#思想-1" class="headerlink" title="思想"></a>思想</h3><p>模仿CNN的做法（分层级的特征提取）</p><h3 id="实现-1"><a href="#实现-1" class="headerlink" title="实现"></a>实现</h3><ol><li><p>Hierarchical Point Set Feature Learning using <code>set abstraction levels</code></p><p>set abstraction levels由Sampling layer、Grouping layer和PointNet layer组成</p><ol><li><p>Sampling layer</p><p>作用：确定local regions的centroid位置</p><p>算法：farthest point sampling (FPS)最远点采样</p></li><li><p>Grouping layer</p><p>作用：确定centroid的neighborhood</p><p>算法（选其一）：</p><ul><li><p>ball query（推荐）：确定球面的半径，然后在半径以内的点都作为邻居</p></li><li><p>kNN（不推荐）：排序出最近的K个点作为邻居</p></li></ul></li><li><p>PointNet layer</p><p>  作用：特征提取</p><p>  算法过程：</p><ul><li><p>先将neighborhood的坐标转换为相对于centroid的相对坐标</p></li><li><p>再使用原始版本的PointNet作特征提取，提取到的特征作为centroid的特征。原本的neighborhood则不必再用，centroid作为下一个set abstraction levels的输入。</p></li></ul></li></ol></li><li><p>Robust Feature Learning under Non-Uniform Sampling Density</p><p>刚才所谓的set abstraction levels可能对density不鲁棒，我们需要融合多尺度（multi-scale，可以理解为neighborhood数量的多少，不一定是相同centroid的）的信息来达到鲁棒性。</p><p>不是多尺度的我们叫SSG（ablated PointNet++ with single scale grouping in each level），多尺度的我们叫MSG（multi-scale grouping）或MRG（multi-resolution grouping）。</p><p><img src="2_1.png" alt></p><p>我们提出了两种多尺度融合方式：MSG、MRG。</p><p>举个例子，MSG就是分别使用9999个、999个、99个neighborhood所提取到的features作concatenation所得到的features的方式；而MRG就是使用3个99neighborhood接着串联用1个99neighborhood再cancat（并联）直接使用999个neighborhood得到的特征。</p><p>效果上MSG慢但是精度高，MRG快但是精度不高。</p><p>注意：作者训练时使用了DropPoints（DP）技术（randomly dropping out input points with a randomized probability for each instance），但是model的density variation是否一部分得益于此（data的augmentation）不得而知。</p></li><li><p>训练流程</p></li></ol><p><img src="http://stanford.edu/~rqi/pointnet2/images/pnpp.jpg" alt></p><p>对于segmentation，还要对feature插值回原图的分辨率才能对每个点进行分类。</p><p>而classification则直接fc+softmax就好了。</p><h2 id="KCNet"><a href="#KCNet" class="headerlink" title="KCNet"></a>KCNet</h2><blockquote><p>CVPR2018《Mining Point Cloud Local Structures by Kernel Correlation and Graph Pooling》</p><p>paper下载：<a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Shen_Mining_Point_Cloud_CVPR_2018_paper.html" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2018/html/Shen_Mining_Point_Cloud_CVPR_2018_paper.html</a></p></blockquote><h3 id="概括-2"><a href="#概括-2" class="headerlink" title="概括"></a>概括</h3><ul><li><p>作者针对PointNet对local features提取能力弱的缺点（然而PointNet++速度慢），提出使用Kernel Correlation + Graph Pooling的方法来更有效地提取local features。</p></li><li><p>特点：Kernel的Shape可学习（并且可视化后有直观的几何意义）；不再使用handcrafted的features（如法向量、协方差矩阵）学习了</p></li><li><p>有益效果：在PointNet的基础上，能有效地提取local features</p></li><li><p>与PointNet++的比较：同样是局部化的特征提取，KCNet使用的是人为设计的度量函数，而PointNet++是自动提取特征（套用PointNet）；KCNet是对每一个点都提取了local feature，而PointNet++是只对FPS采样出来的点为centroid以及确定的邻域范围内使用PointNet来提取local feature。KCNet中有明显的模板/卷积核概念，而PointNet/PointNet++中没有明显的这一概念。</p></li><li><p>TODO IDEA：感觉point set的相似度衡量（correlation）过于依赖于选取，是否可能有自动学习这一过程，或者使这一过程更加自然？KCNet实现了Kernel，但是没有对应于stride的东西？是否可以多次地使用这种Kernel Layer而不是只用一次，类似于CNN的叠层使用一样？其中的高斯径向基的$\sigma$是否有自动学习的方法？</p></li></ul><h3 id="难点-2"><a href="#难点-2" class="headerlink" title="难点"></a>难点</h3><p>如何更加有效地提取local features？</p><p>PointNet++是将Point Cloud划分为多个分区，然后各个分区使用PointNet，以实现层次化的特征提取，那么KCNet又能怎么做呢？</p><p>PointNet的致命缺陷：PointNet只能感知点的是否存在，而不能编码/表征该点neighborhood的几何结构信息，造成局部描述能力弱。</p><h3 id="思想-2"><a href="#思想-2" class="headerlink" title="思想"></a>思想</h3><p>更加模仿CNN的做法（Learnable的PointSet模板匹配）</p><p>具体地说：</p><ul><li>我们有L个Kernel，每个Kernel有M个点，也就是说一个Kernel是一个point set，并且Kernel里点的位置都是learnable的，这就好比是我们有L个可变形的模板，或者说我们有L个可学习的“卷积核”。然后我们用数学方法人为地构造了一个叫KC(A,B)的度量函数用于度量point set A和point set B的相似程度，值越大越相似。然后这里的point set A就是Kernel，point set B就是我们model输入的point cloud里的其中一个点p的neighborhood的点集，也就是Kernel在整个point cloud上每一个点走过了一遍，因为有L个Kernel，所以一个点会得到一个L维的向量。以上就是Kernel Correlation的含义。</li><li>然后我们遍历每个点，基于该点的邻居的feature值，每个维度上取max，便是graph max pooling；将邻居的features作算术平均替换掉该点，便是graph average pooling。这就是graph pooling的含义。</li></ul><h3 id="额外的Knowledge"><a href="#额外的Knowledge" class="headerlink" title="额外的Knowledge"></a>额外的Knowledge</h3><ul><li>点云可能不止xyz坐标，或许还包括强度、法向量等信息，构成更高维的描述向量。点云只是描述Object surface的点，无Object内部的点。</li><li>处理点云的4种主流方法：<ul><li>volumetric-based：Volumetric-based approachpartitions the 3D space into regular voxels and apply 3D convolution on the voxels</li><li>patch-based：Patch-based approach parameterizes 3D surface into local patches and apply convolution over these patches</li><li>graph-based：Graph-based approach characterizes point clouds by graphs.</li><li>point-based：Pointbased approach such as PointNet directly operates on point clouds</li></ul></li></ul><h3 id="实现-2"><a href="#实现-2" class="headerlink" title="实现"></a>实现</h3><p><img src="3_2.png" alt></p><ol><li><p>kernel correlation layer（Learning on Local Geometric Structure）</p><p>所定义的度量函数为（我将论文的式子换了个形式好看些）：</p><script type="math/tex; mode=display">KC(K,p) = \frac{1}{|N|}\sum_{p_K\in K} \sum_{p_N\in N}K_\sigma(p_K,p_N-p)</script><p>其中$K$代表Kernel（里面有$M$个点，其中的点用$p_K$表示）；$N$代表点$p$的邻居（不包含点$p$），邻居点记作$p_N$；$|N|$表示点$p$邻居的个数。</p><p>其中的$p_N-p$就是表示相对坐标，暗示Kernel应该是以原点为centroid的。</p><p>其中$K_\sigma$选用高斯径向基函数，那么最相似时这个$KC(K,p)$的值就是$M$（最大值），最小值就是$0$。</p></li><li><p>graph-based pooling layer（Learning on Local Feature Structure）</p><p>作者借助了一个数据结构，他起名叫“KNNG（K最近邻图）”，用来存储每一个点的邻居是谁，使用KNN算法得出邻居成员。</p><p>图结构的Pooling起到了feature aggregation的作用，据作者声称加强了特征的鲁棒性。</p><ul><li><p>Graph max pooling就是neighborhood特征的最大值（各个通道上）</p></li><li><p>Graph average pooling的直观解释：就是neighborhood特征的算术均值。原文中描述比较复杂，直观地说就是这样：</p><p><img src="3_1.png" alt></p><p>值得注意的是，pooling只是以邻居的特征计算后赋给自己。不知道为什么不算上自己的特征？？</p></li></ul></li></ol><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>速度快（只比PointNet稍慢），精度还算高（虽然不是最高的）</p><p>鲁棒性（随机替换point为噪点）：graph max pooling比graph average pooling更鲁棒；单纯使用graph max pooling比结合着使用graph max pooling + kernel correlation鲁棒性能更好，只使用kernel correlation鲁棒性能最差。</p><p>注意：作者训练时没有用data augmentation的方法</p><h2 id="SO-Net"><a href="#SO-Net" class="headerlink" title="SO-Net"></a>SO-Net</h2><blockquote><p>CVPR2018《SO-Net: Self-Organizing Network for Point Cloud Analysis》</p><p>paper下载：<a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper.html" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2018/html/Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper.html</a></p></blockquote><h3 id="概括-3"><a href="#概括-3" class="headerlink" title="概括"></a>概括</h3><ul><li>针对一些网络在处理point cloud时的缺点，如：不能对点的空间分布进行建模（例如PointNet++，只是能获取局部信息不能得到局部区域之间的空间关系），提出了SO-Net。SO的含义是利用Self-organizing map的Net。</li><li>结果：它具有能够对点的空间分布进行建模、层次化特征提取、可调节的感受野范围的优点，并能够用于多种任务如重建、分类、分割等等。取得了相似或超过SOTA的性能，因为可并行化和架构简单使得训练速度很快。</li><li>贡献：<ul><li>We design a permutation invariant network - the SO-Net that explicitly utilizes the <strong>spatial distribution of point clouds</strong>.</li><li>With point-to-node kNN search on SOM, hierarchical feature extraction is performed with systematically <strong>adjustable receptive field overlap</strong>（可调节的、感受野之间有交叠）.</li><li>We propose a <strong>point cloud autoencoder as pre-training</strong> to improve network performance in various tasks.</li><li>Compared with state-of-the-art approaches, similar or better performance is achieved in various applications with <strong>significantly faster training speed</strong>.</li></ul></li><li>TODO IDEA：作者发现将CNN直接用于SOM图上性能不升反降，为什么（推测：可能是SOM的2D map并不是保持了原本的空间对应关系，可能nodes之间是乱序的，导致用conv2d时精度反而降低）？</li></ul><h3 id="难点-3"><a href="#难点-3" class="headerlink" title="难点"></a>难点</h3><p>如何对local regions之间的空间关系进行建模？举个例子，即怎么显式地知道region A在region B的左边？</p><h3 id="额外的Knowledge-1"><a href="#额外的Knowledge-1" class="headerlink" title="额外的Knowledge"></a>额外的Knowledge</h3><ol><li><p>该文Related Work里对点云处理的“综述”比较详尽，怒赞！！！</p></li><li><p>网上找的SOM简介：<a href="https://www.cnblogs.com/surfzjy/p/7944454.html" target="_blank" rel="noopener">【机器学习笔记】自组织映射网络（SOM）</a></p></li></ol><h3 id="思想-3"><a href="#思想-3" class="headerlink" title="思想"></a>思想</h3><ul><li>点云的空间分布编码：使用SOM中nodes自带的拓扑结构实现</li><li>交叠感受野且可调：使用“point-to-node kNN search”，使得感受野大概率交叠，并且感受野大小受超参数控制。</li><li>无序化：使用作者提出改进的SOM即“Permutation Invariant SOM”，使得SOM的result对输入数据的顺序弱化至无。</li></ul><h3 id="做法"><a href="#做法" class="headerlink" title="做法"></a>做法</h3><ol><li><p>Permutation Invariant SOM</p><p>使用2维的SOM。注意2维的SOM并不是指nodes的features的dimension是2，而是指nodes的拓扑排列是2维正方形地排列。实质上nodes的features的dimension可以是任意维度，当是三维时就好比是学习point cloud的“聚类中心”。</p><p>传统SOM不能真正实现对输入数据的顺序不依赖性，需要作出修改：</p><ul><li>Assign fixed initial nodes for any given SOM configuration</li><li>Perform one update after accumulating the effects of all the points.（batch update）</li></ul></li><li><p>Encoder Architecture</p><p>记号：</p><ul><li><p>point cloud（其元素简称point） $P=\{p_i\in \mathbb{R}^3,i=0,…,N-1\}$</p></li><li><p>SOM nodes（其元素简称node） $S=\{s_j\in \mathbb{R}^3,j=0,…,M-1\}$</p></li></ul><ol><li><p>point-to-node kNN search</p><p>在$S$中寻找$p_i$的k个最近邻：$p_i$的第$k$个邻居记作$s_{ik}$</p></li><li><p>normalization</p><p>将$P$中的点都进行normalization：$p_{ik}=p_i-s_{ik}$</p><p>那么每个点$p_i$都有k个被归一化后的版本。一共有N*k个这样被normalization后的点。</p></li><li><p>shared FC layer（作用：施加非线性，提取point features）</p><p>将$p_{ik}$输入一层FC，得到$p_{ik}^1$；若输入$l$层FC层则得到$p_{ik}^l$</p><p>shared的意思大概是对于N*k个这样的$p_{ik}$输入的都是同一个FC网络。</p><p>构造N*k个$p_{ik}$的用途：</p><ul><li>提取point features：每一个point都有3个feature vectors</li><li>形成感受野：每一个node大致有$int(\frac{k N}{M})$个feature vectors。利用这一性质，我们在后面用max pooling操作提取node features。</li></ul></li><li><p>channel-wise max pooling operation（作用：从point features中提取SOM node features）</p><p>注意这里是从MLP尾巴$p_{ik}^l$提取的max pooling。</p><p>这里$max()$函数的里面的元素的个数就是感受野的大小，可知感受野大小不是确定的，容易知道感受野大小的期望就是$\frac{k N}{M}$，所以说感受野是可调的，并且存在overlap。</p><script type="math/tex; mode=display">s_j^0 = \max(\{ p_{ik}^l ,\forall s_{ik}=s_j \})</script><p>原本$s_j$是3维向量，现在max pooling后额外得到了维度未知的向量$s_j^0$（维度取决于MLP最后一层的输出维度）。</p><p>node concatenation：然后可以将两者concat起来得到$[s_j,\;s_j^0]$，feature的维度更长了，融合了不同层次的信息。</p><p>注意到“shared FC layer + channel-wise max pooling operation”好比就是一个small PointNet，对“mini point cloud”进行编码。</p></li></ol></li></ol><p><img src="4_1.png" alt></p><p>网络结构如上。</p><p>总地来说就是，先计算SOM得到$s_j$，然后计算$p_{ik}$，接着将$p_{ik}$输入MLP得到变换后的$p_{ik}^l$，接着对$p_{ik}^l$作max pooling得到$s_j^0$，与原本的cancat使得node features“变长”（在图中表示就是那个网格拥有depth了），接着对SOM nodes作MLP再max pooling得到global feature。</p><p>segmentation与classification略去不说。</p><ol><li><p>Autoencoder</p><p>In this section, we design a decoder network to recover the input point cloud from the encoded global feature vector.</p><p><img src="4_2.png" alt></p></li><li><p>Experiments结果分析</p><p>略</p></li></ol><h2 id="Using-Local-Spectral-Graph-Convolution"><a href="#Using-Local-Spectral-Graph-Convolution" class="headerlink" title="Using Local Spectral Graph Convolution"></a>Using Local Spectral Graph Convolution</h2><blockquote><p>ECCV2018《Local Spectral Graph Convolution for Point Set Feature Learning》</p><p>paper下载：<a href="http://openaccess.thecvf.com/content_ECCV_2018/html/Chu_Wang_Local_Spectral_Graph_ECCV_2018_paper.html" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_ECCV_2018/html/Chu_Wang_Local_Spectral_Graph_ECCV_2018_paper.html</a></p></blockquote><h3 id="概括-4"><a href="#概括-4" class="headerlink" title="概括"></a>概括</h3><p>在局部图的区域上使用“图卷积”提取local features（将局部点图的图卷积转换到频域上进行操作），提出了fancy的池化操作（多次使用clustering+pooling进行的池化操作）。</p><h3 id="思想-4"><a href="#思想-4" class="headerlink" title="思想"></a>思想</h3><p>利用图卷积的方式取代PointNet进行local regions的features extraction。</p><p>提出使用先划分子区域（clustering）再pooling的池化方法，感觉像是分组卷积的思想？</p><h3 id="实现-3"><a href="#实现-3" class="headerlink" title="实现"></a>实现</h3><p><img src="5_1.png" alt></p><ol><li><p>使用FPS计算出centroids，kNN计算出local regions。</p></li><li><p>接着使用频域的图卷积，以及多次使用clustering+pooling进行池化。</p><p><img src="5_2.png" alt></p><p>频域图卷积过程如上图所示。</p><p>局部点图一共有k个点。其中：</p><p>$X$是输入特征矩阵（一共有k行，每一行是该点的特征向量，特征向量维度为m）</p><p>$G$是spectral modulation matrix，$W$是weights of the feature filter（好像是图的邻接矩阵？）</p><p>图的傅里叶变换：首先计算拉普拉斯矩阵$L=I-D^{-1/2}WD^{-1/2}$，然后对$L$作特征值分解得到特征向量矩阵$U$，那么正傅里叶变换就是$\tilde{X}=U^TX$，反傅里叶变换就是$X=U\tilde{X}$</p><p>池化：We partition the Fiedler vector to perform spectral clustering in a local k-NN. We alternate between max pooling and average pooling between different recurrences.</p></li><li><p>多次重复步骤2，最终获取特征用于分类/分割。</p></li></ol><h2 id="PointCNN"><a href="#PointCNN" class="headerlink" title="PointCNN"></a>PointCNN</h2><blockquote><p>NIPS2018《PointCNN: Convolution On X-Transformed Points》</p><p>paper下载：<a href="http://papers.nips.cc/paper/7362-pointcnn-convolution-on-x-transformed-points" target="_blank" rel="noopener">http://papers.nips.cc/paper/7362-pointcnn-convolution-on-x-transformed-points</a></p></blockquote><h3 id="概括-5"><a href="#概括-5" class="headerlink" title="概括"></a>概括</h3><p>作者声称提出了一种“generalization of typical CNNs to feature learning from point clouds”，因而称作“PointCNN”（我个人觉得人造痕迹比较多），并且接近/达到了SOTA的性能。</p><p>主要流程是：用kNN划分regions，然后对neighborhood的feature matrix进行transform，接着与kernel matrix进行conv；多层这样的“conv层”堆叠，最后得到特征输出。</p><p>但是需要注意的是：作者发现矩阵$X$并没有达到“permutation equivalence”的效果；而且作者训练时使用了augmentation的方法，包括shuffle输入数据点的顺序。</p><p>这里的“卷积”是真正传统意义上的卷积（元素间按位相乘再相加）；不同于KCNet提出的3D Kernel大法，不同于图结构的谱图卷积等等。</p><h3 id="思想-5"><a href="#思想-5" class="headerlink" title="思想"></a>思想</h3><p>想直接弄个conv类似的linear conv kernel，发现conv对输入数据的order sensitivity。于是想办法“纠正”输入数据的order，本想用“Permutation matrix”乘以它就好了，可能是ideal的Permutation matrix很难弄到，所以提出弄了个矩阵$X$，美言称能实现“simultaneously weight and permute the input features”，然后实在不知道怎么弄了，就让矩阵$X$自己从输入点集的坐标里学习吧（使用MLP），然后甚至还将点集坐标nonlinear transform后cancat进原本的features。最后施加矩阵$X$，最后卷积。</p><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>传统卷积（暂不考虑kernel的滑动，只考虑在一个location上）可以表示为（这里的conv实际上是matrices间element-wise的product，最后sum）：</p><script type="math/tex; mode=display">f = conv(K,[f_a,f_b,f_c,f_d]^T)</script><p>其中$K$表示卷积核，$f_a$等表示像素的特征向量（横向量，$f_a\in \mathbb{R}^{C_1}$）。上式表示2*2的kernel在一个location上的computation。</p><p>推广开来，在点云上，假设有4个points，然后做“卷积”，式子跟上面是一样的。但是对点云的输入顺序敏感，即$[f_a,f_b,f_c,f_d]^T$与$[f_b,f_a,f_c,f_d]^T$的结果会不一样，所以我们需要对它进行transform再输进去。我们这里选择用一个矩阵$X$与它相乘（矩阵有“置换矩阵”可以实现对输入顺序的矫正，推广到一般矩阵则相当于“置换+加权”的效果）。即：</p><script type="math/tex; mode=display">f = conv(K,X \times [f_a,f_b,f_c,f_d]^T)</script><p>这个矩阵$X$是由输入数据通过MLP生成的。</p><p>在PointCNN中，一次computation（只在一个location上，不“滑动”）的过程就是：</p><script type="math/tex; mode=display">F_p = conv(K,MLP(P-p) \times [MLP_\delta(P-p),F])</script><p>其中$K$是kernel matrix，$P$是输入点集的空间坐标（在region中，如果kernel覆盖了4个点，那么$P$的元素个数就是4），$p$是输出点的空间坐标（类似于该region的centroid，只有一个，$p\in P$），$F$是输入点集的feature matrix（与$P$的元素个数相同），$MLP()$和$MLP_\delta()$是多层感知机函数。</p><p>handcrafted的流程解释：</p><ul><li><p>转化到相对坐标系中$P-p$</p></li><li><p>矩阵$X$通过输入点集的空间坐标学习得来：$X=MLP(P-p)$</p></li><li>将输入点集维度升高，并与点集的特征向量进行cancat，作为最终的特征向量：$[MLP_\delta(P-p),F]$</li><li>进行“置换+加权”：$MLP(P-p) \times [MLP_\delta(P-p),F]$</li><li>最后进行常规的卷积操作得到$f$</li></ul><h3 id="实现-4"><a href="#实现-4" class="headerlink" title="实现"></a>实现</h3><ol><li><p>Hierarchical Convolution</p><p><img src="6_1.png" alt></p><p>模拟CNN，作者对图结构进行information的aggregation。</p></li><li><p>X-Conv Operator</p><p>作者的卷积结构称作“X-Conv”。我觉得直接从点云坐标学习得到矩阵$X$似乎是个很难的任务，感觉$X$矩阵并没有达到预期目的。</p><script type="math/tex; mode=display">F_p = conv(K,MLP(P-p) \times [MLP_\delta(P-p),F])</script></li><li><p>PointCNN Architectures</p><p><img src="6_2.png" alt></p><p>通过多层的堆叠实现特征提取。</p></li></ol><h2 id="A-CNN"><a href="#A-CNN" class="headerlink" title="A-CNN"></a>A-CNN</h2><blockquote><p>CVPR2019《A-CNN: Annularly Convolutional Neural Networks on Point Clouds》</p><p>paper下载：<a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Komarichev_A-CNN_Annularly_Convolutional_Neural_Networks_on_Point_Clouds_CVPR_2019_paper.html" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_CVPR_2019/html/Komarichev_A-CNN_Annularly_Convolutional_Neural_Networks_on_Point_Clouds_CVPR_2019_paper.html</a></p></blockquote><h3 id="概括-6"><a href="#概括-6" class="headerlink" title="概括"></a>概括</h3><p>使用循环卷积作为点云的特征提取方式：对点集投射到2D平面上进行圆周排序再输入到循环卷积中提取特征，输出的点的数量是上一级的centroids数量，多级嵌套实现“层次化”特征提取。</p><p>效果：提取局部几何结构特征（感觉是很明显的人工设计流程），adapt to the geometric variability and scalability at the signal processing level（文中没论述，不知道怎么看出来的），超过了SOTA（其实只是部分指标超过了SOTA）</p><p>BTW，文章Related Work部分的综述部分写的不错</p><p>TODO IDEA：为什么基本都需要kNN，可以不用吗？是否可以以平面极坐标的形式对投影p_j进行了排序后进行输入，而不需要划分ring再cos angle sort？？或者多个rings的特征组成更高维的tensor再滑动卷积？</p><h3 id="思想-6"><a href="#思想-6" class="headerlink" title="思想"></a>思想</h3><p>其实Annular Convolution的整个目的是为了实现neighborhood的<strong>无序化</strong>。在同一个ring上的确实现了无序化（从3D space project到2D plane，然后用angle的方式一一映射到1D line，最后使用1D circular convolution即可实现邻域特征的融合；同时ring的半径对应了感受野）</p><p>模仿PCA将points变动最大的方向保留（向切平面投影），感知centroid在距离远近上的特征（例如城市的内环特征、中环特征、外环特征分别是繁荣、住宅区、郊区这样子；再将这些“环”特征concat起来，作为这座城市的特征），而在“环”区域里面的顺序则是通过余弦角排序后输入循环卷积里面实现的（思路不错！）</p><p>主要创新点：将邻居点投射到2D平面上，然后对2D平面划分出多个不重叠的区域（它是同心圆环区域），然后指定一个标准点，与其计算余弦夹角后排序，从而得到点的圆周顺序排列，输入到循环卷积中提取特征；不同同心圆环的特征concat起来，再在所有邻居点之间作max pooling。</p><h3 id="实现-5"><a href="#实现-5" class="headerlink" title="实现"></a>实现</h3><ol><li><p>Regular and Dilated Rings on Point Clouds And Constraint based KNN Search</p><p><img src="7_1.png" alt></p><p>将平面空间划分为不重叠的区域，不重叠会减少计算量和避免特征冗余。如果是普通的ball query则大的感受野必然会包含小的感受野。作者使用环形的方式，甚至模拟空洞卷积使用带间隔空环的方式划分平面区域（增大了感受野）。最上面的圆形平面代表平面区域划分，中间和最下面的平面代表将最上面的解体开给你看的，说明没有作者提出的划分方式（Rings）没有重叠。</p><p>Constraint based KNN Search：只是将区域受限，进行的KNN算法。例如原本是$r&lt;R$范围内的，现在可能是$R_1&lt;r&lt;R_2$范围内的。</p><p>centroid使用FPS最远点采样算法获得，邻居使用Constraint based KNN Search获得。</p><p>做完这一步我们就可以得到多个centroid和其对应的neighborhood了（区域划分完成）。</p></li><li><p>Ordering Neighbors</p><ol><li><p>Normal Estimation on Point Clouds</p><p>首先估计centroid的法向量：首先沿着法向量，邻居的位置变化肯定是最小的。所以法向量就是邻居点的相对坐标的协方差矩阵最小的特征值所对的特征向量的方向。</p><p>似乎法向的方向颠倒之后得到的排序后的点的顺序也会相反，但好像只要使得法向始终与某一方向的夹角小于90°就好了？？？</p><p>有了法向量就可以估计切平面。</p></li><li><p>Orthogonal Projection</p><p>将邻居点向切平面投影，得到位于切平面上的投影点$p_j,k\in\{1,…,K\}$。</p></li><li><p>Counterclockwise Ordering</p><p>随便指定一个投影点作为参考点，例如就选$p_1$吧。记centroid为$q$。</p><p>那么以此计算$p_j-q$与$p_1-q$夹角的“余弦值”（借助叉积可以辨别是位于圆的上半部分还是下半部分，从而可以拓展到0~360°的识别范围，所以“余弦值”被我们拓展到$(-3,1]$的区间上了），将“余弦值”降序排列，那么就能对邻居点实现逆时针的排序了</p></li></ol></li><li><p>Annular Convolution on Rings And Pooling on Rings</p><p><img src="7_2.png" alt></p><p>将排序后的邻居点$[x_1,x_2,…,x_K]$输入到循环卷积里进行特征提取。</p><p>如果卷积核的大小是3，那么循环卷积相当于对$[x_1,x_2,…,x_K,x_1,x_2]$进行卷积核为3的普通卷积。</p><p>因为采用了循环卷积，所以参考点（$p_1$）到底是谁就不重要了。</p><p>将不同rings的特征concat，然后在所有邻居点之间做maxpooling。然后再把得到的东西再次送入类似的结构作“层次化提取”。</p></li><li><p>A-CNN Architecture</p><p>整体结构如下：</p><p><img src="7_3.png" alt></p><p>它的Segmentation Network上采样插值部分与之前某个网络有点类似（The interpolation method is based on the inverse squared Euclidean distance weighted average of the three nearest neighbors），不再详述。</p></li></ol><h2 id="PointConv-👍"><a href="#PointConv-👍" class="headerlink" title="PointConv 👍"></a>PointConv 👍</h2><blockquote><p>CVPR2019《PointConv: Deep Convolutional Networks on 3D Point Clouds》</p><p>paper下载：<a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Wu_PointConv_Deep_Convolutional_Networks_on_3D_Point_Clouds_CVPR_2019_paper.html" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_CVPR_2019/html/Wu_PointConv_Deep_Convolutional_Networks_on_3D_Point_Clouds_CVPR_2019_paper.html</a></p></blockquote><h3 id="概括-7"><a href="#概括-7" class="headerlink" title="概括"></a>概括</h3><p>这篇文章将卷积比较自然地拓展到点云的情形，思路很赞！</p><p>文章的主要创新点：“weight function”和“density function”，并能实现translation-invariance和permutation-invariance，可以实现层级化特征提取，而且能自然推广到其deconvolution的情形实现分割，在二维CIFAR-10图像分类任务中精度堪比CNN（表明能够充分近似卷积网络），达到了SOTA的性能。</p><p>缺点：每个kernel都需要由“kernel function”生成，而“kernel function”实质上是一个CNN网络，计算量比较大。</p><h3 id="思想-7"><a href="#思想-7" class="headerlink" title="思想"></a>思想</h3><p>察觉到：二维卷积中pixel的相对centroid位置与kernel vector的生成方式有关。</p><p>以二维卷积为例说明一下如何将卷积拓展到点云。这里只考虑使用一个kernel在一个location的一次卷积操作。</p><p>对于二维图像，我们可以将图像的pixels看作是一个点，那么图像就是整齐排列的点阵。每个point都有维度为$C_{in}$的特征向量（相当于图片是多通道的，一个通道对应于特征向量里的一个位置）。在二维卷积中，我们的kernel是“滑动”的，即我们在多个location的计算中共用一个kernel参数。这是为什么呢？我们如果认为这种共享参数的形式得益于整齐排列的points，那么这就是我们的PointConv的出发点了。</p><p><img src="8_1.png" alt></p><p>现在我们考虑一个point位置上的kernel是什么，在传统2D卷积中，这个值就是一个vector（$C_{in}$尺寸），如果points是正方形（2*2）整齐排列，那么我们可以认为kernel是$2\times2\times C_{in}$的尺寸。但是我们现在不这么认为，我们认为kernel的尺寸是$K\times C_{in}$（其中$K$是输入点的个数，这里是4），这样有助于推广到点云的情形。</p><p>在2D图像中，我们不妨可以认为kernel是这样生成的：kernel在一个point上的值（vector），取决于point的位置。那么在2D图像中，这个生成vector的分布，则是4点的狄拉克分布，即在空间坐标中只在这4个位置下，才有对应的向量值。</p><p>我们推广开来，假设这个分布是实数值的连续分布，不妨叫它做“weight function”，那么在3D点云中，这个“weight function”的输入是3D坐标系x、y和z，它的输出则是一个vector（尺寸$C_{in}$）。（在2D图像中，则$[f(0,0),f(1,0);f(0,1),f(1,1)]$组成了我们三维的kernel，尺寸$2\times2\times C_{in}$）。如果点的坐标是实数值（3D点云中其实就是实数值），那么我们点云的kernel尺寸就是$4\times C_{in}$，即我们每个point都有一个vector kernel与它对应。我们的计算过程就是：每个point的的feature与kernel vector做点积，最后把所有邻居点的点积的结果求和，得到的scalar就是我们一次卷积计算后的结果。</p><p>如果每个point的位置上是一个矩阵，那么相当于有多个kernel，最后得到的卷积结果不是scalar而是一个vector（有点像2D卷积中有多个卷积核的情形，输出就形成了多个channels）。</p><h3 id="实现-6"><a href="#实现-6" class="headerlink" title="实现"></a>实现</h3><ol><li><p>PointConv</p><p>在邻居点中实现卷积：</p><script type="math/tex; mode=display">F_{out} = \sum_{k=1}^K \sum_{c_{in}=1}^{C_{in}} S(k)W(k,c_{in})F_{in}(k,c_{in})</script><p>其中$W\in\mathbb{R}^{K\times(C_{in}\times C_{out})}$是$C_{out}$个kernel，$F_{in} \in \mathbb{R}^{K\times C_{in}}$是邻居的特征矩阵（一行一个），$S$是点密度的修正项（density scale），$F_{out}$是卷积后输出的vector。其中的kernel由“kernel function”生成，而“kernel function”接收x、y和z坐标输入，然后通过2D卷积（用1x1卷积核），得到一个尺寸为$K\times C_{in}\times C_{out}$的tensor，就是$W$。输出的$F_{out}$的空间坐标就是centroid的坐标。</p><p><img src="8_2.png" alt></p><p>关于密度：作者首先使用“kernel density estimation”算法估计每一个点的密度，然后将得到的密度（一个点的密度是一个实数值，有$K$个点所以是一个1d vector）丢进MLP里非线性变换（丢进MLP里是为了让网络自己决定是否采用密度估计），估计密度是为了解决点云的非均匀采样导致的非均匀密度的问题。</p><p>作者发现原始版本的PointConv实现内存占用太大，提出了改进版本，在此不详述。</p><p>PointDeconv其实就是邻域3点线性插值上采样，与前面同级的特征concat，再使用PointConv。</p><p><img src="8_3.png" alt></p><p>上图是用于segmentation的pointconv网络。特征提取的过程好像也是先用FPS采样再grouping的。</p></li></ol><h2 id="KPConv👍"><a href="#KPConv👍" class="headerlink" title="KPConv👍"></a>KPConv👍</h2><blockquote><p>ICCV2019？《KPConv: Flexible and Deformable Convolution for Point Clouds》</p><p>paper下载：<a href="https://arxiv.org/abs/1904.08889v2" target="_blank" rel="noopener">https://arxiv.org/abs/1904.08889v2</a></p></blockquote><h3 id="概括-8"><a href="#概括-8" class="headerlink" title="概括"></a>概括</h3><p>与“PointConv”和“KCNet”稍微有点类似，它也是将卷积自然推广到3D point cloud中，但是区别是生成kernel transform matrix的方式不一样，而且它的point kernel并不是用于直接度量点云之间的相似度而是用于建立分布来计算kernel transform matrix的值。简单地说是通过线性插值来得到矩阵的，而系数与点之间的距离相关。</p><p>效果：达到SOTA，可分类又可分割</p><p>注：</p><ul><li><p>KPConv == Kernel Point Convolution</p></li><li><p>我这里的kernel transform matrix指的是给定一个邻域内的点$y_i$（相对坐标），通过某一种方法得到该点相关联的特征向量$f_i$的转换矩阵$W$，其$W$就是kernel transform matrix。在一个位置上的卷积计算过程就是$output = \sum_i W_i y_i$。</p></li></ul><p>TODO IDEA：还有没有其他方法来生成这个分布？找邻居的方法是否可以更加自然一点，使得嵌入神经网络的架构中？</p><h3 id="思想-8"><a href="#思想-8" class="headerlink" title="思想"></a>思想</h3><p>与“PointConv”类似，区别是它生成kernel transform matrix的方式不一样。在前篇”PointConv”中，生成方式是使用CNN合成，输入xyz坐标自动生成一个矩阵。在这篇“KPConv”则是利用类似于插值或者说平滑的方法来计算kernel transform matrix。</p><p>那么这个“插值”的算法是怎么计算的呢？这套算法需要我们首先定义一个小点云，小点云里面有$K$个点（$K$是任意的，可以不等于邻域内的邻居数，$K$越大则描述的分布越精细），而小点云里的每个点$d_k$都有个矩阵$W_k$与之关联。现在我们考虑输入点云的centroid的邻域内的一个邻居点$x$，那么计算得到的矩阵则是在小点云周围点的矩阵$W_k$的线性表示：</p><script type="math/tex; mode=display">transform\;matrix = \sum h(d_k,x)W_k</script><p>其中的：</p><script type="math/tex; mode=display">h(d_k,x) = max(0,1-\frac{||d_k-x||}{\sigma})</script><p><img src="9_1.png" alt></p><p>意思是这个分布实质上只是由$K$个“狄拉克分布”支撑起来的，只有在这$K$个地方才有真正的值。想要获取其他地方的值就只能通过线性插值得到。线性插值的“原子”则是与之离得比较近的点所对应的矩阵$W_k$，离得太远则会由于ReLU函数的截断性质而不再作出贡献。</p><p><img src="9_4.png" alt></p><p>上图是假设kernel points只有5个，黑色星星的位置是kernel points的位置，假设weight matrix只是一个scalar，使用KPConv的插值算法得到的分布图。画图的代码见<a href="9_code_to_plot_distribution.py">这里</a>。可以看出这个算法的效果就是：相当于允许邻居点neighbor point的位置有稍微的变动，其理想的等值面是一个球形，在波动允许的范围内则使用一个接近的transform matrix值，而没有受到影响的地方则默认是零。这样带来的好处可能是使得分布平滑（猜的），使用CNN生成的方式应该不能保证分布平滑（瞎猜的）。</p><h3 id="实现-7"><a href="#实现-7" class="headerlink" title="实现"></a>实现</h3><ol><li><p>Rigid or Deformable Kernel</p><p>在“思想”中介绍的就是Rigid Kernel，意思是kernel point的位置一开始就是固定好的，是不可改变、不可学习的参数，改变的只能是其小点云上的矩阵$W_k$。Rigid Kernel的初始化方式形象地说就是假想每个kernel point都是带正电的小球相互排斥，但是同时要使得小球被束缚在一个半径尽可能小的包围球里面，包围住全部的带点小球，那么最终稳态时的小球位置就是kernel point的坐标位置。</p><p>Deformable Kernel并不是指kernel point的位置可变，然后最后共用同一个分布，因为这样没什么意义，还不如选用好的初始化方法。</p><p>而Deformable Kernel则参照了二维图像中的deformable convolution的实现方法，就是在Rigid Kernel的基础上学习一个kernel point shift的量，在固定的基础上进行变动，从而实现这$K$个“狄拉克分布”的位置都是可变的。</p><p><img src="9_2.png" alt></p><p>在使用Deformable Kernel的时候，需要在损失函数上加上正则项，用来约束kernel point和neighbor point不能离太远、kernel points之间不能太近，否则效果奇差。</p></li><li><p>网络结构</p><p><img src="9_3.png" alt></p><p>用grid subsampling下采样，用max-pooling或KPConv来做pooling</p><p>用radius neighborhoods而不是k-nearest-neighbors</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 3d </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 3d </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>理解深度学习中的各种卷积</title>
      <link href="/dl/conv-understanding/"/>
      <url>/dl/conv-understanding/</url>
      
        <content type="html"><![CDATA[<h1 id="理解深度学习中的各种卷积"><a href="#理解深度学习中的各种卷积" class="headerlink" title="理解深度学习中的各种卷积"></a>理解深度学习中的各种卷积</h1><blockquote><p>参考文献：<a href="https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215" target="_blank" rel="noopener">A Comprehensive Introduction to Different Types of Convolutions in Deep Learning</a></p></blockquote><h2 id="1-Convolution-v-s-Cross-correlation"><a href="#1-Convolution-v-s-Cross-correlation" class="headerlink" title="1. Convolution v.s. Cross-correlation"></a>1. Convolution v.s. Cross-correlation</h2><p>卷积（Convolution）与互相关（Cross-correlation）</p><p><img src="https://miro.medium.com/max/1908/1*K500B9Jdwddeh3TTlViQLg.jpeg" alt="Difference between convolution and cross-correlation in signal processing."></p><p>In Deep Learning, the filters in convolution are not reversed. Rigorously speaking, it’s cross-correlation. We essentially perform element-wise multiplication and addition. But it’s a convention to just call it convolution in deep learning. </p><h2 id="2-Convolution-in-Deep-Learning"><a href="#2-Convolution-in-Deep-Learning" class="headerlink" title="2. Convolution in Deep Learning"></a>2. Convolution in Deep Learning</h2><ol><li><p>理清概念：</p><ul><li>layers and filters are at the same level</li><li>while channels and kernels are at one level below</li><li>Channels and feature maps are the same thing</li><li>“channel” is usually used to describe the structure of a “layer”</li><li>“kernel” is used to describe the structure of a “filter”（A “Kernel” refers to a 2D array of weights. The term “filter” is for 3D structures of multiple kernels stacked together. ）</li></ul></li><li><p>卷积</p><p><img src="https://miro.medium.com/max/1000/1*Emy_ai48XaOeGDgykLypPg.gif" alt="The first step of 2D convolution for multi-channels: each of the kernels in the filter are applied to three channels in the input layer, separately."></p><p>Then these three channels are summed together（element-wise addition）to form one single channel（3 x 3 x 1）. </p><p><img src="https://miro.medium.com/max/1000/1*5otecXBNlms3lslqlYworA.gif" alt="The second step of 2D convolution for multi-channels: then these three channels are summed together（element-wise addition）to form one single channel."></p><p>上述过程又可以看作这样：（2D卷积）</p><p><img src="https://miro.medium.com/max/3280/1*oFVlkvZp848nh-QoD3pREw.png" alt="Standard 2D convolution. Mapping one layer with depth Din to another layer with depth Dout, by using Dout filters."></p><p>上述卷积被称作2D卷积是因为filter的自由度其实只有2维，不能在channel方向上移动（因为kernel size == channel size）。虽然的确实在3D volumetric data上的卷积。一次卷积操作过后得到的是一个单通道的图片。</p><blockquote><p><strong>1 x 1 Convolution</strong></p><p><img src="https://miro.medium.com/max/1446/1*deVKbCzJs_7eL6p2ltkY0g.png" alt="1 x 1 convolution, where the filter size is 1 x 1 x D."></p><p>Initially, 1 x 1 convolutions were proposed in the Network-in-network <a href="https://arxiv.org/abs/1312.4400" target="_blank" rel="noopener">paper</a>. They were then highly used in the Google Inception <a href="https://arxiv.org/abs/1409.4842" target="_blank" rel="noopener">paper</a>. A few advantages of 1 x 1 convolutions are:</p><ul><li>Dimensionality reduction for efficient computations</li><li>Efficient low dimensional embedding, or feature pooling</li><li>Applying nonlinearity again after convolution</li></ul><p><strong>Convolution Arithmetic</strong></p><p><img src="https://miro.medium.com/max/395/1*d03OGSWsBqAKBTP2QSvi3g.gif" alt></p><p>尺寸计算：</p><script type="math/tex; mode=display">\begin{align}W_{out} &= \lfloor \frac{W_{equal}-K}{S} \rfloor + 1 \\&= \lfloor \frac{W_{in}-K+(P_{left}+P_{right})}{S} \rfloor + 1\\&= \lfloor \frac{W_{in}-K+2P}{S} \rfloor + 1\end{align}</script></blockquote></li></ol><h2 id="3-3D-Convolution"><a href="#3-3D-Convolution" class="headerlink" title="3. 3D Convolution"></a>3. 3D Convolution</h2><p>   当kernel size &lt; channel size时，filter有3个自由度，一次卷积操作过后得到的是多通道的图片（3D data）</p><p>   <img src="https://miro.medium.com/max/1610/1*wUVVgZnzBwYKgQyTBK_5sg.png" alt="In 3D convolution, a 3D filter can move in all 3-direction（height, width, channel of the image）. At each position, the element-wise multiplication and addition provide one number. Since the filter slides through a 3D space, the output numbers are arranged in a 3D space as well. The output is then a 3D data."></p><p>   Similar as 2D convolutions which encode spatial relationships of objects in a 2D domain, 3D convolutions can describe the spatial relationships of objects in the 3D space.</p><h2 id="4-Transposed-Convolution（Deconvolution）"><a href="#4-Transposed-Convolution（Deconvolution）" class="headerlink" title="4. Transposed Convolution（Deconvolution）"></a>4. Transposed Convolution（Deconvolution）</h2><ul><li><p>目的：做上采样（up-sampling）</p></li><li><p>别名：transposed convolution == deconvolution（不太合适） == fractionally strided convolution</p></li></ul><p>It is always possible to implement a transposed convolution with a direct convolution.</p><p><img src="https://miro.medium.com/max/344/1*KGrCz7aav02KoGuO6znO0w.gif" alt="Up-sampling a 2 x 2 input to a 4 x 4 output."></p><p>Interestingly enough, one can map the same 2 x 2 input image to a different image size, by applying fancy padding &amp; stride.</p><p><img src="https://miro.medium.com/max/395/1*Lpn4nag_KRMfGkx1k6bV-g.gif" alt="Up-sampling a 2 x 2 input to a 5 x 5 output."></p><blockquote><p><strong>为什么叫做“转置卷积”？</strong></p><p>我们先看下卷积使用矩阵乘法怎么做（$Kernel \times Large = Small$）：</p><p><img src="https://miro.medium.com/max/1398/1*9ngOwG-uHaJO8Od0ePB-fQ.jpeg" alt="Matrix multiplication for convolution: from a Large input image（4 x 4）to a Small output image（2 x 2）."></p><p>如果我们认为$C^T C \approx I$那么$Kernel^{\, T} \times Small = Large$将可以复原输入：</p><p><img src="https://miro.medium.com/max/1424/1*zfIgQ6uyowDUhkMBBKh4bg.png" alt="Matrix multiplication for convolution: from a Small input image（2 x 2）to a Large output image（4 x 4）."></p><p>As you can see here, we perform up-sampling from a small image to a large image. That is what we want to achieve. And now, you can also see where the name “transposed convolution” comes from.</p><p><strong>Checkerboard artifacts</strong></p><p>One unpleasant behavior that people observe when using transposed convolution is the so-called checkerboard artifacts（棋盘效应）.</p><p><img src="https://miro.medium.com/max/1610/1*w08YkXF_hTBRFoQ1lhffpg.png" alt="A few examples of checkerboard artifacts."></p></blockquote><h2 id="5-Dilated-Convolution（Atrous-Convolution）"><a href="#5-Dilated-Convolution（Atrous-Convolution）" class="headerlink" title="5. Dilated Convolution（Atrous Convolution）"></a>5. Dilated Convolution（Atrous Convolution）</h2><p><img src="https://miro.medium.com/max/395/1*niGh2BkLuAUS2lkctkd3sA.gif" alt="The dilated convolution."></p><p>参数$l$（使Kernel变大的参数）：</p><ul><li>当$l=1$时，空洞卷积的Kernel就是普通的Kernel</li><li>当$l \geq 1$时，空洞卷积Kernel的Element间插入了$l-1$个间隔（以可理解为插入了零元素，增大了Kernel的尺寸，感受野计算方法与传统的计算方法一致）</li></ul><p>The following image shows the kernel size when $l = 1, 2,$ and $4$.</p><p><img src="https://miro.medium.com/max/2019/1*d3rLXrB7IfjvUR2ojBYsVw.jpeg" alt="Receptive field for the dilated convolution. We essentially observe a large receptive field without adding additional costs."></p><p>优点：不增大参数量的情况下，加大感受野（实质上是通过增大Kernel面积，然后使用时多个级联，才能使得感受野迅速增大）</p><h2 id="6-Separable-Convolutions"><a href="#6-Separable-Convolutions" class="headerlink" title="6. Separable Convolutions"></a>6. Separable Convolutions</h2><h3 id="6-1-Spatially-Separable-Convolutions"><a href="#6-1-Spatially-Separable-Convolutions" class="headerlink" title="6.1 Spatially Separable Convolutions"></a>6.1 Spatially Separable Convolutions</h3><ol><li><p>动机：</p><p>Conceptually, spatially separable convolution decomposes a convolution into two separate operations. For an example shown below, a Sobel kernel, which is a 3x3 kernel, is divided into a 3x1 and 1x3 kernel.</p><p><img src="https://miro.medium.com/max/800/1*SMOKzHFFG_9sjWAYpOb6CA.png" alt="A Sobel kernel can be divided into a 3 x 1 and a 1 x 3 kernel."></p><p>做一次 3x3 卷积，等价于先做一次 3x1 卷积再做一次 1x3 卷积。（从9个参数变到了6个参数）</p><p><img src="https://miro.medium.com/max/1187/1*7OF9tl-oRpDK_S3z-AMwnA.png" alt="Spatially separable convolution with 1 channel."></p><p>特点：</p><ul><li>计算量减少（举例：对 5x5 的feature map做 3x3 的普通卷积需要81次乘法，而空间分离卷积只需要72次乘法），$N \times N$的feature map用$m \times m$ 的Kernel作卷积，普通卷积需要$(N-2)^2m^2$次乘法，而空间分离卷积只需要$2m(N-1)(N-2)$次乘法，节省了$\frac{2}{m}+\frac{2}{m(N-2)}$倍的乘法计算量。（当特征图较大的时候，对于$m \times m$大的Kernel，可以使计算量变为$\frac{2}{m}$倍）</li><li>在深度学习中的应用不多：不是所有的Kernel都可以这样分解，使用空间分离卷积的结果可能会欠优。</li><li>将2D的Kernel分解为2个1D的Kernel</li><li>Kernel的rank是1</li></ul></li></ol><blockquote><p><strong>Flattened convolutions</strong></p><ol><li><p>动机：</p><p>与Spatially Separable Convolutions类似，我们将一个3D的Kernel分解为3个1D的Kernel。</p><p><img src="https://miro.medium.com/max/1330/1*dffVi2XjXA-6zAAOh4UWPw.png" alt></p></li><li><p>特点：</p><ul><li>进一步减少参数量</li><li>Kernel的rank是1</li></ul></li></ol></blockquote><h3 id="6-2-Depthwise-Separable-Convolutions"><a href="#6-2-Depthwise-Separable-Convolutions" class="headerlink" title="6.2 Depthwise Separable Convolutions"></a>6.2 Depthwise Separable Convolutions</h3><p>深度可分离卷积的步骤（10275 multiplications）：</p><ol><li><p>先变小（depthwise convolution）：将Layer和Kernel在深度方向上分离，做一次2D卷积，然后在通道方向上concat，得到深度不变、宽长变小的Layer</p></li><li><p>再加深（1x1 convolution）：做128次$1 \times 1$的卷积得到宽长不变、深度为128的Layer</p></li></ol><p><img src="https://miro.medium.com/max/3244/1*jqNtWIEJ_lIRrp9nohquLQ.png" alt="The overall process of depthwise separable convolution."></p><p>即：</p><script type="math/tex; mode=display">depthwise \; separable \; convolutions = depthwise \; convolution \; + \; 1 \times 1 \; convolution</script><p>典型的网络有 <a href="https://arxiv.org/abs/1704.04861" target="_blank" rel="noopener">MobileNet</a>、<a href="https://arxiv.org/abs/1610.02357" target="_blank" rel="noopener">Xception</a></p><p>对比普通的卷积（86400 multiplications）：</p><p><img src="https://miro.medium.com/max/1995/1*VzwZ3Igv9KL-TCV-ZkB4Dg.png" alt="Standard 2D convolution to create output with 128 layer, using 128 filters."></p><p>特点：</p><ul><li><p>乘法数节省比：$\frac{1}{N_C}+\frac{1}{h^2}$，其中$N_C$是输出Layer的深度，$h$是Kernel的尺寸。（一般而言，输出通道数一般很大，使用 3x3 卷积核将减少到$1/9$倍的参数量）</p></li><li><p>对于small model，使用它，反而会减少模型容量（容易欠拟合）；适当地利用则会提高效率，而不会降低很多精度。</p></li></ul><h2 id="7-Grouped-Convolution"><a href="#7-Grouped-Convolution" class="headerlink" title="7. Grouped Convolution"></a>7. Grouped Convolution</h2><p>分组卷积示意图（分组$g=2$）：</p><p><img src="https://miro.medium.com/max/3315/1*dBrsVP0nt_PrBlICSBTttg.png" alt="Grouped convolution with 2 filter groups."></p><p>步骤：</p><ol><li><p>将普通的Kernel拆成$g$段，每一段有$D_{out}/g$个。对Layer来说同理。</p></li><li><p>这就相当于变成了$g$个卷积操作，我们分组地卷积。得到$g$个卷积后的结果（每个结果的深度为$D_{out}/g$）。</p></li><li><p>将结果在channel方向上concat起来，就恰好得到$D_{out}$深度的Layer</p></li></ol><p>典型的网络有： <a href="https://arxiv.org/abs/1611.05431" target="_blank" rel="noopener">ResNeXt</a></p><p><img src="https://miro.medium.com/max/800/1*noJNdoKKBRvDTwUuGBD2aw.png" alt="ResNeXt"></p><p>注：information只是在分组的区域内交互，并没有在所有channels上都进行交互融合，导致学得的特征也具有分组的特性，阻挡了不同分组之间的“信息流”。</p><p><img src="https://blog.yani.io/assets/images/posts/2017-08-10-filter-group-tutorial/alexnetfilters.png" alt="AlexNet conv1 filter separation: as noted by the authors, filter groups appear to structure learned filters into two distinct groups, black-and-white and colour filters."></p><blockquote><p><strong>Grouped convolution v.s. depthwise convolution</strong></p><p>当grouped convolution中的$g=D_{in}=D_{out}$时，就是depthwise convolution了</p><p>分组卷积的优点：</p><ul><li>model-parallelization：不同的分组可以在不同的GPU上面跑（模型并行），可以一次训练更多的图片。（The model-parallelization is considered to be better than data parallelization. ）</li><li>more efficient：参数量变为原来的$1/g$</li><li>may provide a better model： In reducing the number of parameters in the network in this salient way, it is not as easy to over-fit, and hence a regularization-like effect allows the optimizer to learn more accurate, more efficient deep networks. 详情：<a href="https://blog.yani.io/filter-group-tutorial/" target="_blank" rel="noopener">A Tutorial on Filter Groups（Grouped Convolution）</a></li></ul><p><strong>Shuffled Grouped Convolution</strong></p><p>由<a href="https://arxiv.org/abs/1707.01083" target="_blank" rel="noopener">ShuffleNet</a>（主要用于移动端部署）提出的概念，为了加强不同group之间的信息流动</p><p>Overall, the shuffled grouped convolution involves grouped convolution and channel shuffling.</p><p>The idea of channel shuffle is that we want to mix up the information from different filter groups. In the image below, we get the feature map after applying the first grouped convolution GConv1 with 3 filter groups. Before feeding this feature map into the second grouped convolution, we first divide the channels in each group into several subgroups. The we mix up these subgroups.</p><p><img src="https://miro.medium.com/max/1542/1*p_JIlXpJop8n6co36Ud2IA.png" alt="Channel shuffle."></p><p>After such shuffling, we continue performing the second grouped convolution GConv2 as usual. But now, since the information in the shuffled layer has already been mixed, we essentially feed each group in GConv2 with different subgroups in the feature map layer（or in the input layer）. As a result, we allow the information flow between channels groups and strengthen the representations.</p><p><strong>Pointwise grouped convolution</strong></p><p>The pointwise grouped convolution, as the name suggested, performs group operations for 1 x 1 convolution.（分组卷积 + 使用1*1卷积核）</p><p>计算速度更快</p><p>In the ShuffleNet paper, authors utilized three types of convolutions we have learned: (1) shuffled grouped convolution; (2) pointwise grouped convolution; and (3) depthwise separable convolution.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> dl </category>
          
      </categories>
      
      
        <tags>
            
            <tag> dl </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《Deep Learning》读书笔记（Ch4.NumericalComputation）</title>
      <link href="/dl/dlbooknotes4/"/>
      <url>/dl/dlbooknotes4/</url>
      
        <content type="html"><![CDATA[<h1 id="4-Numerical-Computation"><a href="#4-Numerical-Computation" class="headerlink" title="4. Numerical Computation"></a>4. Numerical Computation</h1><h2 id="4-1-Overflow-and-Underflow"><a href="#4-1-Overflow-and-Underflow" class="headerlink" title="4.1 Overflow and Underflow"></a>4.1 Overflow and Underflow</h2><ol><li><p>Underflow：十分靠近零的数，被舍入到零</p><ul><li>例如$\frac{1}{x}$的除数在负数、正数、零的地方表现都不同；例如$log(x)$不能输入零</li></ul></li><li><p>Overflow：溢出（趋近正无穷/负无穷）</p><ul><li>例如softmax的计算（原式）：<ul><li>当$x_j$很负时，分母接近零，可能会underflow</li><li>当$x_j$很正时，各项非常大，可能会overflow</li><li>变式就不会出现underflow/overflow了，使得数值计算稳定；目前也有一些软件包（例如Theano）可以自动检测和修复数值稳定性</li></ul></li></ul><script type="math/tex; mode=display">\begin{align}softmax(x)_i & = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}} \quad （原式） \\& = \frac{e^{x_i-max(x)}}{\sum_{j=1}^{n} e^{x_j-max(x)}} \quad （变式）\end{align}</script></li></ol><h2 id="4-2-Poor-Conditioning"><a href="#4-2-Poor-Conditioning" class="headerlink" title="4.2 Poor Conditioning"></a>4.2 Poor Conditioning</h2><ol><li><p>条件数：</p><script type="math/tex; mode=display">condition \; number = \max_{i,j} |\frac{\lambda_i}{\lambda_j}|</script><ul><li>条件数越大，$A^{-1}$对输入就越敏感（即解方程$Ax=b$的解$x$误差很大），小的输入扰动对输出的影响很大</li></ul></li></ol><h2 id="4-3-Gradient-Based-Optimization"><a href="#4-3-Gradient-Based-Optimization" class="headerlink" title="4.3 Gradient-Based Optimization"></a>4.3 Gradient-Based Optimization</h2><ol><li><p>Steepest descent（gradient descent）：</p><script type="math/tex; mode=display">x^{'} = x - \epsilon \nabla_x f(x)</script></li><li><p>Line search：从几个待定的$\epsilon$中选出使得$f(x-\epsilon\nabla_x f(x))$值最小的$\epsilon$作为结果</p></li><li><p>Hill climbing：上升离散参数的目标函数</p></li></ol><h3 id="4-3-1-Beyond-the-Gradient-Jacobian-and-Hessian-Matrices"><a href="#4-3-1-Beyond-the-Gradient-Jacobian-and-Hessian-Matrices" class="headerlink" title="4.3.1 Beyond the Gradient: Jacobian and Hessian Matrices"></a>4.3.1 Beyond the Gradient: Jacobian and Hessian Matrices</h3><ol><li><p>Jacobian matrix：向量对向量求导</p><p>如果$f:\mathbb{R}^m \rightarrow \mathbb{R}^n$，那么雅可比矩阵定义为$J_{i,j}=\frac{\partial}{\partial x_j}f(x)_i（注：J \in \mathbb{R}^{n \times m}）$</p></li><li><p>Hessian matrix：标量对向量的二阶导</p><p>如果$f:\mathbb{R}^n \rightarrow \mathbb{R}$，那么海森矩阵的定义为$H_{i,j}=\frac{\partial^2}{\partial x_i \partial x_j}f(x)$</p><ul><li>当二阶偏导连续时，海森矩阵是实对称矩阵。（一般情况下都满足二阶导连续，所以一般默认海森矩阵是实对称矩阵，因而可以将$H$作特征值分解，分解为$Q \Lambda Q^T$）</li><li>在某一点的方向二阶导是$d^THd$，其中$d$是该点方向的单位向量，$H$是海森矩阵</li><li>注意到$d^THd$与$H$的特征值相关联：当$d$是$H$的第$i$个特征向量时，该点的二阶导等于第$i$个特征值；二阶导的取值范围是$[\lambda_n,\lambda_1]$。</li><li>所以海森矩阵的特征值可以近似看作二阶导（曲率）</li><li>一阶导为零且海森矩阵正定：局部最小值；一阶导为零且海森矩阵负定：局部最大值；一阶导为零且海森矩阵的特征值有正有负，则该点一定是鞍点。</li></ul></li><li><p>二阶泰勒展式：</p><script type="math/tex; mode=display">f(x) \approx f(x^{(0)}) + (x-x^{(0)})^T \mathcal{g}+\frac{1}{2}(x-x^{(0)})^TH(x-x^{(0)})</script><ul><li>如果使用一阶的梯度下降法，取学习率为$\epsilon$（更新规则$x \leftarrow x-\epsilon \mathcal{g}$），那么当取$\epsilon=\frac{\mathcal{g}^T\mathcal{g}}{\mathcal{g}^TH\mathcal{g}}$时，能一下子跳到此方向（gradient方向）的局部极小值（抛物面近似）。（英文花书Page88）</li><li>使用Newton’s method时，更新规则为$x \leftarrow x - H^{-1} \mathcal{g}$，能一下子跳到极值点（抛物面近似）。（见英文花书Page311）</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> dl </category>
          
      </categories>
      
      
        <tags>
            
            <tag> dl </tag>
            
            <tag> notes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《Deep Learning》读书笔记（Ch3.ProbabilityandInformationTheory）</title>
      <link href="/dl/dlbooknotes3/"/>
      <url>/dl/dlbooknotes3/</url>
      
        <content type="html"><![CDATA[<h1 id="3-Probability-and-Information-Theory"><a href="#3-Probability-and-Information-Theory" class="headerlink" title="3. Probability and Information Theory"></a>3. Probability and Information Theory</h1><ul><li>概率论：作出概率上的推断</li><li>信息论：定量分析不确定性</li></ul><h2 id="3-1-Why-Probability"><a href="#3-1-Why-Probability" class="headerlink" title="3.1 Why Probability?"></a>3.1 Why Probability?</h2><ol><li><p>产生不确定性的原因：</p><ul><li><p>被建模系统中本身就有的的不确定性</p></li><li><p>不完全的观测</p></li><li><p>不充分的建模</p></li></ul></li><li><p>建模过程中，更喜欢用simple but uncertain rule rather than a complex but certain one</p></li><li><p>概率的分类：</p><ul><li>Frequentist probability（频率论）：直接与事件发生的概率关联</li><li>Bayesian probability（贝叶斯）：与定性的确定性关联</li></ul></li></ol><h2 id="3-2-Random-Variables"><a href="#3-2-Random-Variables" class="headerlink" title="3.2 Random Variables"></a>3.2 Random Variables</h2><p>随机变量（that can take on different values randomly）的类型：</p><ul><li>离散型</li><li>连续性</li></ul><h2 id="3-3-Probability-Distributions"><a href="#3-3-Probability-Distributions" class="headerlink" title="3.3 Probability Distributions"></a>3.3 Probability Distributions</h2><h3 id="3-3-1-Discrete-Variables-and-Probability-Mass-Functions"><a href="#3-3-1-Discrete-Variables-and-Probability-Mass-Functions" class="headerlink" title="3.3.1 Discrete Variables and Probability Mass Functions"></a>3.3.1 Discrete Variables and Probability Mass Functions</h3><ol><li><p>概率质量函数（Probability Mass Functions = PMF）记作$ P(x) $</p><p>联合分布（Joint Probability Distribution）记作$ P(x,y) $</p><p>均匀分布（Uniform Distribution）$ P(x=x_i) = \frac{1}{k} $（with k different states）</p></li></ol><h3 id="3-3-2-Continuous-Variables-and-Probability-Density-Functions"><a href="#3-3-2-Continuous-Variables-and-Probability-Density-Functions" class="headerlink" title="3.3.2 Continuous Variables and Probability Density Functions"></a>3.3.2 Continuous Variables and Probability Density Functions</h3><ol><li><p>概率密度函数（Probability Density Functions = PDF）记作$ p(x) $</p><p>In the univariate example, the probability that x lies in the interval [a, b] is given by $ \int_{[a,b]} p(x) dx $</p><p>均匀分布$ u(x;a,b) = \frac{1}{b-a} $（The “;” notation means “parametrized by”），denote by writing $ x\sim U(a,b) $</p></li></ol><h2 id="3-4-Marginal-Probability"><a href="#3-4-Marginal-Probability" class="headerlink" title="3.4 Marginal Probability"></a>3.4 Marginal Probability</h2><ol><li>边缘概率分布：（已知联合分布，求单变量的分布）</li></ol><script type="math/tex; mode=display">P(x) = \sum_y P(x,y)\\p(x) = \int p(x,y) \; dy</script><h2 id="3-5-Conditional-Probability"><a href="#3-5-Conditional-Probability" class="headerlink" title="3.5 Conditional Probability"></a>3.5 Conditional Probability</h2><ol><li><p>条件概率：$ P(y|x) $（已知$ x $，求$ y $。相当于图像分类问题）</p><p>公式：$ P(x,y) = P(y|x) \times P(x) = P(x|y) \times P(y) $</p></li></ol><h2 id="3-6-The-Chain-Rule-of-Conditional-Probabilities"><a href="#3-6-The-Chain-Rule-of-Conditional-Probabilities" class="headerlink" title="3.6 The Chain Rule of Conditional Probabilities"></a>3.6 The Chain Rule of Conditional Probabilities</h2><ol><li><p>多个变量的联合分布，可以拆成多个条件概率的连乘积：</p><script type="math/tex; mode=display">P(a,b,c) = P(a|b,c) \times P(b|c) \times P(c)</script></li></ol><h2 id="3-7-Independence-and-Conditional-Independence"><a href="#3-7-Independence-and-Conditional-Independence" class="headerlink" title="3.7 Independence and Conditional Independence"></a>3.7 Independence and Conditional Independence</h2><ol><li><p>变量x和y独立：$ \forall x, y, \quad p(x,y)=p(x) \times p(y) $, denote as $ x \perp y $</p></li><li><p>变量x和y基于条件z独立：$ \forall x, y, z, \quad p(x,y|z)=p(x|z) \times p(y|z) $, denote as $ x \perp y \, | \, z $</p></li></ol><h2 id="3-8-Expectation-Variance-and-Covariance"><a href="#3-8-Expectation-Variance-and-Covariance" class="headerlink" title="3.8 Expectation, Variance and Covariance"></a>3.8 Expectation, Variance and Covariance</h2><ol><li>期望（expectation）：</li></ol><script type="math/tex; mode=display">\mathbb{E}_{x \sim P}[f(x)] = \sum \limits_{x} P(x) f(x)</script><script type="math/tex; mode=display">\mathbb{E}_{x \sim p}[f(x)] = \int p(x) f(x) \, dx</script><ol><li><p>方差（variance）：</p><script type="math/tex; mode=display">Var(f(x)) = \mathbb{E}[(f(x)-\mathbb{E}[f(x)])^2]</script><p>标准差（standard deviation）：$ \sqrt{Var(f(x))} $</p></li><li><p>协方差（covariance）：</p><script type="math/tex; mode=display">Cov(f(x),g(y)) = \mathbb{E}[(f(x)-\mathbb{E}[f(x)])(g(y)-\mathbb{E}[g(y)])]</script><p>The covariance gives some sense of how much two values are linearly related to each other.（只是描述线性相关程度的度量）</p><ul><li><p>协方差&gt;0时：一个变大另一个也会变大</p></li><li><p>协方差&lt;0时：一个变大另一个会变小</p></li></ul><p>相关系数（Correlation）： $ \rho_{X,Y} = \frac{Cov(X,Y)}{\sigma_X \sigma_Y} $，该系数广泛用于度量两个变量之间的<strong>线性</strong>相关程度</p><p><img src="3.png" alt></p><p>性质：</p><ul><li>$ Cov=0 \Leftarrow independent $</li><li>$ Cov \neq 0 \Leftrightarrow linear \; dependent $</li><li>$ Cov=0 \Rightarrow there \; mush \; be \; no \; linear \; dependence  $</li></ul></li></ol><p><img src="4.png" alt></p><p>   协方差矩阵：$ Cov(x)_{i,j} = Cov(x_i,x_j) $，其对角元素是方差。</p><h2 id="3-9-Common-Probability-Distributions"><a href="#3-9-Common-Probability-Distributions" class="headerlink" title="3.9 Common Probability Distributions"></a>3.9 Common Probability Distributions</h2><h3 id="3-9-1-Bernoulli-Distribution"><a href="#3-9-1-Bernoulli-Distribution" class="headerlink" title="3.9.1 Bernoulli Distribution"></a>3.9.1 Bernoulli Distribution</h3><p>做一次实验得到1维的随机变量，只有两个可能的取值（0或1），像抛硬币一样：$ P(x) = \phi^x (1-\phi)^{1-x} $（好像类似于做一次实验得到了sigmoid输出的的随机变量，即一个node只有0或1？）</p><h3 id="3-9-2-Multinoulli-Distribution"><a href="#3-9-2-Multinoulli-Distribution" class="headerlink" title="3.9.2 Multinoulli Distribution"></a>3.9.2 Multinoulli Distribution</h3><p>做一次实验得到k维的随机变量$ p=[0,1]^{k} $（意味着每一个entry只能是0或1，且只能有一个entry是1其他都是0），且满足$ \sum_i p_i = 1 $（例如投骰子$p_1=p_2=…=p_6=\frac{1}{6}$。此处的$p_i$不代表随机变量的结果，只代表这个entry取到1的概率），（好像类似于做一次实验然后得到了一个one-hot随机变量，即softmax output layer有k个nodes？）</p><h3 id="3-9-3-Gaussian-Distribution"><a href="#3-9-3-Gaussian-Distribution" class="headerlink" title="3.9.3 Gaussian Distribution"></a>3.9.3 Gaussian Distribution</h3><ol><li>高斯分布：</li></ol><script type="math/tex; mode=display">\mathcal{N}(x;\mu,\sigma^2) = \sqrt{\frac{1}{2\pi \sigma^2}}exp(-\frac{(x-\mu)^2}{2\sigma^2})</script><ul><li><p>精度（precision）：$ \beta = \frac{1}{\sigma^2} $，精度越大则高斯分布越集中</p></li><li><p>当不知道采取什么先验时，就选取高斯分布吧（因为高斯分布包含的先验知识少）</p></li><li><p>L2正则等于在MAP Bayesian inference中使用高斯分布作为prior（参见英文版花书的Page236）</p></li><li><p>多变量时：</p><script type="math/tex; mode=display">\mathcal{N}(x;\mu,\Sigma) = \sqrt{\frac{1}{(2\pi)^n det(\Sigma)}}exp(-\frac{1}{2}(x-\mu)^T \Sigma^{-1} (x-\mu))</script><p>其中$ \Sigma $是协方差阵，$ \mu $是均值向量。当然也可以写成精度矩阵的形式（此时精度矩阵$ \beta = \Sigma^{-1} $）</p></li><li><p>isotropic（各向同性）：协方差阵是scalar乘以identity matrix</p><ul><li>isotropic：像圆一样的分布</li><li>diagonal（协方差阵）：沿着轴向分布（像椭圆一样）</li><li>full-rank（协方差阵）：可以任意斜着分布（像斜着的椭圆一样，不必沿着坐标轴）</li></ul></li></ul><h3 id="3-9-4-Exponential-and-Laplace-Distributions"><a href="#3-9-4-Exponential-and-Laplace-Distributions" class="headerlink" title="3.9.4 Exponential and Laplace Distributions"></a>3.9.4 Exponential and Laplace Distributions</h3><ol><li>指数分布：</li></ol><script type="math/tex; mode=display">p(x;\lambda) = \lambda 1_{x \geq 0} exp(-\lambda x)</script><ol><li>拉普拉斯分布（L1正则等于在MAP Bayesian inference中使用isotropic Laplace distribution作为最大化log-prior term的先验，参见英文版花书的Page236）：</li></ol><script type="math/tex; mode=display">Laplace(x;\mu,\gamma) = \frac{1}{2\gamma}exp(-\frac{|x-\mu|}{\gamma})</script><p><img src="5.png" alt></p><h3 id="3-9-5-The-Dirac-Distribution-and-Empirical-Distribution"><a href="#3-9-5-The-Dirac-Distribution-and-Empirical-Distribution" class="headerlink" title="3.9.5 The Dirac Distribution and Empirical Distribution"></a>3.9.5 The Dirac Distribution and Empirical Distribution</h3><ol><li><p>狄拉克分布：$ p(x) = \delta(x-\mu) $（$ \delta $为冲激函数，积分为1）</p></li><li><p>经验分布（概率密度函数，多个冲激的和。通过采样样本点去近似真实的分布）：</p><script type="math/tex; mode=display">\hat{p}(x) = \frac{1}{m} \sum_{i=1}^m \delta(x-x^{(i)})</script></li></ol><h3 id="3-9-6-Mixtures-of-Distributions"><a href="#3-9-6-Mixtures-of-Distributions" class="headerlink" title="3.9.6 Mixtures of Distributions"></a>3.9.6 Mixtures of Distributions</h3><ol><li><p>混合分布定义：（$P(c)$是multinoulli distribution，表示从多个子分布中选取一个distribution的挑选行为。$P(x|c)$则表示被挑选到之后的子分布的贡献）</p><script type="math/tex; mode=display">P(x) = \sum_i P(c=i) P(x|c=i)</script><ul><li>Empirical Distribution是Mixtures of Distributions的一个例子之一。（此时$P(c=i)=\frac{1}{m}$，说明每个子分布被选中的概率是一样的）</li><li>通过混合分布，可以产生出复杂的概率分布。</li></ul></li><li><p>Latent variable（隐藏变量）：是一种不能被直接观测到的随机变量</p><ul><li>在混合分布中，随机变量$c$就是一个latent variable（因为混合分布你只能观测到表象，并不能观测到它的内部机制）</li></ul></li><li><p>Gaussian mixture model（高斯混合模型）：子分布都是高斯分布</p><ul><li>高斯混合模型是一个universal approximator of densities：任何光滑的分布都可以用高斯混合模型近似</li></ul></li><li><p>Prior probability：$P(c)$，未观测任何东西就能被事先确定下来</p></li><li><p>Posterior probability：$P(c|x)$，因为观测了$x$之后才被计算出来</p></li></ol><h2 id="3-10-Useful-Properties-of-Common-Functions"><a href="#3-10-Useful-Properties-of-Common-Functions" class="headerlink" title="3.10 Useful Properties of Common Functions"></a>3.10 Useful Properties of Common Functions</h2><ol><li>Logistic sigmoid：</li></ol><p><img src="6.png" alt></p><script type="math/tex; mode=display">\sigma(x) = \frac{1}{1+e^{-x}}</script><ul><li>常用于生成Bernoulli分布的参数$\phi$（二分类任务的输出概率值）</li><li>当softmax的两个node分别为0和x时，则退化为单个node输出的sigmoid情况</li></ul><ol><li>Softplus：</li></ol><p><img src="7.png" alt></p><script type="math/tex; mode=display">\zeta(x) = ln(1+e^{x})</script><ul><li>常用于产生高斯分布中的$\beta$或$\sigma$值（因为输出值大于零）</li><li>也用于softmax+nll中的合成计算</li><li>是ReLU的softened版本，plus的名字源自于$y=x$的正数部分的含义</li></ul><h2 id="3-11-Bayes’s-Rule"><a href="#3-11-Bayes’s-Rule" class="headerlink" title="3.11 Bayes’s Rule"></a>3.11 Bayes’s Rule</h2><ol><li>贝叶斯公式</li></ol><script type="math/tex; mode=display">P(x|y) = \frac{P(x)P(y|x)}{P(y)}</script><ul><li>此处的$P(y)$可以通过$P(y)=\sum_x P(y|x)P(x)$计算出来（边缘分布的计算公式）</li><li>那么我们就只需要知道$P(y|x)$和$P(x)$，就可以推测出$P(x|y)$了</li></ul><h2 id="3-12-Technical-Details-of-Continuous-Variables"><a href="#3-12-Technical-Details-of-Continuous-Variables" class="headerlink" title="3.12 Technical Details of Continuous Variables"></a>3.12 Technical Details of Continuous Variables</h2><ol><li>Measure zero：零测度（例如线在面中的测度是零）</li><li><p>Almost everywhere：几乎所有（除了可以忽略的特例之外，几乎所有）</p></li><li><p>若随机变量$x$和$y$满足$y=g(x)$，则：</p></li></ol><script type="math/tex; mode=display">p_x(x)=p_y(g(x))|det(\frac{\partial g(x)}{\partial x})|</script><h2 id="3-13-Information-Theory"><a href="#3-13-Information-Theory" class="headerlink" title="3.13 Information Theory"></a>3.13 Information Theory</h2><ol><li><p>Idea：</p><ul><li>info=0：必然事件</li><li>info很大：难以发生的事件</li><li>info有可加性时：独立事件</li></ul></li><li><p>Self-information（自信息，只对一个outcome进行uncertainty的度量）：</p></li></ol><script type="math/tex; mode=display">I(x)=-log(P(x))</script><ul><li>当使用e为底时，单位是nat；当使用2为底时，单位是bits或shannons</li></ul><ol><li>Shannon entropy（香农熵$H(P)$，自信息在distribution上的期望）：</li></ol><script type="math/tex; mode=display">H(x) = \mathbb{E}_{x \sim P} [I(x)] = - \mathbb{E}_{x \sim P} [log \, P(x)]</script><ul><li>近乎确定性的分布（outcome近乎确定发生）：low entropy</li><li>接近均匀分布（不确定性程度大）：high entropy</li><li>对于连续变量，Shannon entropy又称作differential entropy</li></ul><ol><li><p>Kullback-Leibler divergence（KL散度/相对熵）：</p><script type="math/tex; mode=display">D_{KL}(P||Q) = \mathbb{E}_{x \sim P} [log \, \frac{P(x)}{Q(x)}] = \mathbb{E}_{x \sim P} [log \, P(x) - log \, Q(x)]</script><ul><li>用于描述两个分布的差异性</li><li>不可交换性（非对称的）。非负的。分布的差异越大则数值越大（分布相等时值为零）</li></ul></li><li><p>Cross-entropy（交叉熵）：</p><script type="math/tex; mode=display">H(P,Q) = H(P) + D_{KL} (P||Q) = - \mathbb{E}_{x \sim P} [ log \, Q(x)]</script><ul><li>对$Q$最小化交叉熵，等价于最小化KL散度（因为KL散度的第一项与$Q$无关）。</li><li>对离散分布的$p$和$q$，交叉熵即$H(p,q) = -\sum_x p(x)\; log \, q(x)$</li><li>交叉熵损失函数解读（my perspective）：<ul><li>假设softmax层的输出节点数为$n$，那么softmax层的输出就构成了离散概率分布$Q(x)$，且离散随机变量$x$的取值范围是$0 \sim (n-1)$。每一次inference就对应着输入的sample的分布被映射到$Q(x)$的过程。注意，每一次inference得到的prediction并不是deterministic的，只是我们最后取了分布$Q(x)$中概率最大的$x$值作为prediction罢了。</li><li>那么我们training时制作的one-hot label，就是为$Q(x)$制作的标签。我们假设标签服从分布$P(x)$。</li><li>注意，每一次inference所对应的$P(x)$和$Q(x)$都是不同的（除非你输入的sample是同一个）</li><li>我们classification task的目的就是想要model的输出越接近$P(x)$越好，那么我们的任务就是要最小化模型推断得到的分布$Q(x)$和我们制作的标签分布$ P(x) $之间的差异。</li><li>我们使用KL散度度量时，目标就是最小化$D_{KL}(P||Q)$（必须要让$x \sim P$，否则会出现花书Fig3.6的情况，我们想要的是Fig3.6左边那幅图的样子）。对于同一个sample，我们用optimizer调节parameters相当于优化输出分布$Q(x)$，所以$Q(x)$实质上是一个可变的分布。又因为对$Q$最小化交叉熵，等价于最小化KL散度。所以我们只需要最小化交叉熵就可以了。</li><li>举个例子，假设softmax输出层4个nodes。我们inference一次相当于生成了一个分布$Q(x)$，以及我们从dataset中取出所对应sample的标签分布$P(x)$。假设分布$Q(x)$可以用一个向量定量表示成$q=[0.1,0.1,0.6,0.2]$，标签分布$P(x)$定量表示成$p=[0,0,1,0]$。随机变量$x$的取值则自然是$0 \sim 3$。那么交叉熵（其中$q_y$的意思是取标签所对应的softmax输出值）：</li></ul></li></ul></li></ol><script type="math/tex; mode=display">\begin{align}H(P,Q) & = - \mathbb{E}_{x \sim P} [ log \, Q(x)] \\& = - \sum_{i=0}^{3}\; p_i\;log\,q_i \\& = - log \, q_{y} \\& = - log \, 0.6 \\& = 0.51\end{align}</script><h2 id="3-14-Structured-Probabilistic-Models"><a href="#3-14-Structured-Probabilistic-Models" class="headerlink" title="3.14 Structured Probabilistic Models"></a>3.14 Structured Probabilistic Models</h2><ol><li><p>Motivation：</p><ul><li>举例：如果能将联合分布$p(a,b,c)$拆成几个因子的乘积，例如$p(a)p(b|a)p(c|b)$，那么将能极大地减少参数量（这个例子是有向模型）</li><li>推广开来，即这种思想需要将复杂的分布拆解成几个容易表示出来的因子。我们将这种因子分解的方法，用graph表示，就叫做Structured Probabilistic Models</li></ul></li><li><p>结构概率模型的种类（对于一个distribution可能同时存在两种图描述方式，即有向/无向只是对于distribution的描述方式，而非种类的划分方式）：</p><ul><li><p>有向模型（将因子表示成条件概率分布）：</p><script type="math/tex; mode=display">p(x_1,...,x_n)=\prod_{i=1}^n p(x_i|Pa_\mathcal{G}(x_i))</script><p>其中$Pa_\mathcal{G}(x_i)$表示的是$x_i$的父节点</p><p><img src="8.png" alt></p></li><li><p>无向模型（将因子表示成函数）：</p><script type="math/tex; mode=display">p(x_1,...,x_n) = \frac{1}{Z} \prod_{i=1}^n \phi^{(i)}(\mathcal{C^{(i)}})</script><p>其中$\mathcal{C^{(i)}}$表示第$i$个clique，即彼此连接结点的集合，例如下图有三个clique（第一个：abc，第二个：bd，第三个：ce），$\phi^{(i)}(\cdot)$表示函数。前面的$\frac{1}{Z}$作用是归一化（因为不归一化的话，就不能保证这个连乘积项的求和是1）</p><p><img src="9.png" alt></p></li><li><p>有向图模型着重强调了因果的顺序关系；无向图模型着重强调了变量的集合间的interaction关系。</p></li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> dl </category>
          
      </categories>
      
      
        <tags>
            
            <tag> dl </tag>
            
            <tag> notes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>WSL2配置指南</title>
      <link href="/wsl/wsl2config/"/>
      <url>/wsl/wsl2config/</url>
      
        <content type="html"><![CDATA[<h1 id="安装WSL2"><a href="#安装WSL2" class="headerlink" title="安装WSL2"></a>安装WSL2</h1><h2 id="升级Win10系统"><a href="#升级Win10系统" class="headerlink" title="升级Win10系统"></a>升级Win10系统</h2><p>要使用WSL2需要系统的版本号不低于18917。而一般的Win10系统都是稳定版（意味着你的版本号一般来说都是低于18917的，按下Win+R并输入winver回车可以查看系统版本），需要加入Windows的预览体验计划。</p><p>详细教程，参考这里：<a href="https://jingyan.baidu.com/article/1876c85235c709890a137663.html" target="_blank" rel="noopener">Windows 10 20H1快速预览18917版系统更新教程</a></p><h2 id="安装WSL"><a href="#安装WSL" class="headerlink" title="安装WSL"></a>安装WSL</h2><ol><li><p>管理员身份打开powershell，输入下面的语句以打开“适用于Linux的Windows子系统”开关。</p><pre class=" language-lang-bash"><code class="language-lang-bash">Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux</code></pre></li><li><p>重启电脑</p></li><li><p>打开微软的应用商店，搜索WSL，安装Ubuntu即可。</p></li><li><p>安装完成之后，可以在开始菜单找到橙色的Ubuntu图标，点击它打开Ubuntu。</p></li><li><p>第一次运行需要等待安装并设置Ubuntu的用户名和密码。</p></li></ol><p>详细教程，参考这里：<a href="https://www.cnblogs.com/JettTang/p/8186315.html" target="_blank" rel="noopener">WSL（Windows Subsystem for Linux）的安装与使用</a></p><h2 id="升级WSL到WSL2"><a href="#升级WSL到WSL2" class="headerlink" title="升级WSL到WSL2"></a>升级WSL到WSL2</h2><ol><li><p>刚才安装的WSL还不是WSL2，需要虚拟化技术才可以。管理员身份打开powershell，输入下面指令以启用“虚拟机平台”可选组件：</p><pre class=" language-lang-bash"><code class="language-lang-bash">Enable-WindowsOptionalFeature -Online -FeatureName VirtualMachinePlatform</code></pre></li><li><p>重启计算机</p></li><li><p>在 PowerShell 中运行下面指令以将WSL转变为WSL2：</p><pre class=" language-lang-bash"><code class="language-lang-bash">wsl --set-version Ubuntu 2wsl --set-default-version 2</code></pre><blockquote><p>可能提示需要开启虚拟化技术<br>参考这里：</p><ul><li><a href="https://guwq2014.iteye.com/blog/2426896" target="_blank" rel="noopener">如何开启windows的虚拟化？</a></li><li><a href="https://jingyan.baidu.com/article/8ebacdf0261d3249f65cd531.html" target="_blank" rel="noopener">华硕主板BIOS设置中VT虚拟化技术选项怎么开启</a></li></ul></blockquote></li><li><p>在PowerShell 中输入下面指令确保WSL成功转化为第二版：</p><pre class=" language-lang-bash"><code class="language-lang-bash">wsl -l -v</code></pre><p>详细教程，参考这里：<a href="https://zhuanlan.zhihu.com/p/69121280" target="_blank" rel="noopener">在 Windows 中运行 Linux：WSL 2 使用入门</a></p></li></ol><h1 id="配置Ubuntu"><a href="#配置Ubuntu" class="headerlink" title="配置Ubuntu"></a>配置Ubuntu</h1><p>详细教程，参考这里：</p><ul><li><a href="https://zhuanlan.zhihu.com/p/57556340" target="_blank" rel="noopener">用WSL,MobaXterm,Cmder配置linux开发环境</a></li></ul><h2 id="更换源"><a href="#更换源" class="headerlink" title="更换源"></a>更换源</h2><p>输入下面命令更换软件源，并更新：</p><pre class=" language-lang-bash"><code class="language-lang-bash">sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak sudo sed -i 's/archive.ubuntu.com/mirrors.aliyun.com/g' /etc/apt/sources.listsudo apt updatesudo apt upgrade -y</code></pre><p>如果报错，则按照报错的提示，再输入一遍就好了。</p><h2 id="修改Win10盘的挂载点"><a href="#修改Win10盘的挂载点" class="headerlink" title="修改Win10盘的挂载点"></a>修改Win10盘的挂载点</h2><p>这步的操作是<strong>可选的</strong>，可以跳过。</p><p>默认的Windows磁盘在WSL的访问方式是<code>/mnt</code>开头，可能不太方便，我们下面更换成是<code>/</code>开头的吧。</p><p>打开这个文件：</p><pre class=" language-lang-bash"><code class="language-lang-bash">sudo vim /etc/wsl.conf</code></pre><p>添加下面三行内容，并保存此文件：</p><pre><code>[automount] root = / options = &quot;metadata&quot;</code></pre><p>关掉WSL窗口，然后重启WSL，重启WSL的方法为在CMD输入：</p><pre class=" language-lang-bash"><code class="language-lang-bash">net stop LxssManagernet start LxssManager</code></pre><p>再次打开WSL，输入<code>df -h</code>可以看到Win10盘的挂载点成功变更了。</p><h2 id="安装并配置VSCode"><a href="#安装并配置VSCode" class="headerlink" title="安装并配置VSCode"></a>安装并配置VSCode</h2><p>这步的操作是<strong>可选的</strong>，可以跳过。</p><p>安装VSCode和中文字体，因为WSL2没中文字体将显示为豆腐块</p><ol><li>安装中文字体（Noto Sans Mono CJK SC）：</li></ol><pre class=" language-lang-bash"><code class="language-lang-bash">sudo apt install -y fonts-noto-cjk fonts-noto-cjk-extra</code></pre><ol><li><p>在Win10下载VSCode的deb包：<a href="https://code.visualstudio.com/#alt-downloads" target="_blank" rel="noopener">官网下载页面</a></p></li><li><p>然后cd到deb包的下载目录，用下面的命令来安装此deb包（替换下面包的名字）：</p></li></ol><pre class=" language-lang-bash"><code class="language-lang-bash">sudo apt install ./code_1.31.1-1549938243_amd64.deb</code></pre><ol><li>安装要启动VSCode必要依赖：</li></ol><pre class=" language-lang-bash"><code class="language-lang-bash">sudo apt install libgtk2.0-0 libxss1 libasound2</code></pre><ol><li><p>输入<code>code .</code>以启动VSCode。（若还没图形界面则不会显示，等安装好了再进行第5步和第6步）</p></li><li><p>找到下面属性，并配置VSCode：</p></li></ol><pre class=" language-lang-json"><code class="language-lang-json">"Window.titleBarStyle": "native","editor.fontFamily": "monospace,'Noto Sans Mono CJK SC'"</code></pre><h2 id="安装并配置Python"><a href="#安装并配置Python" class="headerlink" title="安装并配置Python"></a>安装并配置Python</h2><p>这步的操作是<strong>可选的</strong>，可以跳过。</p><p>我们使用Miniconda来安装和配置Python。</p><ol><li><p>下载清华大学开源软件镜像站的Miniconda镜像（下载最新版本即可，可以在Win10下载好然后cd到下载目录中）：<a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/" target="_blank" rel="noopener">下载地址</a></p></li><li><p>输入<code>bash Miniconda3-latest-Linux-x86_64.sh</code>命令安装。按照提示输入即可，建议在最后输入yes初始化python环境（会安装最新版本的python）。</p></li><li><p>重新打开终端窗口，python环境会自动启动。</p></li></ol><p>详细教程，参考这里：<a href="https://ywnz.com/linuxjc/3834.html" target="_blank" rel="noopener">在linux系统中安装与卸载miniconda的方法</a></p><h2 id="配置Xrdp"><a href="#配置Xrdp" class="headerlink" title="配置Xrdp"></a>配置Xrdp</h2><ol><li>安装xrdp及相关：</li></ol><pre class=" language-lang-bash"><code class="language-lang-bash">sudo apt-get install xrdpsudo apt-get install vnc4serversudo apt-get install xubuntu-desktop</code></pre><ol><li>修改port 3389，避免跟本机的端口冲突（例如修改成3390）：</li></ol><pre class=" language-lang-bash"><code class="language-lang-bash">sudo vim /etc/xrdp/xrdp.ini</code></pre><ol><li>设为启动桌面</li></ol><pre class=" language-lang-bash"><code class="language-lang-bash">echo xfce4-session >~/.xsession</code></pre><ol><li>在<code>~</code>创建新文件，例如取名叫<code>run_remote_desktop.sh</code>，添加如下内容</li></ol><pre class=" language-lang-bash"><code class="language-lang-bash">echo 'Going to start xrdp so as to open remote desktop port...'echo 'Please input your password...'echo '========================================='sudo service xrdp restartecho '========================================='ipv4_address=$(/sbin/ifconfig -a|grep inet|grep -v 127.0.0.1|grep -v inet6|awk '{print $2}'|tr -d "addr:")echo "Your WSL2 is running on $ipv4_address:3390"</code></pre><ol><li>最后执行<code>chmod 777 ~/run_remote_desktop.sh</code>以赋予权限</li><li>每次想要启动桌面就输入<code>bash ~/run_remote_desktop.sh</code>，然后输入密码，最后会提示：<code>Your WSL2 is running on 172.28.246.150:3390</code>类似的字样，打开Win10的远程桌面，输入此ip就可以远程连接了。</li><li>然后配置图形界面（例如前面还没有完成的对VSCode的配置）</li><li>Enjoy developing！</li></ol><blockquote><p>注：输入<code>netstat -tulpn | grep LISTEN</code>可以查看到监听的端口，确保3390端口正在监听，否则无法远程桌面。</p></blockquote><p>详细教程，参考这里：</p><ul><li><a href="https://blog.csdn.net/BigWrist/article/details/81208667" target="_blank" rel="noopener">windows 10 WSL：Ubuntu 18.04 + xrdp</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> wsl </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> wsl </tag>
            
            <tag> ubuntu </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《Deep Learning》读书笔记（Ch2.LinearAlgebra）</title>
      <link href="/dl/dlbooknotes2/"/>
      <url>/dl/dlbooknotes2/</url>
      
        <content type="html"><![CDATA[<h1 id="2-Linear-Algebra"><a href="#2-Linear-Algebra" class="headerlink" title="2. Linear Algebra"></a>2. Linear Algebra</h1><h2 id="2-1-Scalars-Vectors-Matrices-and-Tensors"><a href="#2-1-Scalars-Vectors-Matrices-and-Tensors" class="headerlink" title="2.1 Scalars, Vectors, Matrices and Tensors"></a>2.1 Scalars, Vectors, Matrices and Tensors</h2><ul><li>Scalars：标量，就是一个数字</li><li>Vectors：向量，默认是列向量</li><li>Matrices：二维矩阵</li><li>Tensors：张量（多于两个axes）</li></ul><h2 id="2-2-Multiplying-Matrices-and-Vectors"><a href="#2-2-Multiplying-Matrices-and-Vectors" class="headerlink" title="2.2 Multiplying Matrices and Vectors"></a>2.2 Multiplying Matrices and Vectors</h2><ol><li><p>矩阵</p><ul><li><p>按位相乘（Element-wise product / Hadamard product）：$A \odot B$</p></li><li><p>矩阵乘法：$AB$</p></li></ul><p>性质：</p><ul><li>$(AB)^T = B^T A^T$</li></ul></li><li><p>向量的dot product</p><p>性质：</p><ul><li>$x^T y = y^T x$</li></ul></li></ol><h2 id="2-3-Identity-and-Inverse-Matrices"><a href="#2-3-Identity-and-Inverse-Matrices" class="headerlink" title="2.3 Identity and Inverse Matrices"></a>2.3 Identity and Inverse Matrices</h2><ol><li><p>$ n \times n $ 的单位矩阵 $I_n$</p></li><li><p>逆矩阵 $A^{-1}$</p></li></ol><h2 id="2-4-Linear-Dependence-and-Span"><a href="#2-4-Linear-Dependence-and-Span" class="headerlink" title="2.4 Linear Dependence and Span"></a>2.4 Linear Dependence and Span</h2><p>概念：</p><ul><li>无解/一个解/无穷多解</li><li>线性组合</li><li>线性张成空间（span）</li><li>线性相关（linear dependence）/线性独立（linearly independent）</li></ul><h2 id="2-5-Norms"><a href="#2-5-Norms" class="headerlink" title="2.5 Norms"></a>2.5 Norms</h2><ol><li><p>$ L^p $范数（$ p \ge 1 $）：（目标：Measure the size of a vector）</p><script type="math/tex; mode=display">||x||_p=(\sum_i|x_i|^p)^{\frac{1}{p}}</script></li><li><p>$ L^p $范数的特殊情形</p><ul><li><p>$ L^2 $ norm： Euclidean norm, denoted as $ ||x|| $; </p><p>Squared $ L^2 $ norm can be calculated as $ x^T x $ </p></li><li><p>$ L^1 $ norm</p></li><li><p>$ L^0 $ norm：非零元素的个数</p></li><li><p>Max norm：$||x||_{\infty} = \max \limits_{i} | x_{i} |$</p></li></ul></li></ol><ol><li><p>Frobenius norm（目标：Measure the size of a matrix）：<script type="math/tex">|| A ||_{ F } = \sqrt { \sum_{i,j} A_{i,j} ^ 2}</script></p></li><li><p>Dot product and angle： $x^T y=||x||_2 ||y||_2 cos \theta$</p></li></ol><h2 id="2-6-Special-Kinds-of-Matrices-and-Vectors"><a href="#2-6-Special-Kinds-of-Matrices-and-Vectors" class="headerlink" title="2.6 Special Kinds of Matrices and Vectors"></a>2.6 Special Kinds of Matrices and Vectors</h2><ol><li><p>Diagonal matrix（对角阵）</p><p>只有主对角线上是非零元素（可以不是方阵）</p><p>性质：</p><ul><li>$diag(v)^{-1} = diag([1/v_1, … , 1/v_n]^T)$</li></ul></li><li><p>Symmetric matrix（对称阵）</p><p>沿着对角线对称（$ A = A^T $）</p></li><li><p>Unit vector（$ ||x||_2 = 1 $）</p></li><li><p>Orthogonal matrix（正交矩阵，$ A^T A=A A^T = I $）</p><p>性质：</p><ul><li>行/列向量都是标准正交的（Orthonormal, 模长为一、相互垂直）</li><li>$ A^{-1} = A^T $</li></ul></li></ol><h2 id="2-7-Eigendecomposition"><a href="#2-7-Eigendecomposition" class="headerlink" title="2.7 Eigendecomposition"></a>2.7 Eigendecomposition</h2><ol><li><p>特征值$ \lambda $和特征向量$ v $，基本性质：</p><script type="math/tex; mode=display">A v = \lambda v</script></li><li><p>类型：</p><ul><li><p>对一个方阵$ A $的特征分解为：</p><script type="math/tex; mode=display">A = V diag(\lambda) V^{-1}</script><p>其中矩阵$ V = [v^{(1)}, …, v^{(n)}]$ ，向量$ \lambda = [\lambda_1, …, \lambda_n]^T $</p></li><li><p>实对称矩阵$ A $的特征分解（eg: 二次型、Hessian阵）：</p><script type="math/tex; mode=display">A = Q \Lambda Q^{-1} = Q \Lambda Q^T</script></li></ul></li></ol><p>   其中$ Q $是由特征向量组成的正交矩阵（具有性质：$ Q^{-1} = Q^T $）；$ \Lambda $是由特征值组成的对角阵（降序排列）</p><ol><li><p>性质：</p><ul><li>矩阵降秩$ \Leftrightarrow $ 至少其中一个特征值为零（因为$ det(A) = \prod \limits_{i} \lambda_i $）</li><li>正定：特征值都是正数；正定矩阵在二次型中的性质：$ \forall x, x^T A x \ge 0 $</li><li>对实对称矩阵$ A $，其特征向量矩阵$ Q $的第$ i $列记作向量$ d_i $（若特征值按降序排列那么特征矩阵也是唯一确定的），第$ i $大的特征值为$ \lambda_i $，则$ \lambda_i = d_i^T A d_i $（PCA分析要用到这条性质）</li><li>乘以矩阵$ A $之后，单位圆被缩放了：</li></ul><p><img src="2.png" alt></p></li></ol><h2 id="2-8-Singular-Value-Decomposition"><a href="#2-8-Singular-Value-Decomposition" class="headerlink" title="2.8 Singular Value Decomposition"></a>2.8 Singular Value Decomposition</h2><ol><li><p>SVD分解（$A$的size: $ m \times n $）：</p><script type="math/tex; mode=display">A = U D V^T</script><p>其中 $ U $（size: $ m \times m $） 和 $ V $（size: $ n \times n $）都是正交矩阵， $ D $（size: $ m \times n $）是对角阵。<br>且$ D $的组成元素是奇异值，$ U $的列向量是左奇异向量，$ V $的列向量是右奇异向量。</p></li><li><p>性质：</p><ul><li>实矩阵皆可SVD</li><li>$ A $的左奇异向量等于$ A A^T $的特征向量</li><li>$ A $的右奇异向量等于$ A^T A $的特征向量</li><li>$ A $的非零奇异值等于$ A^T A $的特征值的平方根，对$ A A^T $而言同理</li></ul></li></ol><h2 id="2-9-The-Moore-Penrose-Pseudoinverse"><a href="#2-9-The-Moore-Penrose-Pseudoinverse" class="headerlink" title="2.9 The Moore-Penrose Pseudoinverse"></a>2.9 The Moore-Penrose Pseudoinverse</h2><ol><li><p>伪逆的定义式：</p><script type="math/tex; mode=display">A^+ = \lim \limits_{\alpha \rightarrow 0} (A^T A + \alpha I)^{-1} A^T</script></li><li><p>计算式（因为定义式往往不好计算）：</p><script type="math/tex; mode=display">A^+ = V D^+ U^T</script><p>其中$ U $ 、 $ D $ 和 $ V $ 是矩阵$ A $ 的SVD分解。而$ D^+ $是通过对$ D $取元素的倒数、转置得到的。</p></li><li><p>性质（考察解方程$ Ax=y $）：</p><ul><li><p>伪逆的用途：当方程严格求解不出来时，使用伪逆可以求得近似解。</p></li><li><p>当方程数不足时（行数少于列数），解应该有无穷多个。使用伪逆求得的唯一解$ x = A^+ y $自动满足性质：$ ||x||_2 $最小（类似于自动求得在L2正则情况下的最优解？）</p></li><li>当方程数很多时（行数多于列数），解可能不存在，对应于直线拟合。使用伪逆求得的唯一解$ x = A^+ y $自动满足性质：$ ||Ax-y||_2 $最小（类似于欧氏误差/回归误差最小？）</li></ul></li></ol><h2 id="2-10-The-Trace-Operator"><a href="#2-10-The-Trace-Operator" class="headerlink" title="2.10 The Trace Operator"></a>2.10 The Trace Operator</h2><ol><li><p>“迹”的定义：对角线元素的和</p><script type="math/tex; mode=display">Tr(A) = \sum_i A_{i,i}</script></li><li><p>用Trace来简便表示：</p><ul><li>表示Frobenius norm：$ ||A||_F = \sqrt{Tr(A A^T)} $</li></ul></li><li><p>Trace的性质：</p><ul><li>$ Tr(A) = Tr(A^T) $</li><li>循环置换性：$ Tr(ABC) = Tr(CAB) = Tr(BCA) $，即可以把最后一个移到第一个（反之亦然），当两个矩阵时：$ Tr(AB) = Tr(BA) $</li><li>对标量$ a $：$ Tr(a) = a $</li></ul></li></ol><h2 id="2-11-The-Determinant"><a href="#2-11-The-Determinant" class="headerlink" title="2.11 The Determinant"></a>2.11 The Determinant</h2><ol><li>定义式：$ det(A) = \prod \limits_{i} v_i $，其中$ v_i $是第$ i $个特征值</li><li>性质：<ul><li>行列式为零：降秩，奇异</li><li>行列式为一：正交矩阵行列式的绝对值必定为一</li></ul></li></ol><h2 id="2-12-Example-Principal-Components-Analysis"><a href="#2-12-Example-Principal-Components-Analysis" class="headerlink" title="2.12 Example: Principal Components Analysis"></a>2.12 Example: Principal Components Analysis</h2><ol><li><p>Principal Components Analysis (PCA) 问题的数学描述（参考英文版花书的Page48）：</p><ul><li><p>数据集：我们有m个点$ \{x^{(1)},…,x^{(m)}\} $，其中对任意一个点$ x^{(i)}\in \mathbb{R}^n $</p></li><li><p>目标：构造从$ x^{(i)} \in \mathbb{R}^n $到$ c^{(i)} \in \mathbb{R}^l $的映射$ f(x)=c $（此处$ l \leq n $），以及复原的映射$ x \approx g(c) $，使得复原的误差最小。从而获得编码向量$ c $</p></li><li><p>选取：复原映射是线性映射$ g(c)=Dc $，其中矩阵$ D $的列向量都是相互正交的，且模长为一（注意$ D $可以不是一个方阵）</p></li></ul></li><li><p>求解：</p><ul><li>Step1：<br>对于一个点$ x $，以及已知的$ D $，求什么样的$ c $使得重建误差最小，相当于求$ c=f(x) $。通过化简以及通过对$c$求导，可以解得$ c = D^T x $，即$ f(x) = D^T x $</li></ul><script type="math/tex; mode=display">\min \limits_{c} ||x-Dc||^2_2</script><ul><li><p>Step2：</p><p>那么完整的重建回来的值就是$ r(x) = D D^T x $。假设$ l=1 $（压缩到1维），那么$ D $就只是一个列向量$ d $，则优化问题就是（求$ d $）</p><script type="math/tex; mode=display">\min_d \sum_i ||x^{(i)}-dd^Tx^{(i)}||^2,\quad subject \; to \; ||d||=1\\\Leftrightarrow \min_d \sum_i ||x^{(i)}-x^{(i)T}dd||^2,\quad subject \; to \; ||d||=1</script><p>我们用design matrix来表示样本，即$ X \in \mathbb{R}^{m \times n} $，矩阵的第$ i $行是第$ i $个样本$ x^{(i)T} $，那么上述问题则可以用矩阵范数表示成</p><script type="math/tex; mode=display">\Leftrightarrow \min_d  ||X-Xdd^T||_F^2,\quad subject \; to \; d^Td=1</script><script type="math/tex; mode=display">\Leftrightarrow \min_d  Tr((X-Xdd^T)^T(X-Xdd^T)),\quad subject \; to \; d^Td=1</script><script type="math/tex; mode=display">\Leftrightarrow \max_d  Tr(d^TX^TXd),\quad subject \; to \; d^Td=1</script><script type="math/tex; mode=display">\Leftrightarrow \max_d  (d^T(X^TX)d),\quad subject \; to \; d^Td=1</script><p>注意到上述最大化问题形似特征值分解问题（$ \lambda_{1} = d_1^T A d_1 $），即求某一个特征向量，使得特征值最大。那么只需要对矩阵$ X^T X $作特征值分解，找到最大的特征值所对应的特征向量，即为所求的$ d $。</p><p>若$ l $为其他情形，类似地，取降序排列的前$ l $个特征值所对的特征向量（模长要为一）构造出矩阵$ D = (d_1, …, d_l) $即为所求的复原映射矩阵。</p></li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> dl </category>
          
      </categories>
      
      
        <tags>
            
            <tag> dl </tag>
            
            <tag> notes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《Deep Learning》读书笔记（Ch1.Introduction）</title>
      <link href="/dl/dlbooknotes1/"/>
      <url>/dl/dlbooknotes1/</url>
      
        <content type="html"><![CDATA[<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p>Knowledge based approach -&gt; Machine learning </p><p>Representation learning: 学会如何表示东西，而不仅仅是从input到output的映射</p><p>Model深度的计算方法有两种：</p><ul><li>计算图的深度（Instructions）</li><li>概率图的深度（Concepts）</li></ul><p>AI领域的维恩图：</p><p><img src="1.png" alt></p><h2 id="1-1-Who-should-read-this-book"><a href="#1-1-Who-should-read-this-book" class="headerlink" title="1.1 Who should read this book?"></a>1.1 Who should read this book?</h2><ul><li>University students</li><li>Software engineers</li></ul><h2 id="1-2-Historical-trends-in-deep-learning"><a href="#1-2-Historical-trends-in-deep-learning" class="headerlink" title="1.2 Historical trends in deep learning"></a>1.2 Historical trends in deep learning</h2><ol><li><p>一波三折：</p><ul><li><p>控制论（Cybernetics, 1940~1960）</p><blockquote><p>Linear model，分类边界是直线，不能XOR</p></blockquote></li><li><p>连结主义（Connectionism, 1980~1990）</p><blockquote><p>小单元的组合可以形成智能<br>BP出现</p></blockquote><p>其他ML方法兴起</p></li><li><p>深度学习（Deep Learning, 2006~NOW）</p><blockquote><p>逐层预训练（最初用在DBN上），更深，硬件更好</p></blockquote></li></ul></li><li><p>特点：</p><ul><li>数据集变大</li><li>模型变大</li><li>精度高、结构复杂、可落地</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> dl </category>
          
      </categories>
      
      
        <tags>
            
            <tag> dl </tag>
            
            <tag> notes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git快速入门与速查</title>
      <link href="/git/git-beginner/"/>
      <url>/git/git-beginner/</url>
      
        <content type="html"><![CDATA[<h1 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h1><p>Git是什么？说起Git你可能会知道它是一个版本管理的工具，但究竟什么是版本管理？</p><p><img src="1.png" alt></p><p>举个栗子，假如你要写某个很牛逼的论文，你先写了个稿子A（包含idea1和idea2）。然后过了一天你发现idea1不work了，不能写在论文里了，然后你把idea1的部分全部删掉了，并将稿子A保存为稿子B。然后又过了一天，你发现idea1又可以work了，可以将其重新写入文章里了，于是你需要将之前写过的关于idea1的部分重头写过。。。</p><p>是不是很麻烦，不过如果你稍微机智一些，你可以在保存稿子B时选择另存为，那么你就会同时拥有稿子A和B了，后面你就可以方便地直接从稿子A恢复回稿子了。不过当文件一多起来，就会变得很麻烦，想想你的目录下有一堆文件的情形。。。</p><p>如果你会使用Git的话，它就能很好地帮你管理这些稿子的“版本”了。可以这么说，你在第一天保存的稿子A是版本A，在第二天删掉了idea1的部分的稿子B是版本B，第三天你就需要将版本B回退到版本A。</p><p>是不是听起来很方便呢？嘿嘿嘿。。。</p><h1 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h1><h2 id="安装配置"><a href="#安装配置" class="headerlink" title="安装配置"></a>安装配置</h2><p>按照网上的教程，安装好Git，并配置好用户信息，配置SSH等等。</p><h2 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h2><p>打开一个文件夹，点出鼠标右键的菜单，选择“Git Bash Here”，然后就能在此处打开Git的命令行界面了。</p><p>本地用法：</p><pre class=" language-lang-bash"><code class="language-lang-bash">git init # git库初始化git add . # 添加当前目录下的所有文件到缓存区git commit -m "first commit" # 提交缓存区的所有文件到git里，并附上提交的说明</code></pre><p>远程用法（在使用了“本地用法”之后）：</p><pre class=" language-lang-bash"><code class="language-lang-bash">git remote add origin [GitHub的上传网址] # 仅第一次需要：添加远端的推送地址git push -u origin master # 第一次推送到远端用这条，以后不用# git push # 以后推送到远端都用这条</code></pre><h2 id="分支管理"><a href="#分支管理" class="headerlink" title="分支管理"></a>分支管理</h2><ol><li><code>git branch dev</code>：创建名字为dev的分支</li><li><code>git checkout dev</code>：切换到分支dev</li><li><code>git branch</code>：查看分支</li><li>然后和平常一样的add和commit等，这些操作会在当前分支（dev）上进行</li><li><code>git checkout master</code>：切换回master分支</li><li><code>git merge dev</code>：将dev分支合并到master分支上</li><li><code>git branch -d dev</code>：删除dev分支</li></ol><h2 id="其他用法"><a href="#其他用法" class="headerlink" title="其他用法"></a>其他用法</h2><ol><li><code>git status</code>：查看状态和提示（例如哪些文件被修改了）</li><li><code>git diff xxx</code>：查看文件xxx被修改了哪里</li><li><code>git log</code>：查看commit的历史记录</li><li><code>git reset --hard HEAD^</code>：回退到上一个版本<br> <code>git reset --hard xxxx</code>：回退到指定的版本号（xxxx），版本号是用<code>git log</code>看到的一大串的数字字母字符串（版本号不需要打全，它会自动搜索和匹配前缀）<br> <code>HEAD</code>：当前版本<br> <code>HEAD^</code>：上一个当前版本<br> <code>HEAD^^</code>：上上一个当前版本<br> <code>HEAD~100</code>：上100个版本<br> （<strong>注意：回退版本号后，你将看不见你刚才在“未来”的版本，需要使用<code>git reflog</code>查看输入命令的历史记录，例如包括了版本号</strong>）</li><li><code>git checkout -- thefile</code>：把文件thefile回退到版本库中的状态（即取出版本库中的该文件，并覆盖到工作区）</li><li><code>git rm thefile</code>：从版本库移除thefile文件（需要再commit才能提交更改）</li><li><code>git clone [GitHub某仓库的下载地址]</code>：把仓库克隆到本地</li><li><code>git pull</code>：从远端拉取修改（获取最新的版本到本地）</li></ol>]]></content>
      
      
      <categories>
          
          <category> git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>如何从源码编译linux内核并使其从u盘启动</title>
      <link href="/linux/linux-u-drive/"/>
      <url>/linux/linux-u-drive/</url>
      
        <content type="html"><![CDATA[<blockquote><p>准备 linux 环境，可以在 VMware 虚拟机里运行。<br>我这里用的是 Archlinux 系统，不同系统大同小异。</p></blockquote><h1 id="编译内核"><a href="#编译内核" class="headerlink" title="编译内核"></a>编译内核</h1><h2 id="安装环境"><a href="#安装环境" class="headerlink" title="安装环境"></a>安装环境</h2><p>首先安装 <code>base-devel</code> 软件组，这个组包含了 make 和 gcc 等需要的软件包。</p><pre class=" language-lang-bash"><code class="language-lang-bash">sudo pacman -S base-devel</code></pre><h2 id="下载内核并解压"><a href="#下载内核并解压" class="headerlink" title="下载内核并解压"></a>下载内核并解压</h2><p>从<a href="https://www.kernel.org/" target="_blank" rel="noopener">这里</a>下载linux内核。<br>我们以版本<a href="https://cdn.kernel.org/pub/linux/kernel/v5.x/linux-5.0.11.tar.xz" target="_blank" rel="noopener">5.0.11</a>为例子。<br>首先新建个文件夹叫 <code>kernelbuild</code> 然后下载内核压缩包到该文件夹</p><pre class=" language-lang-bash"><code class="language-lang-bash">mkdir ~/kernelbuildcd ~/kernelbuildwget https://cdn.kernel.org/pub/linux/kernel/v5.x/linux-5.0.11.tar.xz</code></pre><p>然后我们解压内核压缩包：</p><pre class=" language-lang-bash"><code class="language-lang-bash">tar -xvJf linux-5.0.11.tar.xz</code></pre><p>为确保内核树绝对干净，进入内核目录并执行 <code>make mrproper</code> 命令：</p><pre class=" language-lang-bash"><code class="language-lang-bash">cd linux-5.0.11make clean && make mrproper</code></pre><h2 id="配置内核并编译"><a href="#配置内核并编译" class="headerlink" title="配置内核并编译"></a>配置内核并编译</h2><p>内核配置，会进入配置界面：</p><pre class=" language-lang-bash"><code class="language-lang-bash">make x86_64_defconfigmake menuconfig</code></pre><p><img src="menuconfig.png" alt></p><p>确保选中以下选项（按 y 选中，按 n 取消选择；按上下左右方向键移动选择的光标位置；回车确认）：</p><ul><li><p>General Setup –&gt; [*] Initial RAM filesystem and RAM disk (initramfs/initrd) support</p></li><li><p>Device Drivers –&gt; Block Devices –&gt; [*] RAM block device support</p></li><li><p>Device Drivers -&gt; Input device support -&gt; [*] Keyboard -&gt; 全部选择为[*]</p></li><li><p>Device Drivers -&gt; HID support -&gt; 全部选择为[*]</p></li><li>Device Drivers -&gt; HID support -&gt; USB HID support -&gt; 全部选择为[*]</li></ul><p>然后编译（需要等上十几分钟吧，再此期间你可以继续进行下一步busybox的准备）：</p><pre class=" language-lang-bash"><code class="language-lang-bash">make -j4 bzImage</code></pre><p>最后生成的 <code>arch/x86_64/boot/bzImage</code> 才是我们想要的内核文件（这个文件才8M多而已）</p><h1 id="准备-BusyBox-工具"><a href="#准备-BusyBox-工具" class="headerlink" title="准备 BusyBox 工具"></a>准备 BusyBox 工具</h1><p>到<a href="https://busybox.net/" target="_blank" rel="noopener">官网</a>下载 BusyBox，以1.30.1版本为例。</p><p>下载并解压：</p><pre class=" language-lang-bash"><code class="language-lang-bash">cd ～wget https://busybox.net/downloads/busybox-1.30.1.tar.bz2tar -jxvf busybox-1.30.1.tar.bz2cd busybox-1.30.1/</code></pre><p>然后进行配置：</p><pre class=" language-lang-bash"><code class="language-lang-bash">make defconfigmake menuconfig</code></pre><p>确保选中：</p><ul><li>BusyBox Setting-&gt;Build Options-&gt;[*]Build Busybox as a static binary (no shared libs)</li></ul><p>然后编译（编译很快）：</p><pre class=" language-lang-bash"><code class="language-lang-bash">make</code></pre><p>最后安装：</p><pre class=" language-lang-bash"><code class="language-lang-bash">make install</code></pre><p>执行 ls ，会发现多了一个 <code>_install</code> 目录，我们要用它来构建 linux 的根目录。</p><h1 id="准备-linux-所需文件"><a href="#准备-linux-所需文件" class="headerlink" title="准备 linux 所需文件"></a>准备 linux 所需文件</h1><p>首先在用户目录下新建一个文件夹 romfs ，然后把 <code>_install</code> 目录中的内容全部复制到 <code>romfs</code> 中。</p><pre class=" language-lang-bash"><code class="language-lang-bash">cd ~mkdir romfscp -r busybox-1.30.1/_install/* romfs/</code></pre><p>然后创建我们 linux 目录下的文件夹：</p><pre class=" language-lang-bash"><code class="language-lang-bash">cd romfs/mkdir proc mnt var tmp dev sys etc</code></pre><p>然后创建软链接：</p><pre class=" language-lang-bash"><code class="language-lang-bash">ln -s bin/sh init</code></pre><p>接着我们创建设备：</p><pre class=" language-lang-bash"><code class="language-lang-bash">cd dev/sudo mknod console c 5 1sudo mknod null c 1 3sudo mknod tty c 5 0sudo mknod tty1 c 4 1sudo mknod tty2 c 4 2sudo mknod tty3 c 4 3sudo mknod tty4 c 4 4</code></pre><p>然后制作压缩镜像（可能还要执行<code>sudo pacman -S cpio</code>）：</p><pre class=" language-lang-bash"><code class="language-lang-bash">cd ~/romfs/find . | cpio -H newc -o > ../romfs.imgcd ../gzip romfs.img -f</code></pre><h1 id="在-u-盘建立文件系统和-EFI-引导"><a href="#在-u-盘建立文件系统和-EFI-引导" class="headerlink" title="在 u 盘建立文件系统和 EFI 引导"></a>在 u 盘建立文件系统和 EFI 引导</h1><blockquote><p><strong>这部分教程使用 GPT+EFI 作为启动引导。</strong><br>如果你执意使用 GPT+BIOS 作为引导，则请按照下面的步骤来操作（应该可行）：</p><blockquote><ol><li>创建两个分区：FAT32（例如64MB）和EXT4（例如256MB）。并设置FAT32的启动标识为<code>bios_grub</code>类型。</li><li>创建文件夹/mnt/usb，将EXT4分区挂载到 /mnt/usb，再创建文件夹 /mnt/usb/boot</li><li>然后安装grub：<code>grub-install --target=i386-pc --debug --removable --boot-directory=/mnt/usb/boot /dev/sdd</code></li><li>复制 bzImage 和 romfs.img.gz 过去到 /mnt/usb/boot</li><li>创建 grub.cfg，并写入一些内容（与下面教程的相应部分类似），酌情修改。</li><li>重启电脑，选择从Legacy启动，保存修改，将u盘的优先级调到最高。再次启动即可。</li></ol></blockquote></blockquote><h2 id="u-盘分区"><a href="#u-盘分区" class="headerlink" title="u 盘分区"></a>u 盘分区</h2><p>插入u盘，使用<code>GParted</code>进行分区。</p><ol><li><p>在右上角选中你的u盘（例如我的是/dev/sdd），删除所有分区，然后点菜单栏的勾勾按钮。<br><img src="gparted.png" alt></p></li><li><p>然后新建两个分区，一个是FAT32，一个是EXT4，大小分别为64M和256M足矣。然后打勾勾确定。</p></li><li><p>更改刚刚建立的FAT32分区的标识为<code>boot,esp</code>。</p></li></ol><p><img src="gparted2.png" alt></p><h2 id="安装-Grub-到-u-盘"><a href="#安装-Grub-到-u-盘" class="headerlink" title="安装 Grub 到 u 盘"></a>安装 Grub 到 u 盘</h2><p>首先挂载刚刚建立的 <code>FAT32</code> 分区到 <code>/mnt/usb</code> 目录下：</p><pre class=" language-lang-bash"><code class="language-lang-bash">cd /mntsudo mkdir efi_partitionsudo mkdir boot_partitionsudo mount /dev/sdd1 ./efi_partitionsudo mount /dev/sdd2 ./boot_partition</code></pre><p>然后将 grub 安装到 u 盘：</p><pre class=" language-lang-bash"><code class="language-lang-bash">sudo grub-install --target=x86_64-efi --boot-directory=/mnt/boot_partition --efi-directory=/mnt/efi_partition --bootloader-id=karbo --removable --debug</code></pre><p>安装成功后应该会在最后的输出看到：<code>安装完成。没有报告错误。</code>类似的字样。</p><p>然后将编译好的文件复制过去：</p><pre class=" language-lang-bash"><code class="language-lang-bash">sudo cp ~/kernelbuild/linux-5.0.11/arch/x86_64/boot/bzImage /mnt/boot_partition/sudo cp ~/romfs.img.gz /mnt/boot_partition</code></pre><p>最后创建个文本文件：</p><pre class=" language-lang-bash"><code class="language-lang-bash">sudo micro /mnt/boot_partition/grub/grub.cfg</code></pre><p>内容为：</p><pre class=" language-lang-bash"><code class="language-lang-bash">menuentry "karbo-linux" {set root='hd0,gpt2'linux /bzImage root=/dev/raminitrd /romfs.img.gz}</code></pre><p>最后重启电脑，狂按F2，调整为<code>UEFI</code>启动，并把你u盘启动的优先级调到最顶上来，保存更改，再次启动即可。</p><p>应该会看到类似如下的界面：</p><p><img src="final.jpeg" alt></p><blockquote><p>如果在UEFI里没有找到你的u盘，那么可能是你插入了多个usb存储设备，尝试将其他usb存储设备拔掉，并重试。</p></blockquote><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ol><li><p><a href="https://wiki.archlinux.org/index.php/Kernels/Compilation/Traditional_(%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87" target="_blank" rel="noopener">Kernels/Compilation/Traditional (简体中文)</a>)</p></li><li><p><a href="https://zhuanlan.zhihu.com/p/27009845" target="_blank" rel="noopener">制作 U 盘 Linux</a></p></li><li><p><a href="https://wiki.archlinux.org/index.php/GRUB/Tips_and_tricks_(%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87" target="_blank" rel="noopener">GRUB/Tips and tricks (简体中文)</a>)</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> kernels </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>用Django搭建第一个投票网站</title>
      <link href="/django/python-django/"/>
      <url>/django/python-django/</url>
      
        <content type="html"><![CDATA[<p>本文教你用Django搭建第一个网站，投票网站。</p><p><img src="0.png" alt></p><blockquote><p>我们的投票网站要实现两个站点：</p><ul><li>一个让人们查看和投票的公共站点。</li><li>一个让你能添加、修改和删除投票的管理站点。</li></ul></blockquote><p>我们使用的操作系统：win10</p><p>关键思想：通过服务器上的数据库存储相关信息，通过网站对数据库的信息呈现并修改。</p><h1 id="Django安装和配置"><a href="#Django安装和配置" class="headerlink" title="Django安装和配置"></a>Django安装和配置</h1><p>创建一个项目工程文件夹，以后所有的操作都要在此文件夹下进行，例如我是<code>web</code>文件夹下。打开系统自带的<code>cmd</code>（最好是cmd），并切换到此文件夹下</p><p>例如我的文件夹路径是<code>E:\PythonProjects\web</code>，那么我先输入<code>e:</code>再敲回车，使得切换到<code>E</code>盘，然后再<code>cd E:\PythonProjects\web</code>即可。</p><p>然后在<code>cmd</code>输入（其中<code>my_env</code>是创建的虚拟环境的名字）：</p><pre class=" language-lang-bash"><code class="language-lang-bash">python -m venv my_env</code></pre><p>来创建<code>python</code>的虚拟环境。然后输入：</p><pre class=" language-lang-bash"><code class="language-lang-bash">my_env\Scripts\activate.bat</code></pre><p>来激活虚拟环境，此时<code>cmd</code>提示输入的前端应该有<code>(my_env)</code>字样，表明进入了虚拟环境。<br>若要退出虚拟环境，请输入：<code>my_env\Scripts\deactivate.bat</code></p><p>最后安装<code>Django</code>：</p><pre><code>pip install Django</code></pre><p>安装后，输入：<code>python -m django --version</code>命令，可以查看<code>Django</code>的版本。<br>我这里的版本是<code>2.2</code>。</p><h1 id="建立工程"><a href="#建立工程" class="headerlink" title="建立工程"></a>建立工程</h1><p>紧接着，我们来创建一些建立我们网站所必备的文件，输入下面的代码来创建一个工程。</p><pre class=" language-lang-bash"><code class="language-lang-bash">django-admin startproject mysite</code></pre><p>我们会发现在当前目录下多了一个叫<code>mysite</code>的文件夹，里面的文件是工程的必备文件。<br>这个<code>startproject</code>命令创建了如下的文件结构：</p><pre><code>mysite/        manage.py        mysite/                __init__.py                settings.py                urls.py                wsgi.py</code></pre><p>这些目录和文件的用处是：</p><ol><li>最外层的<code>mysite/</code> 根目录只是你项目的容器， <code>Django</code> 不关心它的名字，你可以将它重命名为任何你喜欢的名字。</li><li><code>manage.py</code>: 一个让你用各种方式管理 <code>Django</code> 项目的命令行工具。你可以阅读 <a href="https://docs.djangoproject.com/zh-hans/2.2/ref/django-admin/" target="_blank" rel="noopener">django-admin and manage.py</a> 获取所有 <code>manage.py</code> 的细节。</li><li>里面一层的 <code>mysite/</code> 目录包含你的项目，它是一个纯 <code>Python</code> 包。它的名字就是当你引用它内部任何东西时需要用到的 <code>Python</code> 包名。 (比如 <code>mysite.urls</code>).</li><li><code>mysite/__init__.py</code>：一个空文件，告诉 <code>Python</code> 这个目录应该被认为是一个 <code>Python</code> 包。如果你是<code>Python</code> 初学者，阅读官方文档中的<a href="https://docs.python.org/3/tutorial/modules.html#tut-packages" target="_blank" rel="noopener">更多关于包的知识</a>。</li><li><code>mysite/settings.py</code>：<code>Django</code> 项目的配置文件。如果你想知道这个文件是如何工作的，请查看 <a href="https://docs.djangoproject.com/zh-hans/2.2/topics/settings/" target="_blank" rel="noopener">Django settings</a> 了解细节。</li><li><code>mysite/urls.py</code>：<code>Django</code> 项目的 <code>URL</code> 声明，就像你网站的“目录”。阅读 <a href="https://docs.djangoproject.com/zh-hans/2.2/topics/http/urls/" target="_blank" rel="noopener">URL调度器</a>文档来获取更多关于<code>URL</code> 的内容。</li><li><code>mysite/wsgi.py</code>：作为你的项目的运行在<code>WSGI</code> 兼容的<code>Web</code>服务器上的入口。阅读如何<a href="https://docs.djangoproject.com/zh-hans/2.2/howto/deployment/wsgi/" target="_blank" rel="noopener">使用 WSGI 进行部署</a>了解更多细节。</li></ol><p>然后我们运行如下命令：</p><pre><code>cd mysitepython manage.py runserver</code></pre><p>来在本地端口运行服务器。我们在浏览器中输入<code>http://localhost:8000/</code>或<code>https://127.0.0.1:8000/</code>来打开网页。正常情况下应该会看到如下界面：</p><p><img src="1.png" alt></p><p>表示已经成功运行啦！<br>注意：</p><ol><li>千万不要将这个服务器用于和生产环境相关的任何地方。这个服务器只是为了开发而设计的。（Django 在 Web 框架方面是专家，在 Web 服务器方面并不是。）</li><li>仅仅是修改代码后，不必重启服务器来查看效果；但是添加新代码文件后则需要重启服务器来查看效果。</li></ol><h1 id="创建应用"><a href="#创建应用" class="headerlink" title="创建应用"></a>创建应用</h1><p>在 <code>Django</code> 中，每一个应用都是一个<code>Python</code> 包，并且遵循着相同的约定。<code>Django</code> 自带一个工具，可以帮你生成应用的基础目录结构，这样你就能专心写代码，而不是创建目录了。</p><blockquote><p><strong>项目 VS 应用</strong><br>项目和应用有啥区别？应用是一个专门做某件事的网络应用程序——比如博客系统，或者公共记录的数据库，或者简单的投票程序。项目则是一个网站使用的配置和应用的集合。项目可以包含很多个应用。应用可以被很多个项目使用。</p></blockquote><p>请确定你现在处于 <code>manage.py</code> 所在的目录下，然后我们使用如下指令来创建一个名为<code>polls</code>的应用：</p><pre><code>python manage.py startapp polls</code></pre><p>这将会创建一个 <code>polls</code> 目录，它的目录结构大致如下：</p><pre><code>polls/        migrations/                __init__.py        __init__.py        admin.py        apps.py        models.py        tests.py        views.py</code></pre><p>这个目录结构包括了投票应用的全部内容。</p><h1 id="编写简单的视图和绑定URL"><a href="#编写简单的视图和绑定URL" class="headerlink" title="编写简单的视图和绑定URL"></a>编写简单的视图和绑定URL</h1><p>打开 <code>polls/views.py</code>，把下面这些 <code>Python</code> 代码添加进去：</p><pre class=" language-lang-python"><code class="language-lang-python">from django.http import HttpResponsedef index(request):    return HttpResponse("Hello, world. You're at the polls index.")</code></pre><p>这是 <code>Django</code> 中最简单的视图。如果想看见效果，我们需要将一个 <code>URL</code> 映射到它——这就是我们需要 <code>URLconf</code> 的原因了。</p><p>为了创建 <code>URLconf</code>，请在 <code>polls</code> 目录里新建一个 <code>urls.py</code> 文件。<br>在 <code>polls/urls.py</code> 中，输入如下代码：</p><pre class=" language-lang-python"><code class="language-lang-python">from django.urls import pathfrom . import viewsurlpatterns = [    path('', views.index, name='index'),]</code></pre><p>一般来说，每一个应用都应该有一个自己的<code>urls.py</code>文件。第一项为空<code>&#39;&#39;</code>表明不再增加<code>url</code>的下级路径；第二项<code>views.index</code>表明调用刚刚编写的<code>views.py</code>文件里的<code>index()</code>函数来响应请求。</p><p>下一步是要在根 <code>URLconf</code> 文件（可以理解为全局的<code>url</code>配置文件）中记录我们刚刚创建的 <code>polls.urls</code> 模块。在 <code>mysite/urls.py</code> 文件的 <code>urlpatterns</code> 列表里插入一个 <code>include()</code>， 如下：</p><pre class=" language-lang-python"><code class="language-lang-python">from django.contrib import adminfrom django.urls import include, pathurlpatterns = [    path('polls/', include('polls.urls')),    path('admin/', admin.site.urls),]</code></pre><p>函数 <code>include()</code> 允许引用其它 <code>URLconfs</code>。每当 <code>Django</code> 遇到 <code>include</code> 时，它会截断与此项匹配的 <code>URL</code> 的部分，并将剩余的字符串发送到 <code>URLconf</code> 以供进一步处理。举个例子，就是假如输入的地址是<code>http://localhost:8000/polls/</code>，那么它就会截断匹配的部分，在这个例子里也就是整个<code>http://localhost:8000/polls/</code>，剩下的空字符串<code>&#39;&#39;</code>给<code>polls.urls</code>的<code>urlpatterns</code>去做匹配处理，恰好匹配到空字符串<code>&#39;&#39;</code>，然后就会调用<code>polls.view.index</code>函数去处理。</p><p><code>Django</code>设计 <code>include()</code> 的理念是使其可以即插即用。因为投票应用有它自己的 <code>URLconf( polls/urls.py )</code>，他们能够被放在 <code>&quot;/polls/&quot;</code> ，<code>&quot;/fun_polls/&quot;</code> ，<code>&quot;/content/polls/&quot;</code>，或者其他任何路径下，这个应用都能够正常工作。</p><blockquote><p><strong>何时使用 include()</strong><br>当包括其它 <code>URL</code> 文件时你应该总是使用 <code>include()</code>，而 <code>admin.site.urls</code> 是例外。</p></blockquote><p>你刚刚已经把视图<code>view</code>与<code>url</code>绑定了，输入：</p><pre class=" language-lang-bash"><code class="language-lang-bash">python manage.py runserver</code></pre><p>来运行服务器，在浏览器输入<code>http://localhost:8000/polls/</code>，正常情况下应该看到字符串：<code>Hello, world. You&#39;re at the polls index.</code></p><blockquote><p>函数 <code>path()</code> 具有四个参数，两个必须的参数：<code>route</code> 和 <code>view</code>，两个可选参数：<code>kwargs</code> 和 <code>name</code>。现在，是时候来研究这些参数的含义了。<br><strong>path() 参数： route</strong><br>route 是一个匹配 URL 的准则（类似正则表达式）。当 Django 响应一个请求时，它会从 urlpatterns 的第一项开始，按顺序依次匹配列表中的项，直到找到匹配的项。<br>这些准则不会匹配 GET 和 POST 参数或域名。例如，URLconf 在处理请求 <a href="https://www.example.com/myapp/" target="_blank" rel="noopener">https://www.example.com/myapp/</a> 时，它会尝试匹配 myapp/ 。处理请求 <a href="https://www.example.com/myapp/?page=3" target="_blank" rel="noopener">https://www.example.com/myapp/?page=3</a> 时，也只会尝试匹配 myapp/。<br><strong>path() 参数： view</strong><br>当 Django 找到了一个匹配的准则，就会调用这个特定的视图函数，并传入一个 HttpRequest 对象作为第一个参数，被“捕获”的参数以关键字参数的形式传入。稍后，我们会给出一个例子。<br><strong>path() 参数： kwargs</strong><br>任意个关键字参数可以作为一个字典传递给目标视图函数。本教程中不会使用这一特性。<br><strong>path() 参数： name</strong><br>为你的 URL 取名能使你在 Django 的任意地方唯一地引用它，尤其是在模板中。这个有用的特性允许你只改一个文件就能全局地修改某个 URL 模式。</p></blockquote><h1 id="数据库配置"><a href="#数据库配置" class="headerlink" title="数据库配置"></a>数据库配置</h1><h2 id="setting-py文件"><a href="#setting-py文件" class="headerlink" title="setting,py文件"></a>setting,py文件</h2><p>现在，打开 <code>mysite/settings.py</code> 。这是个包含了 <code>Django</code> 项目设置的 <code>Python</code> 模块。</p><p>通常，这个配置文件使用 <code>SQLite</code> 作为默认数据库。如果你不熟悉数据库，或者只是想尝试下 <code>Django</code>，这是最简单的选择。<code>Python</code> 内置 <code>SQLite</code>，所以你无需安装额外东西来使用它。如果你使用 <code>SQLite</code>，那么你不需要在使用前做任何事——数据库会在需要的时候自动创建。</p><p>此外，关注一下文件头部的 <code>INSTALLED_APPS</code> 设置项。这里包括了会在你项目中启用的所有 <code>Django</code> 应用。应用能在多个项目中使用，你也可以打包并且发布应用，让别人使用它们。</p><p>通常， <code>INSTALLED_APPS</code> 默认包括了以下 Django 的自带应用：</p><ul><li><code>django.contrib.admin</code> — 管理员站点， 你很快就会使用它。</li><li><code>django.contrib.auth</code> — 认证授权系统。</li><li><code>django.contrib.contenttypes</code> — 内容类型框架。</li><li><code>django.contrib.sessions</code> — 会话框架。</li><li><code>django.contrib.messages</code> — 消息框架。</li><li><code>django.contrib.staticfiles</code> — 管理静态文件的框架。</li></ul><p>这些应用被默认启用是为了给常规项目提供方便。</p><p>默认开启的某些应用需要至少一个数据表，所以，在使用他们之前，需要在数据库中创建一些表。请执行以下命令：</p><pre><code>python manage.py migrate</code></pre><p>这个 <code>migrate</code> 命令检查 <code>INSTALLED_APPS</code> 设置，为其中的每个应用创建需要的数据表，至于具体会创建什么，这取决于你的 <code>mysite/settings.py</code> 设置文件和每个应用的数据库迁移文件（我们稍后会介绍这个）。这个命令所执行的每个迁移操作都会在终端中显示出来。<code>migrate</code>命令只会为在 <code>INSTALLED_APPS</code> 里声明了的应用进行数据库迁移。</p><h2 id="创建模型"><a href="#创建模型" class="headerlink" title="创建模型"></a>创建模型</h2><p>在 <code>Django</code> 里写一个数据库驱动的 <code>Web</code> 应用的第一步是定义模型 - 也就是数据库结构设计和附加的其它元数据。</p><p>在这个简单的投票应用中，需要创建两个模型：问题 <code>Question</code> 和选项 <code>Choice</code>。<code>Question</code> 模型包括问题描述和发布时间。<code>Choice</code> 模型有两个字段，选项描述和当前得票数。每个选项属于一个问题。</p><p>这些概念可以通过一个简单的 <code>Python</code> 类来描述。按照下面的例子来编辑 <code>polls/models.py</code> 文件：</p><pre class=" language-lang-python"><code class="language-lang-python">from django.db import modelsclass Question(models.Model):    question_text = models.CharField(max_length=200)    pub_date = models.DateTimeField('date published')class Choice(models.Model):    question = models.ForeignKey(Question, on_delete=models.CASCADE)    choice_text = models.CharField(max_length=200)    votes = models.IntegerField(default=0)</code></pre><p>每个字段都是 <code>Field</code> 类的实例 - 比如，字符字段被表示为 <code>CharField</code> ，日期时间字段被表示为 <code>DateTimeField</code> 。这将告诉 <code>Django</code> 每个字段要处理的数据类型。</p><p>每个 <code>Field</code> 类实例变量的名字（例如 <code>question_text</code> 或 <code>pub_date</code> ）也是字段名，所以最好使用对机器友好的格式。你将会在 <code>Python</code> 代码里使用它们，而数据库会将它们作为列名。</p><p>你可以使用可选的选项来为 <code>Field</code> 定义一个人类可读的名字。这个功能在很多 <code>Django</code> 内部组成部分中都被使用了，而且作为文档的一部分。如果某个字段没有提供此名称，<code>Django</code> 将会使用对机器友好的名称，也就是变量名。在上面的例子中，我们只为 <code>Question.pub_date</code> 定义了对人类友好的名字。对于模型内的其它字段，它们的机器友好名也会被作为人类友好名使用。</p><p>注意在最后，我们使用 <code>ForeignKey</code> 定义了一个关系。这将告诉 <code>Django</code>，每个 <code>Choice</code> 对象都关联到一个 <code>Question</code> 对象。<code>Django</code> 支持所有常用的数据库关系：多对一、多对多和一对一。</p><h2 id="激活模型"><a href="#激活模型" class="headerlink" title="激活模型"></a>激活模型</h2><p>首先得把 <code>polls</code> 应用安装到我们的项目里。</p><blockquote><p><strong>设计哲学</strong><br><code>Django</code> 应用是“可插拔”的。你可以在多个项目中使用同一个应用。除此之外，你还可以发布自己的应用，因为它们并不会被绑定到当前安装的 <code>Django</code> 上。</p></blockquote><p>为了在我们的工程中包含这个应用，我们需要在配置类 <code>INSTALLED_APPS</code> 中添加设置。因为 <code>PollsConfig</code> 类写在文件 <code>polls/apps.py</code> 中，所以它的点式路径是 <code>&#39;polls.apps.PollsConfig&#39;</code>。在文件 <code>mysite/settings.py</code> 中 <code>INSTALLED_APPS</code>子项添加点式路径后，它看起来像这样：</p><pre><code>INSTALLED_APPS = [    &#39;polls.apps.PollsConfig&#39;,    &#39;django.contrib.admin&#39;,    &#39;django.contrib.auth&#39;,    &#39;django.contrib.contenttypes&#39;,    &#39;django.contrib.sessions&#39;,    &#39;django.contrib.messages&#39;,    &#39;django.contrib.staticfiles&#39;,]</code></pre><p>现在你的 <code>Django</code> 项目会包含 <code>polls</code> 应用。接着运行下面的命令：</p><pre><code>python manage.py makemigrations polls</code></pre><p><code>makemigrations</code>命令会在<code>polls/migrations/</code>目录下生成一些关于<code>polls/models.py</code>修改记录的文件。但是并没有修改到数据库。</p><p>你将会看到类似于下面这样的输出：</p><pre><code>Migrations for &#39;polls&#39;:  polls/migrations/0001_initial.py:    - Create model Choice    - Create model Question    - Add field question to choice</code></pre><p>现在，再次运行 <code>migrate</code> 命令，在数据库里创建刚刚建立的模型的数据表：</p><pre><code>python manage.py migrate</code></pre><p>这会使得数据库得以修改，使数据库包含了我们刚刚新建立的模型参数。</p><p>我们应该会看到类似的输出：</p><pre><code>Operations to perform:  Apply all migrations: admin, auth, contenttypes, polls, sessionsRunning migrations:  Rendering model states... DONE  Applying polls.0001_initial... OK</code></pre><p>最后一行表明，模型更改成功同步于数据库上（在数据库里创建了与模型相关的存储结构）。</p><p>这个 <code>migrate</code> 命令选中所有还没有执行过的迁移（<code>Django</code> 通过在数据库中创建一个特殊的表 <code>django_migrations</code> 来跟踪执行过哪些迁移）并应用在数据库上 - 也就是将你对模型的更改同步到数据库结构上。</p><p>迁移是非常强大的功能，它能让你在开发过程中持续的改变数据库结构而不需要重新删除和创建表 - 它专注于使数据库平滑升级而不会丢失数据。我们会在后面的教程中更加深入的学习这部分内容。</p><p>现在，你只需要记住，改变模型并生效只需要这三步：</p><ol><li>编辑 <code>models.py</code> 文件，改变模型。</li><li>运行 <code>python manage.py makemigrations &lt;app_name&gt;</code> 为模型的改变生成迁移文件。</li><li>运行 <code>python manage.py migrate</code> 来应用数据库迁移。</li></ol><h1 id="Django命令行"><a href="#Django命令行" class="headerlink" title="Django命令行"></a>Django命令行</h1><p>运行：</p><pre><code>python manage.py shell</code></pre><p>来打开<code>Django</code>的命令行。</p><p>我们使用这个命令而不是简单的使用 <code>Python</code> 是因为 <code>manage.py</code> 会设置 <code>DJANGO_SETTINGS_MODULE</code> 环境变量，这个变量会让 <code>Django</code> 根据 <code>mysite/settings.py</code> 文件来设置 <code>Python</code> 包的导入路径。</p><p>当你成功进入命令行后，来试试数据库的<code>API</code> 吧，它允许你从数据库中读取和修改<code>Choice</code>和<code>Question</code>的数据。</p><p>输入下述Python命令：</p><pre class=" language-lang-python"><code class="language-lang-python">from polls.models import Choice, Question  #导入我们刚刚写的类Question.objects.all() #执行后输出<QuerySet []>，表明Question类还没有对象（还没有数据）from django.utils import timezone #导入时区q = Question(question_text="What's new?", pub_date=timezone.now()) #创建一个问题q.save() #将q保存到数据库中q.id # 现在它有id值了（为1）q.question_text #输出："What's new?"q.pub_date #输出时间q.question_text = "What's up?" #再次改变question_text值q.save() #再次保存到数据库中Question.objects.all() #展示数据库中Question的所有数据</code></pre><p>但是，我们注意到，输入<code>Question.objects.all()</code>时，返回的<code>&lt;QuerySet [&lt;Question: Question object (1)&gt;]&gt;</code>值，并没有让我们了解到这个对象具体是什么样的，我们想让它有个对自己的描述，让我们清楚这是个什么样的对象。</p><p>让我们通过编辑 <code>Question</code> 模型的代码（位于 <code>polls/models.py</code> 中）来修复这个问题。给 <code>Question</code> 和 <code>Choice</code> 增加 <code>__str__()</code>方法：</p><pre class=" language-lang-python"><code class="language-lang-python">from django.db import modelsclass Question(models.Model):    # ...    def __str__(self):        return self.question_textclass Choice(models.Model):    # ...    def __str__(self):        return self.choice_text</code></pre><p>给模型增加<code>__str__()</code> 方法是很重要的，这不仅仅能给你在命令行里使用带来方便，<code>Django</code> 自动生成的 <code>admin</code> 里也使用这个方法来表示对象。</p><p>注意：这些都是常规的 <code>Python</code>方法。让我们添加一个自定义的方法，这只是为了演示：</p><pre class=" language-lang-python"><code class="language-lang-python">import datetimefrom django.db import modelsfrom django.utils import timezoneclass Question(models.Model):    # ...    def was_published_recently(self):        return self.pub_date >= timezone.now() - datetime.timedelta(days=1)</code></pre><p>新加入的 <code>import datetime</code> 和 <code>from django.utils import timezone</code> 分别导入了 Python 的标准 <code>datetime</code> 模块和 <code>Django</code> 中和时区相关的 <code>django.utils.timezone</code>工具模块。如果你不太熟悉 <code>Python</code> 中的时区处理，看看 <a href="https://docs.djangoproject.com/zh-hans/2.2/topics/i18n/timezones/" target="_blank" rel="noopener">时区支持文档</a> 吧。</p><p>保存文件然后通过 <code>python manage.py shell</code> 命令再次打开 <code>Python</code> 交互式命令行：</p><pre class=" language-lang-python"><code class="language-lang-python">from polls.models import Choice, QuestionQuestion.objects.all() #输出：<QuerySet [<Question: What's up?>]>Question.objects.filter(id=1) #依据id查找对象Question.objects.filter(question_text__startswith='What') #依据question_text内容查找对象from django.utils import timezonecurrent_year = timezone.now().year #今年Question.objects.get(pub_date__year=current_year) #依据年份查找对象Question.objects.get(id=2) #访问不存在的对象会引起异常Question.objects.get(pk=1) #大多时候，该语句等价于Question.objects.get(id=1)q = Question.objects.get(pk=1)q.was_published_recently() #调用was_published_recently()方法q = Question.objects.get(pk=1)q.choice_set.all() #显示与q相关联的choice，返回<QuerySet []>，注：choice_set是由于ForeignKey自动生成的子成员# 创建三个choice，会在Choice自动创建对象q.choice_set.create(choice_text='Not much', votes=0)q.choice_set.create(choice_text='The sky', votes=0)c = q.choice_set.create(choice_text='Just hacking again', votes=0)c.question #Choice对象访问自己的question子属性，返回<Question: What's up?># Question对象反之亦能关联到Choice对象q.choice_set.all() #返回q关联到的choice的集合q.choice_set.count() #返回q关联到的choice的个数Choice.objects.filter(question__pub_date__year=current_year) #依据年份返回choice的集合c = q.choice_set.filter(choice_text__startswith='Just hacking') #获取一个choicec.delete() #删除这个choice</code></pre><p>阅读 <a href="https://docs.djangoproject.com/zh-hans/2.2/ref/models/relations/" target="_blank" rel="noopener">访问关系对象</a> 文档可以获取关于数据库关系的更多内容。想知道关于双下划线的更多用法，参见 <a href="https://docs.djangoproject.com/zh-hans/2.2/topics/db/queries/#field-lookups-intro" target="_blank" rel="noopener">查找字段</a> 文档。数据库 API 的所有细节可以在 <a href="https://docs.djangoproject.com/zh-hans/2.2/topics/db/queries/" target="_blank" rel="noopener">数据库 API 参考</a> 文档中找到。</p><h1 id="Django管理页面"><a href="#Django管理页面" class="headerlink" title="Django管理页面"></a>Django管理页面</h1><p>管理界面不是为了网站的访问者，而是为管理者准备的。</p><h2 id="创建一个管理员账号"><a href="#创建一个管理员账号" class="headerlink" title="创建一个管理员账号"></a>创建一个管理员账号</h2><p>首先，我们得创建一个能登录管理页面的用户。请运行下面的命令：</p><pre><code>python manage.py createsuperuser</code></pre><p>然后输入用户名、邮箱和两次密码，以创建管理员账号。</p><h2 id="进入管理界面"><a href="#进入管理界面" class="headerlink" title="进入管理界面"></a>进入管理界面</h2><p><code>Django</code> 的管理界面默认就是启用的。让我们启动开发服务器，看看它到底是什么样的。<br>如果开发服务器未启动，用以下命令启动它：</p><pre><code>python manage.py runserver</code></pre><p>然后输入浏览器地址：<code>http://localhost:8000/admin/</code>来打开管理页面，输入账号和密码登陆即可。</p><p>正常情况下，你会看到类似如下图的界面：</p><p><img src="2.png" alt></p><p>其中<code>Groups</code>和<code>Users</code>是可编辑的。它们是由 <code>django.contrib.auth</code> 提供的，这是 <code>Django</code> 开发的认证框架。</p><h2 id="向管理页面中加入投票应用"><a href="#向管理页面中加入投票应用" class="headerlink" title="向管理页面中加入投票应用"></a>向管理页面中加入投票应用</h2><p>但是我们的投票应用在哪呢？它没在索引页面里显示。</p><p>只需要做一件事：我们得告诉管理页面，问题 <code>Question</code> 对象需要被管理。打开 <code>polls/admin.py</code> 文件，把它编辑成下面这样：</p><pre class=" language-lang-python"><code class="language-lang-python">from django.contrib import adminfrom .models import Questionadmin.site.register(Question)</code></pre><p>类似地，你也可以往<code>admin</code>中添加<code>Choice</code>类。</p><p>添加<code>Question</code>类后，刷新界面，应该会是这个样子：</p><p><img src="3.png" alt></p><p>你可以在这个网页管理页面，管理你的数据库对象，修改<code>Question</code>的数据，或是增删用户等。</p><p>如果你发现<code>Django</code>的时间不对，请修改<code>mysite/settings.py</code>中的<code>TIME_ZONE = &#39;Asia/Shanghai&#39;</code>。</p><h1 id="编写视图"><a href="#编写视图" class="headerlink" title="编写视图"></a>编写视图</h1><p>而在我们的投票应用中，我们需要下列几个视图：</p><ul><li>问题索引页——展示最近的几个投票问题。</li><li>问题详情页——展示某个投票的问题和不带结果的选项列表。</li><li>问题结果页——展示某个投票的结果。</li><li>投票处理器——用于响应用户为某个问题的特定选项投票的操作。</li></ul><p>在 Django 中，网页和其他内容都是从视图派生而来。每一个视图表现为一个简单的 Python 函数（或者说方法，如果是在基于类的视图里的话）。Django 将会根据用户请求的 URL 来选择使用哪个视图（更准确的说，是根据 URL 中域名之后的部分）。</p><p>为了将 URL 和视图关联起来，Django 使用了 URLconfs 来配置。URLconf 将 URL 模式映射到视图。本教程只会介绍 URLconf 的基础内容，你可以看看 <a href="https://docs.djangoproject.com/zh-hans/2.2/topics/http/urls/" target="_blank" rel="noopener">URL调度器</a> 以获取更多内容。</p><h2 id="编写更多视图"><a href="#编写更多视图" class="headerlink" title="编写更多视图"></a>编写更多视图</h2><p>现在让我们向 <code>polls/views.py</code> 里添加更多视图。这些视图有一些不同，因为他们接收参数：</p><pre class=" language-lang-python"><code class="language-lang-python">def detail(request, question_id):    return HttpResponse("You're looking at question %s." % question_id)def results(request, question_id):    response = "You're looking at the results of question %s."    return HttpResponse(response % question_id)def vote(request, question_id):    return HttpResponse("You're voting on question %s." % question_id)</code></pre><p>把这些新视图添加进 <code>polls.urls</code> 模块里，只要添加几个 <a href="https://docs.djangoproject.com/zh-hans/2.2/ref/urls/#django.conf.urls.url" target="_blank" rel="noopener"><code>url()</code></a> 函数调用就行：</p><pre class=" language-lang-python"><code class="language-lang-python">from django.urls import pathfrom . import viewsurlpatterns = [    # example: /polls/    path('', views.index, name='index'),    # example: /polls/5/    path('<int:question_id>/', views.detail, name='detail'),    # example: /polls/5/results/    path('<int:question_id>/results/', views.results, name='results'),    # example: /polls/5/vote/    path('<int:question_id>/vote/', views.vote, name='vote'),]</code></pre><p>然后看看你的浏览器，如果你转到 <code>/polls/34/</code> ，Django 将会运行 <code>detail()</code> 方法并且展示你在 URL 里提供的问题 ID。再试试 <code>/polls/34/vote/</code> 和 <code>/polls/34/vote/</code> ——你将会看到暂时用于占位的结果和投票页。</p><p>当某人请求你网站的某一页面时——比如说，<code>/polls/34/</code> ，Django 将会载入 <code>mysite.urls</code> 模块，因为这在配置项 <a href="https://docs.djangoproject.com/zh-hans/2.2/ref/settings/#std:setting-ROOT_URLCONF" target="_blank" rel="noopener"><code>ROOT_URLCONF</code></a> 中设置了。然后 Django 寻找名为 <code>urlpatterns</code> 变量并且按序匹配正则表达式。在找到匹配项 <code>polls/</code>，它切掉了匹配的文本（<code>polls/</code>），将剩余文本——<code>34/</code>，发送至 <code>polls.urls</code> ，URLconf 做进一步处理。在这里剩余文本匹配了 <code>&lt;int:question_id&gt;/</code>，使得我们 Django 以如下形式调用 <code>detail()</code>:</p><pre class=" language-lang-python"><code class="language-lang-python">detail(request=<HttpRequest object>, question_id=34)</code></pre><p><code>question_id=34</code> 由 <code>&lt;int:question_id&gt;</code> 匹配生成。使用尖括号“捕获”这部分 URL，且以关键字参数的形式发送给视图函数。上述字符串的 <code>:question_id&gt;</code> 部分定义了将被用于区分匹配模式的变量名，而 <code>int:</code> 则是一个转换器决定了应该以什么变量类型匹配这部分的 URL 路径。</p><p>为每个 URL 加上不必要的东西，例如 <code>.html</code> ，是没有必要的。不过如果你非要加的话，也是可以的:</p><pre class=" language-lang-python"><code class="language-lang-python">path('polls/latest.html', views.index),</code></pre><h2 id="写一个真正有用的视图"><a href="#写一个真正有用的视图" class="headerlink" title="写一个真正有用的视图"></a>写一个真正有用的视图</h2><p>每个视图必须要做的只有两件事：返回一个包含被请求页面内容的 <a href="https://docs.djangoproject.com/zh-hans/2.2/ref/request-response/#django.http.HttpResponse" target="_blank" rel="noopener"><code>HttpResponse</code></a> 对象，或者抛出一个异常，比如 <a href="https://docs.djangoproject.com/zh-hans/2.2/topics/http/views/#django.http.Http404" target="_blank" rel="noopener"><code>Http404</code></a> 。至于你还想干些什么，随便你。</p><p>你的视图可以从数据库里读取记录，可以使用一个模板引擎（比如 Django 自带的，或者其他第三方的），可以生成一个 PDF 文件，可以输出一个 XML，创建一个 ZIP 文件，你可以做任何你想做的事，使用任何你想用的 Python 库。</p><p>因为 Django 自带的数据库 API 很方便，我们曾在 <a href="https://docs.djangoproject.com/zh-hans/2.2/intro/tutorial02/" target="_blank" rel="noopener">教程第 2 部分</a> 中学过，所以我们试试在视图里使用它。我们在 <code>index()</code> 函数里插入了一些新内容，让它能展示数据库里以发布日期排序的最近 5 个投票问题，以空格分割：</p><pre class=" language-lang-python"><code class="language-lang-python">from django.http import HttpResponsefrom .models import Questiondef index(request):    latest_question_list = Question.objects.order_by('-pub_date')[:5]    output = ', '.join([q.question_text for q in latest_question_list])    return HttpResponse(output)# Leave the rest of the views (detail, results, vote) unchanged</code></pre><p>这里有个问题：页面的设计写死在视图函数的代码里的。如果你想改变页面的样子，你需要编辑 Python 代码。</p><p>所以让我们使用 Django 的模板系统，只要创建一个视图，就可以将页面的设计从代码中分离出来。</p><h3 id="使用模板"><a href="#使用模板" class="headerlink" title="使用模板"></a>使用模板</h3><p>首先，在你的 <code>polls</code> 目录里创建一个 <code>templates</code> 目录。Django 将会自动地在这个目录里查找模板文件。</p><p>你项目的 <a href="https://docs.djangoproject.com/zh-hans/2.2/ref/settings/#std:setting-TEMPLATES" target="_blank" rel="noopener"><code>TEMPLATES</code></a> 配置项描述了 Django 如何载入和渲染模板。默认的设置文件设置了 <code>DjangoTemplates</code> 后端，并将 <a href="https://docs.djangoproject.com/zh-hans/2.2/ref/settings/#std:setting-TEMPLATES-APP_DIRS" target="_blank" rel="noopener"><code>APP_DIRS</code></a> 设置成了 True。这一选项将会让 <code>DjangoTemplates</code>在每个 <a href="https://docs.djangoproject.com/zh-hans/2.2/ref/settings/#std:setting-INSTALLED_APPS" target="_blank" rel="noopener"><code>INSTALLED_APPS</code></a> 文件夹中寻找 “templates” 子目录。这就是为什么尽管我们没有像在第二部分中那样修改 DIRS 设置，Django 也能正确找到 polls 的模板位置的原因。</p><p>在你刚刚创建的 <code>templates</code> 目录里，再创建一个目录 <code>polls</code>，然后在其中新建一个文件 <code>index.html</code> 。换句话说，你的模板文件的路径应该是 <code>polls/templates/polls/index.html</code>。因为 Django 会寻找到对应的 <code>app_directories</code> ，所以你只需要使用 <code>polls/index.html</code> 就可以引用到这一模板了。</p><blockquote><p><strong>模板命名空间</strong></p><p>虽然我们现在可以将模板文件直接放在 <code>polls/templates</code> 文件夹中（而不是再建立一个 <code>polls</code> 子文件夹），但是这样做不太好。Django 将会选择第一个匹配的模板文件，如果你有一个模板文件正好和另一个应用中的某个模板文件重名，Django 没有办法 <em>区分</em> 它们。我们需要帮助 Django 选择正确的模板，最简单的方法就是把他们放入各自的 <em>命名空间</em> 中，也就是把这些模板放入一个和 <em>自身</em> 应用重名的子文件夹里。</p><p>换句话说，Django不会区分<code>templates</code> 文件夹的父文件夹，即需要 <code>templates</code> 文件夹下的子文件夹来区分所属的应用程序。</p></blockquote><p>将下面的代码输入到刚刚创建的模板文件中：</p><pre class=" language-lang-html"><code class="language-lang-html">{ % if latest_question_list % }    <ul>    { % for question in latest_question_list % }        <li><a href="/polls/{{ question.id }}/">{{ question.question_text }}</a></li>    { % endfor % }    </ul>{ % else % }    <p>No polls are available.</p>{ % endif % }</code></pre><p>然后，让我们更新一下 <code>polls/views.py</code> 里的 <code>index</code> 视图来使用模板：</p><pre class=" language-lang-python"><code class="language-lang-python">from django.http import HttpResponsefrom django.template import loaderfrom .models import Questiondef index(request):    latest_question_list = Question.objects.order_by('-pub_date')[:5]    template = loader.get_template('polls/index.html')    context = {        'latest_question_list': latest_question_list,    }    return HttpResponse(template.render(context, request))</code></pre><p>上述代码的作用是，载入 <code>polls/index.html</code> 模板文件，并且向它传递一个上下文(context)。这个上下文是一个字典，它将模板内的变量映射为 Python 对象。</p><p>用你的浏览器访问 <code>/polls/</code> ，你将会看见一个无序列表，列出了我们添加的 “What’s up” 投票问题，链接指向这个投票的详情页。</p><h3 id="一个快捷函数：render"><a href="#一个快捷函数：render" class="headerlink" title="一个快捷函数：render()"></a>一个快捷函数：render()</h3><p>「载入模板，填充上下文，再返回由它生成的 <a href="https://docs.djangoproject.com/zh-hans/2.2/ref/request-response/#django.http.HttpResponse" target="_blank" rel="noopener"><code>HttpResponse</code></a> 对象」是一个非常常用的操作流程。于是 Django 提供了一个快捷函数，我们用它来重写 <code>index()</code> 视图：</p><pre class=" language-lang-python"><code class="language-lang-python">from django.shortcuts import renderfrom .models import Questiondef index(request):    latest_question_list = Question.objects.order_by('-pub_date')[:5]    context = {'latest_question_list': latest_question_list}    return render(request, 'polls/index.html', context)</code></pre><p>注意到，我们不再需要导入 <a href="https://docs.djangoproject.com/zh-hans/2.2/topics/templates/#module-django.template.loader" target="_blank" rel="noopener"><code>loader</code></a> 和 <a href="https://docs.djangoproject.com/zh-hans/2.2/ref/request-response/#django.http.HttpResponse" target="_blank" rel="noopener"><code>HttpResponse</code></a> 。</p><p><code>render()</code>函数以request作为第一个参数，模板的路径为第二个参数，第三个的字典参数为可选参数。然后返回<a href="https://docs.djangoproject.com/zh-hans/2.2/ref/request-response/#django.http.HttpResponse" target="_blank" rel="noopener"><code>HttpResponse</code></a>类型的对象。</p><h3 id="抛出-404-错误"><a href="#抛出-404-错误" class="headerlink" title="抛出 404 错误"></a>抛出 404 错误</h3><p>现在，我们来处理投票详情视图——它会显示指定投票的问题标题。下面是这个视图的代码：</p><pre class=" language-lang-python"><code class="language-lang-python">from django.http import Http404from django.shortcuts import renderfrom .models import Question# ...def detail(request, question_id):    try:        question = Question.objects.get(pk=question_id)    except Question.DoesNotExist:        raise Http404("Question does not exist")    return render(request, 'polls/detail.html', {'question': question})</code></pre><p>这里有个新原则。如果指定问题 ID 所对应的问题不存在，这个视图就会抛出一个 <a href="https://docs.djangoproject.com/zh-hans/2.2/topics/http/views/#django.http.Http404" target="_blank" rel="noopener"><code>Http404</code></a> 异常。</p><p>我们稍后再讨论你需要在 <code>polls/detail.html</code> 里输入什么，但是如果你想试试上面这段代码是否正常工作的话，你可以暂时把下面这段输进去：</p><pre class=" language-lang-html"><code class="language-lang-html">{{ question }}</code></pre><p>这样你就能测试了。</p><p>输入不存在的id会出现类似如下的结果：</p><p><img src="4.png" alt></p><h3 id="一个快捷函数：get-object-or-404"><a href="#一个快捷函数：get-object-or-404" class="headerlink" title="一个快捷函数：get_object_or_404()"></a>一个快捷函数：get_object_or_404()</h3><p>尝试用 <a href="https://docs.djangoproject.com/zh-hans/2.2/ref/models/querysets/#django.db.models.query.QuerySet.get" target="_blank" rel="noopener"><code>get()</code></a> 函数获取一个对象，如果不存在就抛出 <a href="https://docs.djangoproject.com/zh-hans/2.2/topics/http/views/#django.http.Http404" target="_blank" rel="noopener"><code>Http404</code></a> 错误也是一个普遍的流程。Django 也提供了一个快捷函数，下面是修改后的详情 <code>detail()</code> 视图代码：</p><pre class=" language-lang-python"><code class="language-lang-python">from django.shortcuts import get_object_or_404, renderfrom .models import Question# ...def detail(request, question_id):    question = get_object_or_404(Question, pk=question_id)    return render(request, 'polls/detail.html', {'question': question})</code></pre><p><code>get_object_or_404()</code>函数使用Django的数据对象作为第一个参数，使用任意数量的筛选参数作为余下的参数，这些筛选参数会传递给<code>get()</code>函数。如果对象不存在，会引起404错误。</p><p>也有 <a href="https://docs.djangoproject.com/zh-hans/2.2/topics/http/shortcuts/#django.shortcuts.get_list_or_404" target="_blank" rel="noopener"><code>get_list_or_404()</code></a> 函数，工作原理和 <a href="https://docs.djangoproject.com/zh-hans/2.2/topics/http/shortcuts/#django.shortcuts.get_object_or_404" target="_blank" rel="noopener"><code>get_object_or_404()</code></a> 一样，除了 <a href="https://docs.djangoproject.com/zh-hans/2.2/ref/models/querysets/#django.db.models.query.QuerySet.get" target="_blank" rel="noopener"><code>get()</code></a> 函数被换成了 <a href="https://docs.djangoproject.com/zh-hans/2.2/ref/models/querysets/#django.db.models.query.QuerySet.filter" target="_blank" rel="noopener"><code>filter()</code></a> 函数。如果列表为空的话会抛出 <a href="https://docs.djangoproject.com/zh-hans/2.2/topics/http/views/#django.http.Http404" target="_blank" rel="noopener"><code>Http404</code></a> 异常。</p><h3 id="使用模板系统"><a href="#使用模板系统" class="headerlink" title="使用模板系统"></a>使用模板系统</h3><p>回过头去看看我们的 <code>detail()</code> 视图。它向模板传递了上下文变量 <code>question</code> 。下面是 <code>polls/detail.html</code> 模板里正式的代码：</p><pre class=" language-lang-html"><code class="language-lang-html"><h1>{{ question.question_text }}</h1><ul>{ % for choice in question.choice_set.all % }    <li>{{ choice.choice_text }}</li>{ % endfor % }</ul></code></pre><p>模板系统统一使用点符号来访问变量的属性。在示例 <code></code> 中，首先 Django 尝试对 <code>question</code> 对象使用字典查找（也就是使用 <code>obj.get(str)</code> 操作），如果失败了就尝试属性查找（也就是 <code>obj.str</code> 操作），结果是成功了。如果这一操作也失败的话，将会尝试列表查找（也就是 <code>obj[int]</code> 操作）。</p><p>在 <a href="https://docs.djangoproject.com/zh-hans/2.2/ref/templates/builtins/#std:templatetag-for" target="_blank" rel="noopener"><code>{ % for % }</code></a> 循环中发生的函数调用：<code>question.choice_set.all</code> 被解释为 Python 代码 <code>question.choice_set.all()</code> ，将会返回一个可迭代的 <code>Choice</code> 对象，这一对象可以在 <a href="https://docs.djangoproject.com/zh-hans/2.2/ref/templates/builtins/#std:templatetag-for" target="_blank" rel="noopener"><code>{ %for % }</code></a> 标签内部使用。</p><p>查看 <a href="https://docs.djangoproject.com/zh-hans/2.2/topics/templates/" target="_blank" rel="noopener">模板指南</a> 可以了解关于模板的更多信息。</p><h3 id="去除模板中的硬编码-URL"><a href="#去除模板中的硬编码-URL" class="headerlink" title="去除模板中的硬编码 URL"></a>去除模板中的硬编码 URL</h3><p>还记得吗，我们在 <code>polls/index.html</code> 里编写投票链接时，链接是硬编码的：</p><pre class=" language-lang-html"><code class="language-lang-html"><li><a href="/polls/{{ question.id }}/">{{ question.question_text }}</a></li></code></pre><p>问题在于，硬编码和强耦合的链接，对于一个包含很多应用的项目来说，修改起来是十分困难的。然而，因为你在 <code>polls.urls</code> 的 <a href="https://docs.djangoproject.com/zh-hans/2.2/ref/urls/#django.conf.urls.url" target="_blank" rel="noopener"><code>url()</code></a> 函数中通过 name 参数为 URL 定义了名字，你可以使用 <code>{ % url % }</code> 标签代替它：</p><pre class=" language-lang-html"><code class="language-lang-html"><li><a href="{ % url 'detail' question.id % }">{{ question.question_text }}</a></li></code></pre><p>这个标签的工作方式是在 <code>polls.urls</code> 模块的 URL 定义中寻具有指定名字的条目。你可以回忆一下，具有名字 <code>detail</code> 的 URL 是在如下语句中定义的：</p><pre class=" language-lang-python"><code class="language-lang-python">...# the 'name' value as called by the { % url % } template tagpath('<int:question_id>/', views.detail, name='detail'),...</code></pre><p>也就是说，<code>url &#39;detail&#39; question.id</code>返回的是<code>/polls/2/</code>，因为<code>question.id</code>类似于参数被传入<code>&lt;int:question_id&gt;/</code>。</p><p>如果你想改变投票详情视图的 URL，比如想改成 <code>polls/specifics/12/</code> ，你不用在模板里修改任何东西（包括其它模板），只要在 <code>polls/urls.py</code> 里稍微修改一下就行：</p><pre class=" language-lang-python"><code class="language-lang-python">...# added the word 'specifics'path('specifics/<int:question_id>/', views.detail, name='detail'),...</code></pre><h3 id="为-URL-名称添加命名空间"><a href="#为-URL-名称添加命名空间" class="headerlink" title="为 URL 名称添加命名空间"></a>为 URL 名称添加命名空间</h3><p>教程项目只有一个应用，<code>polls</code> 。在一个真实的 Django 项目中，可能会有五个，十个，二十个，甚至更多应用。Django 如何分辨重名的 URL 呢？举个例子，<code>polls</code> 应用有 <code>detail</code> 视图，可能另一个博客应用也有同名的视图。Django 如何知道 <code>{ % url % }</code> 标签到底对应哪一个应用的 URL 呢？</p><p>答案是：在根 URLconf 中添加命名空间。在 <code>polls/urls.py</code> 文件中稍作修改，加上 <code>app_name</code> 设置命名空间：</p><pre class=" language-lang-python"><code class="language-lang-python">from django.urls import pathfrom . import viewsapp_name = 'polls'urlpatterns = [    path('', views.index, name='index'),    path('<int:question_id>/', views.detail, name='detail'),    path('<int:question_id>/results/', views.results, name='results'),    path('<int:question_id>/vote/', views.vote, name='vote'),]</code></pre><p>现在，编辑 <code>polls/index.html</code> 文件，从：</p><pre class=" language-lang-html"><code class="language-lang-html"><li><a href="{ % url 'detail' question.id % }">{{ question.question_text }}</a></li></code></pre><p>修改为指向具有命名空间的详细视图：</p><pre class=" language-lang-html"><code class="language-lang-html"><li><a href="{ % url 'polls:detail' question.id % }">{{ question.question_text }}</a></li></code></pre><h1 id="TO-BE-CONTINUED…"><a href="#TO-BE-CONTINUED…" class="headerlink" title="TO BE CONTINUED…"></a>TO BE CONTINUED…</h1><p><a href="https://docs.djangoproject.com/zh-hans/2.2/intro/tutorial04/" target="_blank" rel="noopener">https://docs.djangoproject.com/zh-hans/2.2/intro/tutorial04/</a></p>]]></content>
      
      
      <categories>
          
          <category> django </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> web </tag>
            
            <tag> django </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python的网络编程</title>
      <link href="/python/python-web/"/>
      <url>/python/python-web/</url>
      
        <content type="html"><![CDATA[<p>本文教你使用Python进行网络编程。</p><h1 id="API请求"><a href="#API请求" class="headerlink" title="API请求"></a>API请求</h1><h2 id="安装requests包"><a href="#安装requests包" class="headerlink" title="安装requests包"></a>安装requests包</h2><pre class=" language-lang-python"><code class="language-lang-python">pip install --user requests</code></pre><h2 id="处理API响应"><a href="#处理API响应" class="headerlink" title="处理API响应"></a>处理API响应</h2><p>我们以找出<code>GitHub</code>上<code>Stars</code>最高的项目为例。代码如下：</p><pre class=" language-lang-python"><code class="language-lang-python">import requestsimport matplotlib.pyplot as plturl = "https://api.github.com/search/repositories?q=language:python&sort=stars"r = requests.get(url)response_dict = r.json()  #解析成字典print('complete_results:', not response_dict['incomplete_results'])  #获取的结果是否完整print('total_count:', response_dict['total_count'])  #GitHub上的Python项目总数repo_len = len(response_dict['items'])  #我们获取到的项目数量print('get_repo_num:', repo_len)forks = [0 for i in range(repo_len)]stars = [0 for i in range(repo_len)]for i in range(repo_len):    item = response_dict['items'][i]    print('rank:', i + 1)    print('name:', item['name'])    forks[i] = item['forks_count']    stars[i] = item['stargazers_count']    print('fork:', forks[i])    print('star:', stars[i])    print('--------------')plt.plot([i for i in range(repo_len)], forks)plt.plot([i for i in range(repo_len)], stars)plt.legend(loc='upper right', labels=['forks', 'stars'])plt.show()</code></pre><p>输出结果：</p><pre><code>complete_results: Truetotal_count: 3660823get_repo_num: 30rank: 1name: awesome-pythonfork: 12560star: 65258--------------rank: 2...略...star: 20375--------------rank: 30name: python-patternsfork: 4350star: 19983--------------</code></pre><p><img src="0.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> web </tag>
            
            <tag> request </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python和NumPy语法回顾</title>
      <link href="/python/learn-python-and-numpy/"/>
      <url>/python/learn-python-and-numpy/</url>
      
        <content type="html"><![CDATA[<h1 id="Python3语法"><a href="#Python3语法" class="headerlink" title="Python3语法"></a>Python3语法</h1><h2 id="语法特点"><a href="#语法特点" class="headerlink" title="语法特点"></a>语法特点</h2><ol><li>句末不加分号</li><li>用tab对齐的方法行使c++里花括号的功能</li><li>太长需要分行书写时，行末尾需要有连接符，新行需要缩进</li><li>输入<code>help(classname)</code>，即可得到那个类的使用帮助</li><li>输入<code>type(variablename)</code>，即可得到该变量的类型</li><li>列表元组字典都可以相互嵌套</li></ol><h2 id="Print-用法"><a href="#Print-用法" class="headerlink" title="Print()用法"></a>Print()用法</h2><ol><li>自动末尾追加<code>\n</code>（可以设置关键字end=””使得末尾不自动追加换行符）</li><li><code>Print(A,B)</code>中间有个空格：A B</li><li>可以直接打印列表[]</li></ol><h2 id="input-用法"><a href="#input-用法" class="headerlink" title="input()用法"></a>input()用法</h2><ol><li>用户输入函数，返回值就是输入的字符串（返回是字符串！），如：<code>str=input(&quot;Please input something:&quot;)</code></li></ol><h2 id="变量的用法"><a href="#变量的用法" class="headerlink" title="变量的用法"></a>变量的用法</h2><ol><li>不用声明</li></ol><h2 id="字符串的用法"><a href="#字符串的用法" class="headerlink" title="字符串的用法"></a>字符串的用法</h2><ol><li><code>s=&quot;abc123&quot;</code>   # 单引号双引号都可以</li><li><code>s[2]</code>取出第3个字符</li><li><code>s.title()</code>    # 暂时每个单词首字母大写</li><li><code>s.upper()</code>    # 暂时大写</li><li><code>s.lower()</code>    # 暂时小写</li><li><code>s1+s2</code>        # 连接字符串</li><li><code>\n，\t</code>等     # 转义符</li><li><code>s.rstrip()</code>   # 暂时去除右边空白</li><li><code>s.lstrip()</code>   # 暂时去除左边空白</li><li><code>s.strip()</code>    # 暂时去除两边空白</li><li><code>str(number)</code>  # 暂时转换为字符串，以便连接为字符串</li></ol><h2 id="数字的用法"><a href="#数字的用法" class="headerlink" title="数字的用法"></a>数字的用法</h2><ol><li>可以计算复数</li><li><code>3/2</code>为1.5</li><li><code>3**2</code>为三的二次方</li><li><code>7%3</code>求余数（7%3==1）</li><li><code>int(&#39;123&#39;)</code>把字符串转整数，不能包含小数点</li><li><code>float(&#39;12.3&#39;)</code>转换为浮点数</li><li><code>+=，-=，*=，/=，%=</code> 同c++的含义(但没有类似自增++的缩写用法)，注意：式子左边的变量要事先定义</li></ol><h2 id="注释的用法"><a href="#注释的用法" class="headerlink" title="注释的用法"></a>注释的用法</h2><ol><li>井号#等效于c++里的//</li><li>三个单引号’’’等效于c++里的/*或*/</li></ol><h2 id="列表-的用法【class的一种】"><a href="#列表-的用法【class的一种】" class="headerlink" title="列表[]的用法【class的一种】"></a>列表[]的用法【class的一种】</h2><ol><li>元素类型可以不同</li><li><code>a=[77,&#39;AB&#39;]</code></li><li><code>a[0]</code>访问第一个元素</li><li><code>a[-2]</code>访问倒数第二个元素</li><li><code>len(a)</code>返回元素个数</li><li><code>a.append(elem)</code>末尾添加元素</li><li><code>a.insert(pos,elem)</code>在位置pos插入元素（列表头是pos==0，列表尾是pos==len(a)）</li><li><code>del a[1]</code>删除列表a里第二个元素</li><li><code>elem = a.pop()</code>弹出（删除）列表尾的元素并赋值给elem</li><li><code>elem = a.pop(i)</code>取出（删除）列表里索引为i（可为负数，表示倒数）的元素并赋值给elem</li><li><code>a.remove(value)</code>删除列表a里第一个值为value的元素</li><li><code>a.sort()</code>永久性的升排序（数字增序或字典顺序）（参数填reverse=True则是降序）</li><li><code>sorted(a)</code>暂时性的升排序</li><li><code>a.reverse()</code>永久性的逆转序列</li><li><code>min(a)</code>返回列表最小值</li><li><code>max(a)</code>返回列表最大值</li><li><code>sum(a)</code>返回元素之和（元素必须是数字）</li><li><code>a[i1:i2]</code>返回子列表（又称切片），范围是索引i1&lt;=i&lt;i2，即不包括i2，好处是i2-i1就是子列表的元素个数</li><li><code>a[:i2]</code>等价于a[0:i2]</li><li><code>a[i1:]</code>等价于a[i1:len(a)]</li><li><code>a[-3:]</code>等价于a[len(a)-3:len(a)]，即返回末尾三个元素组成的列表</li><li><code>b=a[:]</code>列表深复制（因为切片并不与a共用内存空间）</li><li><code>b=a</code>列表浅复制（b是a的引用）</li></ol><h2 id="元组-的用法【class的一种】"><a href="#元组-的用法【class的一种】" class="headerlink" title="元组()的用法【class的一种】"></a>元组()的用法【class的一种】</h2><ol><li>元组里一个元素的值不可修改，但可以给整个元组赋值如：<code>a=(7,8,9,10)</code></li><li><code>a=(1,2,3)</code> # 定义元组a</li><li><code>a[0]</code>调用</li><li>其他用法和列表[]类似</li></ol><h2 id="字典-的用法【class的一种】"><a href="#字典-的用法【class的一种】" class="headerlink" title="字典{}的用法【class的一种】"></a>字典{}的用法【class的一种】</h2><ol><li><p>即键值表，并不关心多对键值对的顺序，可修改</p><pre class=" language-lang-python"><code class="language-lang-python">rect = {'x':10, 'y':5, 0:20}</code></pre><p>上述字典rect里有三个键（key），分别是’x’，’y’和0。作为下标带入rect[key]就可以得到相应的值</p></li><li><p>新建键值对：直接赋值即可（如<code>rect[&#39;newkey&#39;]=&#39;newval&#39;</code>）</p></li><li>删除键值对：如<code>del rect[&#39;newkey&#39;]</code></li></ol><h2 id="集合-的简单介绍【class的一种】"><a href="#集合-的简单介绍【class的一种】" class="headerlink" title="集合{}的简单介绍【class的一种】"></a>集合{}的简单介绍【class的一种】</h2><ol><li>通过函数<code>set(a)</code>可以将数组a的元素去除重复，返回一个集合类型的量</li></ol><h2 id="逻辑的用法"><a href="#逻辑的用法" class="headerlink" title="逻辑的用法"></a>逻辑的用法</h2><ol><li>True真，False假</li><li>空列表==False，非空列表==True（如while mylist然后逐个pop实现逐个取出）</li><li>空字符串==False，非空字符串==True</li><li>==判断等号</li><li>!=不等号</li><li>and与，or或，not非</li><li>in被包含（如elem in a），not in不被包含</li></ol><h2 id="for循环的用法"><a href="#for循环的用法" class="headerlink" title="for循环的用法"></a>for循环的用法</h2><ol><li>基本格式：<pre class=" language-lang-python"><code class="language-lang-python">for elem in arr: print(elem)</code></pre>例子：</li></ol><ul><li><code>for elem in a</code> # a是列表</li><li><code>for i in range(1,11)</code> # i=1~10</li><li><code>for key,val in a.items()</code> # a是字典</li><li><code>for key in a.keys()</code> # a是字典</li><li><code>for val in a.values()</code> # a是字典</li><li><code>for key in a</code> # a是字典(仅遍历键key)</li></ul><ol><li>注意事项：</li></ol><ul><li>for语句行末尾有冒号，下一行tab缩进（不推荐space缩进）</li><li>循环退出后elem的值可访问，且值是a的最后一个元素a[-1]</li><li><code>range(start,end,step)</code>返回迭代对象(区间[A,B))，用作for循环的循环域，不是列表,（但通过<code>list(range(...))</code>可以变为列表）</li><li><code>a=[val**2 for val in range(1,11)]</code>列表解析，用于快速生成列表[1，4，…,100]</li><li><code>dict.items()</code>返回一个元素是元组(key_i,val_i)的dict_items对象（而<code>list(dict.items())</code>才是返回真正的列表）</li><li><code>dict.keys()</code>返回一个元素是键的dict_keys对象（而<code>list(dict.keys())</code>才是真正的列表）</li><li><code>dict.values()</code>返回一个元素是值的dict_values对象（而<code>list(dict.values())</code>才是真正的列表）</li></ul><h2 id="if语句的用法"><a href="#if语句的用法" class="headerlink" title="if语句的用法"></a>if语句的用法</h2><ol><li>例子：<pre class=" language-lang-python"><code class="language-lang-python"> if a==1:     #... elif a==2:     #... else:     #...</code></pre></li></ol><h2 id="while循环的用法"><a href="#while循环的用法" class="headerlink" title="while循环的用法"></a>while循环的用法</h2><ol><li>例子<pre class=" language-lang-python"><code class="language-lang-python"> while boolvar:     #TODO</code></pre></li><li>用break退出while或for循环</li><li>continue跳过此次循环，进入下一轮</li></ol><h2 id="函数的用法"><a href="#函数的用法" class="headerlink" title="函数的用法"></a>函数的用法</h2><ol><li>例子<pre class=" language-lang-python"><code class="language-lang-python"> def 函数名(参数表): # 参数可以是列表（与实参共用内存空间，除非传递副本如a[:]）     # 计算     return ReturnVal # 非必需，ReturnVal可以是字典</code></pre></li><li>有形参与实参之分，实参传值给形参进入函数内部</li><li>参数传递的两种方法：<ul><li>按参数表的顺序依次传递如f(2,5,-5)</li><li>给形参赋值，如f(x=3,y=7) # 赋值顺序不重要</li></ul></li><li>参数可以有默认值，如def f(x,y=0)，默认值要列在最后</li><li>传递任意数量的实参的写法：(“任意数量的参数*inputs”必须放在参数表的末尾)<pre class=" language-lang-python"><code class="language-lang-python"> def f(*inputs): # 将多个输入的参数封装到一个名为inputs的元组里，调用例子：f(4,8,7,1,3)     print(inputs)</code></pre></li><li>传递任意数量的关键字实参的写法：（同理，放在参数表的末尾）<pre class=" language-lang-python"><code class="language-lang-python"> def f(x,**dict):     #获得x和字典dict（调用例子：f('3',name='karbo',age=99)）</code></pre></li></ol><h2 id="类的用法"><a href="#类的用法" class="headerlink" title="类的用法"></a>类的用法</h2><ol><li>例子：<pre class=" language-lang-python"><code class="language-lang-python"> class Rect():  # 约定：首字母大写的是类（推荐驼峰命名法）</code></pre></li><li>类里的函数（方法）必定包含参数self<pre class=" language-lang-python"><code class="language-lang-python">def __init__(self, x=1, y=1):  # 构造函数（不包含return语句） self.x = x #有默认值1 self.y = y #有默认值1 self.is_active = True  # 置默认值def compute_area(self): if self.is_active:     return self.x * self.y return 0</code></pre></li><li>使用方法<pre class=" language-lang-python"><code class="language-lang-python">MYRECT = Rect(3)  #用赋值的方法创建对象（不必传递self参数）print(MYRECT.compute_area())  # 调用函数并打印</code></pre></li><li>继承的例子：<pre class=" language-lang-python"><code class="language-lang-python">class Cube(Rect): #Cube继承自Rect def __init__(self, x=1, y=1, z=1):     super().__init__(x, y) #初始化继承到的内容（通过super()函数的返回值可以访问继承到的东西）     self.z = z</code></pre></li><li>注意事项：<ul><li>子类同名方法会覆盖父类的</li><li>类的成员可以是类</li></ul></li></ol><h2 id="模块的用法"><a href="#模块的用法" class="headerlink" title="模块的用法"></a>模块的用法</h2><ol><li>在模块文件（.py）写入函数，然后在另一个.py文件import模块文件名即可。<br> 如：<pre class=" language-lang-python"><code class="language-lang-python"> #在模块PRINTABC.py def printabc():     print('abc') #调用者main.py import PRINTABC #import模块 PRINTABC.printabc() #要加上作用域PRINTABC.</code></pre></li><li>import其他用法：<ul><li>使用模块别名:import tensorflow as tf # 调用时需要加上tf.</li><li>显式导入特定函数或类:from tensorflow import constant, Session # 调用时不需加tensorflow.</li><li>显式导入特定函数或类并使用别名:from tensorflow import constant as c, Session as s # 调用时不需加tensorflow.</li><li>显式导入所有:from tensorflow import * # 调用时不需加tensorflow.且容易重复命名造成覆盖</li></ul></li><li>模块中也可以import哦</li></ol><h2 id="文件操作的用法-文本操作"><a href="#文件操作的用法-文本操作" class="headerlink" title="文件操作的用法(文本操作)"></a>文件操作的用法(文本操作)</h2><ol><li>【读】</li></ol><ul><li>全部一次性读取：<pre class=" language-lang-python"><code class="language-lang-python">with open('a.txt') as myfile: #使用with使得文件在不再被调用后自动关闭（不用with的写法：打开myfile=open("a.txt")关闭myfile.close()）  print(myfile.read()) #read()函数将内容全部读取</code></pre>文件路径是相对路径或绝对路径</li><li>逐行读取：<pre class=" language-lang-python"><code class="language-lang-python">for line in myfile: #读取一行到line里，注意：它不抛弃末尾的\n，即line字符串末尾有换行符</code></pre></li><li>读取所有行到一个列表中：<pre class=" language-lang-python"><code class="language-lang-python">arr_lines = myfile.readlines() #一行为一个元素的列表，同理不抛弃末尾的\n</code></pre></li></ul><ol><li>【写】<pre class=" language-lang-python"><code class="language-lang-python">with open('a.txt', 'w') as myfile: # 可选项：读r（默认），写w，追加a，读写r+ myfile.write('hello!') # 只将内容写入，并不会自动添加\n</code></pre></li></ol><ul><li>注意事项：<ul><li>以’w’方式会重写文件（不存在则创建文件）</li><li>以’a’方式会追加到文件尾（不存在则创建文件）</li></ul></li></ul><h2 id="存储数据结构（JSON）："><a href="#存储数据结构（JSON）：" class="headerlink" title="存储数据结构（JSON）："></a>存储数据结构（JSON）：</h2><ol><li>完整例子：<pre class=" language-lang-python"><code class="language-lang-python">import jsondef READ_JSON_FILE(FILENAME): with open(FILENAME) as myfile:     return json.load(myfile) #返回读取到的文件内容def WRITE_JSON_FILE(WHAT, FILENAME): with open(FILENAME, 'w') as myfile:     json.dump(WHAT, myfile) #将WHAT写入路径为FILENAME的文件里</code></pre><h2 id="异常机制"><a href="#异常机制" class="headerlink" title="异常机制"></a>异常机制</h2></li><li>例子：<pre class=" language-lang-python"><code class="language-lang-python"> try:     with open('a.txt') as myfile:         contents = myfile.read() except FileNotFoundError: #try失败后     print('Sorry, the file not found.') #换成语句pass可以跳过（pass相当于占位符，不起作用，是空语句） else: #try成功后执行else部分     print('It has', str(len(contents.split())), 'words.')</code></pre></li><li>常见的异常有：</li></ol><div class="table-container"><table><thead><tr><th style="text-align:center">错误类型</th><th style="text-align:center">解释</th></tr></thead><tbody><tr><td style="text-align:center">OverflowError</td><td style="text-align:center">数值运算超出最大限制</td></tr><tr><td style="text-align:center">ZeroDivisionError</td><td style="text-align:center">除(或取模)零 (所有数据类型)</td></tr><tr><td style="text-align:center">IOError</td><td style="text-align:center">输入/输出操作失败</td></tr><tr><td style="text-align:center">IndexError</td><td style="text-align:center">序列中没有此索引(index)</td></tr><tr><td style="text-align:center">FileNotFoundError</td><td style="text-align:center">文件未找到</td></tr><tr><td style="text-align:center">NameError</td><td style="text-align:center">访问一个不存在的变量</td></tr></tbody></table></div><h1 id="NumPy语法"><a href="#NumPy语法" class="headerlink" title="NumPy语法"></a>NumPy语法</h1><h2 id="ndarray基本属性"><a href="#ndarray基本属性" class="headerlink" title="ndarray基本属性"></a>ndarray基本属性</h2><ol><li><code>ndarray.ndim</code>维度</li><li><code>ndarray.shape</code>形状尺寸</li><li><code>ndarray.size</code>元素总数（等于shape的乘积）</li><li><code>ndarray.dtype</code>元素的类型（如：numpy.int32，numpy.int16，numpy.float64）</li><li><code>ndarray.itemsize</code>每个元素所占的字节数（如float64为8，相当于ndarray.dtype.itemsize）</li><li><code>ndarray.data</code>数据内存区域</li></ol><h2 id="ndarray生成"><a href="#ndarray生成" class="headerlink" title="ndarray生成"></a>ndarray生成</h2><p>array()是函数，ndarray()是类</p><ol><li>list转ndarray<pre class=" language-lang-python"><code class="language-lang-python">a = np.array([1,2,3,4])b = np.array([[1,2],[3,4]], dtype=complex) #显式指定为复数类型c = np.array([i for i in range(1, 11)]) #0~10</code></pre></li><li>arange生成<pre class=" language-lang-python"><code class="language-lang-python">a = np.arange(15) #0~14b = np.arange(5, 10) #5~9c = np.arange(1, 10, 0.5) #1~9.5，间隔0.5</code></pre></li><li>zeros、ones、eye、empty生成（默认是np.float64类型）<pre class=" language-lang-python"><code class="language-lang-python">a = np.zeros([2, 3], dtype=np.int64) #指定为np.int64类型，元素全是0b = np.ones([2, 3]) #默认为np.float64类型，元素全是1c = np.eye(3) #默认为np.float64类型，大小为3*3的单位矩阵d = np.empty([2, 3], dtype=complex) #指定为复数，元素未初始化（依据内存状态）</code></pre></li><li>linspace生成<br>返回等间隔分布的数组<pre class=" language-lang-python"><code class="language-lang-python">a = np.linspace(1, 10, 5) #a == [ 1.    3.25  5.5   7.75 10.  ]</code></pre></li></ol><h2 id="ndarray的操作"><a href="#ndarray的操作" class="headerlink" title="ndarray的操作"></a>ndarray的操作</h2><ol><li>reshape<br>注意<code>reshape</code>函数返回的是引用，指向同一内存空间。<br>若要不同请使用深拷贝，请使用深拷贝<code>.copy()</code>。<pre class=" language-lang-python"><code class="language-lang-python">x = np.array([i for i in range(1, 13)]) #生成长度为12的ndarraya = np.reshape(x, [3, 4]) #执行后x的值仍不变b = x.reshape([3, 4]) #执行后x的值仍不变c = x.reshape(3, 4) #执行后x的值仍不变</code></pre>输出效果（a、b和c都一样）：<pre class=" language-lang-python"><code class="language-lang-python">[[ 1  2  3  4][ 5  6  7  8][ 9 10 11 12]]</code></pre></li><li>读取单个元素<pre class=" language-lang-python"><code class="language-lang-python">a[1,2] #取出第2行第3列的元素（建议）a[1][2] #取出第2行第3列的元素（先取出第二行[ 5  6  7  8]，再取出第三个元素7）</code></pre></li><li>读取整行/列<pre class=" language-lang-python"><code class="language-lang-python">a[:,1] #取出第2列（建议）a[:][1] #取出第2行（注意：a[:]等价于a）a[1] # 取出第2行（建议）a[1,:] #取出第2行（建议）a[1][:] #取出第2行（注意：[:]不起作用）</code></pre></li><li>拷贝<br>ndarray深拷贝请使用<code>y=x.copy()</code>，而不是<code>y=x[:]</code>！<br>对数值型的赋值都是深拷贝</li></ol><p>拷贝实验一：</p><pre class=" language-lang-python"><code class="language-lang-python">import numpy as npx = np.array([i for i in range(1, 13)])a = x.reshape([3, 4])b = a #b是a的引用b[0, 0] = 99 #实质上是修改了a输出结果：[[99  2  3  4] [ 5  6  7  8] [ 9 10 11 12]] #a被修改了print(a)</code></pre><p>拷贝实验二：</p><pre class=" language-lang-python"><code class="language-lang-python">import numpy as npx = np.array([i for i in range(1, 13)])a = x.reshape([3, 4])b = a[0] #取出a第1行的引用b[0] = 99 #实质上是修改了a的第1行的第1个元素print(a)输出结果：[[99  2  3  4] [ 5  6  7  8] [ 9 10 11 12]] #a被修改了</code></pre><p>拷贝实验三：</p><pre class=" language-lang-python"><code class="language-lang-python">import numpy as npx = np.array([i for i in range(1, 13)])a = x.reshape([3, 4])b = a[0, 0] #数值型赋值，b不是a[0,0]的引用b = 99 #仅修改了bprint(a)输出结果：[[ 1  2  3  4] [ 5  6  7  8] [ 9 10 11 12]] #a保持原样</code></pre><p>拷贝实验四：</p><pre class=" language-lang-python"><code class="language-lang-python">import numpy as npx = np.array([i for i in range(1, 13)])a = x.reshape([3, 4])b = a[:] #这种操作对ndarray无效，仅对list有效b[0, 0] = 99print(a)输出结果：[[99  2  3  4] [ 5  6  7  8] [ 9 10 11 12]] #a被修改了</code></pre><p>拷贝实验五：</p><pre class=" language-lang-python"><code class="language-lang-python">import numpy as npx = np.array([i for i in range(1, 13)])a = x.reshape([3, 4])b = a.copy() #使用了ndarray的深拷贝b[0, 0] = 99print(a)输出结果：[[1  2  3  4] [ 5  6  7  8] [ 9 10 11 12]] #a保持原样</code></pre><ol><li>连续区域赋值<br>```python<br>import numpy as np<br>x = np.array([i for i in range(1, 13)])<br>y = x.reshape([3, 4]).copy()<br>x[3:6] = 99<br>y[1:3, 0:3] = 99<br>print(x)<br>print(y)</li></ol><p>输出结果：<br>[ 1  2  3 99 99 99  7  8  9 10 11 12]<br>[[ 1  2  3  4]<br> [99 99 99  8]<br> [99 99 99 12]]</p><pre><code>6. 逻辑运算```pythonimport numpy as npx = np.array([i for i in range(1, 13)])y = x.reshape([3, 4]).copy()a = y &gt;= 8b = y.copy()b[a] = 0print(a)print(b)输出结果：[[False False False False] [False False False  True] [ True  True  True  True]][[1 2 3 4] [5 6 7 0] [0 0 0 0]]</code></pre><ol><li>ndarray保存到文件</li></ol><ul><li>单个ndarray的二进制保存（.npy后缀）<br>```python<br>import numpy as np<br>x = np.array([i for i in range(1, 13)]).reshape([3, 4])<br>np.save(‘x.npy’, x)<br>y = np.load(‘x.npy’)<br>print(y)</li></ul><p>输出结果：<br>[[ 1  2  3  4]<br> [ 5  6  7  8]<br> [ 9 10 11 12]]</p><pre><code>- 单个ndarray的文本保存（.txt后缀）```pythonimport numpy as npx = np.array([i for i in range(1, 13)]).reshape([3, 4])np.savetxt(&#39;x.txt&#39;, x)y = np.loadtxt(&#39;x.txt&#39;)print(y)输出结果：[[ 1.  2.  3.  4.] [ 5.  6.  7.  8.] [ 9. 10. 11. 12.]]</code></pre><ul><li>多个ndarray的二进制保存（.npz后缀）<br>```python<br>import numpy as np<br>a = np.array([[1, 2, 3], [4, 5, 6]])<br>b = np.arange(0, 1.0, 0.1)<br>c = np.sin(b)<br>np.savez(“result.npz”, a, b, sin_array=c)<br>r = np.load(“result.npz”)<br>print(r[“arr_0”])<br>print(r[“arr_1”])<br>print(r[“sin_array”])</li></ul><p>输出结果：<br>[[1 2 3]<br> [4 5 6]]<br>[0.  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9]<br>[0.         0.09983342 0.19866933 0.29552021 0.38941834 0.47942554<br> 0.56464247 0.64421769 0.71735609 0.78332691]<br>```</p><h1 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接"></a>相关链接</h1><ol><li><a href="https://docs.python.org/zh-cn/3/" target="_blank" rel="noopener">Python 3.7.3 文档</a></li><li><a href="https://docs.python.org/zh-cn/3/tutorial/" target="_blank" rel="noopener">Python 教程</a></li><li><a href="https://www.numpy.org/devdocs/user/quickstart.html" target="_blank" rel="noopener">NumPy Tutorial</a></li><li><a href="https://www.numpy.org/devdocs/user/numpy-for-matlab-users.html" target="_blank" rel="noopener">NumPy for Matlab users</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> numpy </tag>
            
            <tag> grammar </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一种颜色识别算法</title>
      <link href="/algorithm/colorclassify/"/>
      <url>/algorithm/colorclassify/</url>
      
        <content type="html"><![CDATA[<h1 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h1><ol><li>将RGB色彩空间转化为HSV色彩空间</li><li>对HSV立方体色彩空间进行变换</li><li>度量色彩距离</li><li>对于在阈值范围内的色彩赋予标签</li><li>若该色彩在所有备选颜色的阈值范围外，则拒绝识别</li></ol><p><img src="2.png" alt></p><h1 id="色彩空间RGB转HSV"><a href="#色彩空间RGB转HSV" class="headerlink" title="色彩空间RGB转HSV"></a>色彩空间RGB转HSV</h1><p>通过调用<code>rgb2hsv</code>函数来实现转换。</p><p>输入参数r、g和b范围均在0到1，输出参数h范围0到360，输出参数s和v范围在0到1</p><pre class=" language-lang-c++"><code class="language-lang-c++">/* RGB转HSV *//* 0~1, 0~1, 0~1 -> 0~360, 0~1, 0~1 */void rgb2hsv(float r, float g, float b, float& h, float& s, float& v){    float max_val = MAX3(r, g, b), min_val = MIN3(r, g, b);    float diff = max_val - min_val;    // 计算H    if (max_val == min_val)        h = 0;    else if (max_val == r)    {        if (g >= b)            h = 60 * ((g - b) / diff) + 0;        else            h = 60 * ((g - b) / diff) + 360;    }    else if (max_val == g)    {        h = 60 * ((b - r) / diff) + 120;    }    else if (max_val == b)    {        h = 60 * ((r - g) / diff) + 240;    }    // 计算S    if (max_val == 0)        s = 0;    else        s = (diff / max_val);    // 计算V    v = max_val;}</code></pre><h1 id="HSV色彩空间变换"><a href="#HSV色彩空间变换" class="headerlink" title="HSV色彩空间变换"></a>HSV色彩空间变换</h1><h2 id="HSV立方体色彩空间"><a href="#HSV立方体色彩空间" class="headerlink" title="HSV立方体色彩空间"></a>HSV立方体色彩空间</h2><p>传统的HSV立方体色彩空间并不能很好地度量颜色的相似程度。</p><blockquote><p>举个例子，考虑四种颜色。（假定h、s和v范围均在0~1）</p><p>颜色A为<code>h=0, s=0.1, v=0.5</code>，颜色B为<code>h=0.5, s=0.1, v=0.5</code></p><p>采用<code>L2</code>距离度量，颜色A到B的距离为0.5</p><p>颜色C为<code>h=0, s=0.9, v=0.5</code>，颜色D为<code>h=0.5, s=0.9, v=0.5</code></p><p>采用<code>L2</code>距离度量，颜色C到D的距离仍为0.5</p></blockquote><p><img src="1.png" alt></p><p>这两个距离居然相同！这显然是不符合直觉的。我们直觉上应当认为A到B的距离应该小于C到D的距离，因为C和D两者色彩差异更大，而A和B都近似是某种灰色。</p><p>在实践应用上，我们感兴趣的往往不是灰度，而是某一种鲜艳的颜色。而且灰度颜色受光照的影响比较大，难以用于实践。</p><p>造成这种度量不均衡的原因是单纯在HSV立方体色彩空间中颜色的分布是不均衡的。如下图，我们发现灰度占据了绝大多数的空间，造成灰度对色彩距离度量的影响比较大。</p><p><img src="2.png" alt></p><p>所以我们要做的就是减少灰度对颜色距离度量的影响，突出不同色相的差异性。</p><p>接下来我们将其变换到HSV圆盘色彩空间，以突出不同色相区别。</p><h2 id="HSV圆盘色彩空间"><a href="#HSV圆盘色彩空间" class="headerlink" title="HSV圆盘色彩空间"></a>HSV圆盘色彩空间</h2><p>我们将HSV立方体色彩空间变换到锥体中。</p><p><img src="3.png" alt></p><p>考虑到色彩距离应该具备光照不变性，我们丢弃掉V轴，仅保留H和S分量构成的圆盘结构。</p><p><img src="4.png" alt></p><p>我们在圆盘中度量点A、B的距离便得到颜色相似度的度量值。</p><p><img src="5.png" alt></p><h1 id="度量颜色距离"><a href="#度量颜色距离" class="headerlink" title="度量颜色距离"></a>度量颜色距离</h1><p>在HSV圆盘色彩空间中，我们可以采用直线距离度量方式，也可以采用弧线形的距离度量方式。</p><p>为简便起见，我们采用直线型的距离度量。</p><p>为了更好的调节色彩识别的鲁棒性因素，我们引入调节参数<code>COLOR_ROBUST</code>，用于调节色彩饱和度的鲁棒性。该参数值越大，则将会具有更大的允许饱和度变化的范围；该参数越小，则对饱和度变化允许的范围越小。</p><p>其中参数<code>COLOR_ROBUST</code>的实现是通过调节半径<code>s</code>的缩放来实现的。我们在半径<code>s</code>方向上对其进行套用函数：</p><script type="math/tex; mode=display">{1-(1-x)^n}</script><p>其中n就是<code>COLOR_ROBUST</code>参数。</p><p><img src="6.png" alt></p><p>计算颜色距离的代码如下：</p><pre class=" language-lang-c++"><code class="language-lang-c++">/* 计算颜色距离(输入范围均在0~1) */float getColorDistance(float h, float s, float h_dst, float s_dst){    float x_src, y_src;    float x_dst, y_dst;    x_src = (1 - powf(1 - s, COLOR_ROBUST)) * cos(h * 2 * CV_PI);    y_src = (1 - powf(1 - s, COLOR_ROBUST)) * sin(h * 2 * CV_PI);    x_dst = (1 - powf(1 - s_dst, COLOR_ROBUST)) * cos(h_dst * 2 * CV_PI);    y_dst = (1 - powf(1 - s_dst, COLOR_ROBUST)) * sin(h_dst * 2 * CV_PI);    return sqrt(pow(x_src - x_dst, 2) + pow(y_src - y_dst, 2));}</code></pre><h1 id="颜色距离阈值"><a href="#颜色距离阈值" class="headerlink" title="颜色距离阈值"></a>颜色距离阈值</h1><h2 id="阈值计算原理"><a href="#阈值计算原理" class="headerlink" title="阈值计算原理"></a>阈值计算原理</h2><p>由于颜色是任意指定的，如果选取固定的距离阈值将不能广泛适应实际情况。我们在这里应该采取一种自动的方式来计算这个阈值。</p><p>标准颜色类似于聚类中心，我们把它抽象为颜色空间中的一个点，两两之间的距离抽象为线段。那么这个阈值可以用下图的方式计算。</p><p><img src="7.png" alt></p><p>这个阈值就是上图中各个圆的半径。原则是圆的半径从小到大依次选取。</p><ol><li><p>一开始选取最小的半径必定是由最短的边决定的，半径为最短边的一半。如上图的圆A。</p></li><li><p>第二个圆是与第一个相邻的圆，半径等于第一个圆的半径，如上图的圆B。</p></li><li>然后就从剩余的可选择半径中选取半径较小的，即圆C。</li><li>最后选取圆D。</li></ol><h2 id="实现代码"><a href="#实现代码" class="headerlink" title="实现代码"></a>实现代码</h2><p>如下的<code>initColorInfo</code>函数实现了颜色信息的初始化功能。<br>调用该函数后，二维数组<code>color_table</code>存储了各个标准颜色的HSV值，数组<code>color_range_table</code>存储了颜色阈值信息。</p><blockquote><p><code>COLOR_NUM</code>是颜色数量，在此例子中<code>#define COLOR_NUM 6</code><br><code>color_table</code>的定义：<code>float color_table[COLOR_NUM][3]</code><br><code>color_range_table</code>的定义：<code>float color_range_table[COLOR_NUM]</code></p></blockquote><pre class=" language-lang-c++"><code class="language-lang-c++">/* 初始化颜色信息 */void initColorInfo(){    float h_dst, s_dst, v_dst;    for (int i = 0; i < COLOR_NUM; ++i)    {        // RGB空间        switch (i)        {        case 0: // Unknown(White)            color_table[i][0] = 255;            color_table[i][1] = 255;            color_table[i][2] = 255;            break;        case 1: // Red            color_table[i][0] = 148;            color_table[i][1] = 19;            color_table[i][2] = 24;            break;        case 2: // Orange            color_table[i][0] = 198;            color_table[i][1] = 115;            color_table[i][2] = 35;            break;        case 3: // Yellow            color_table[i][0] = 177;            color_table[i][1] = 152;            color_table[i][2] = 23;            break;        case 4: // Green            color_table[i][0] = 45;            color_table[i][1] = 93;            color_table[i][2] = 19;            break;        case 5: // Blue            color_table[i][0] = 39;            color_table[i][1] = 92;            color_table[i][2] = 132;            break;        }        // 转化为HSV空间        rgb2hsv(color_table[i][0] / 255.0, color_table[i][1] / 255.0, color_table[i][2] / 255.0, h_dst, s_dst, v_dst);        color_table[i][0] = h_dst / 360.0f;        color_table[i][1] = s_dst;        color_table[i][2] = v_dst;    }    // 计算可识别的颜色距离    float dist;    float min_dist;    float adjMat[COLOR_NUM][COLOR_NUM]; // 邻接矩阵(非对称)    bool activeNode[COLOR_NUM];    // 初始化邻接矩阵    for (int i = 0; i < COLOR_NUM; ++i)    {        for (int j = i + 1; j < COLOR_NUM; ++j)        {            dist = getColorDistance(color_table[i][0], color_table[i][1], color_table[j][0], color_table[j][1]);            adjMat[i][j] = dist;        }    }    // 初始化activeNode    for (int i = 0; i < COLOR_NUM; ++i) activeNode[i] = true;    // 初始化color_range_table    for (int i = 0; i < COLOR_NUM; ++i) color_range_table[i] = 0;    // 计算距离    while (1)    {        // 求出最短距离        min_dist = 1e9;        for (int i = 0; i < COLOR_NUM; ++i)        {            for (int j = i + 1; j < COLOR_NUM; ++j)            {                dist = adjMat[i][j];                if ((activeNode[i] == false) ^ (activeNode[j] == false)) dist *= 2;                if (dist>1e-6 && dist < min_dist)                {                    min_dist = dist;                }            }        }        // 一起减去最短距离        for (int i = 0; i < COLOR_NUM; ++i)        {            for (int j = i + 1; j < COLOR_NUM; ++j)            {                if (adjMat[i][j] > 1e-6)                {                    if ((activeNode[i] == false) ^ (activeNode[j] == false)) adjMat[i][j] -= min_dist / 2;                    else adjMat[i][j] -= min_dist;                }            }        }        // 更新color_range_table        for (int i = 0; i < COLOR_NUM; ++i)        {            if (activeNode[i] == true)            {                color_range_table[i] += min_dist / 2;            }        }        // 更新有效结点        for (int i = 0; i < COLOR_NUM; ++i)        {            for (int j = i + 1; j < COLOR_NUM; ++j)            {                if (adjMat[i][j] < 1e-6)                {                    activeNode[i] = false;                    activeNode[j] = false;                }            }        }        // 退出条件        int activeNodeNum = 0;        for (int i = 0; i < COLOR_NUM; ++i)        {            activeNodeNum += int(activeNode[i]);        }        if (activeNodeNum == 0) break;    }}</code></pre><h1 id="颜色识别"><a href="#颜色识别" class="headerlink" title="颜色识别"></a>颜色识别</h1><p>当距离小于阈值时则判定为该颜色。如果都不符合，则拒绝识别。</p><blockquote><p>代码中使用参数<code>COLOR_RANGE_A</code>来控制颜色的容限，值越接近1代表颜色的可变化性越大，值越接近0代表容不得颜色变化过大。<br><code>ColorType</code>是颜色的枚举类型数组</p></blockquote><pre class=" language-lang-c++"><code class="language-lang-c++">/* 从HSV识别颜色 *//* H = 0~360, S = 0~1, V = 0~1 */ColorType hsvColorReg(float h, float s, float v){    float dist;    h /= 360.0f;    for (int i = 0; i < COLOR_NUM; ++i)    {        dist = getColorDistance(h, s, color_table[i][0], color_table[i][1]);        if (dist <= COLOR_RANGE_A * color_range_table[i])        {            return ColorType(i);        }    }    return Unknown;}</code></pre>]]></content>
      
      
      <categories>
          
          <category> algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> algorithm </tag>
            
            <tag> classifier </tag>
            
            <tag> ml </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>在Kaggle免费使用GPU训练自己的神经网络</title>
      <link href="/dl/kaggle-gpu/"/>
      <url>/dl/kaggle-gpu/</url>
      
        <content type="html"><![CDATA[<h1 id="Kaggle是什么"><a href="#Kaggle是什么" class="headerlink" title="Kaggle是什么"></a>Kaggle是什么</h1><p><code>Kaggle</code>是一个数据建模和数据分析竞赛平台。企业和研究者可在其上发布数据，统计学者和数据挖掘专家可在其上进行竞赛以产生最好的模型。</p><p><img src="0.png" alt></p><p>在<code>Kaggle</code>，你可以：</p><ol><li><p>参加竞赛赢取奖金。<code>Kaggle</code>上会发布一些赛题，做的好会赢得奖金。</p></li><li><p>下载数据集。<code>Kaggle</code>上包含了众多的数据集供大家免费下载，常见的数据集都可以在上面找到。</p></li><li><p>学习别人的代码。类似<code>GitHub</code>，你可以在<code>Kaggle</code>上学习冠军的代码来强化数据科学技能。</p></li><li><p>免费使用计算资源。<code>Kaggle</code>的<code>Kernels</code>功能允许你在浏览器编程、并通过服务器的<code>GPU</code>来加速你的计算。</p></li><li><p>讨论交流学习。<code>Kaggle</code>上有论坛交流功能，允许你与相同的爱好者一起交流学习。</p></li><li><p>学习<code>Python</code>、<code>ML</code>、<code>Pandas</code>、<code>DL</code>等技能。<code>Kaggle</code>上提供了免费的微课给大家学习，供初学者快速入门学习。</p></li></ol><p>本篇文章侧重点是第<code>4</code>条，教你如何将自己的代码丢到<code>Kaggle</code>上训练。</p><p>注意，<code>Kaggle</code>目前只支持<code>Python</code>和<code>R</code>两种编程语言。</p><h1 id="Kernel硬件配置"><a href="#Kernel硬件配置" class="headerlink" title="Kernel硬件配置"></a>Kernel硬件配置</h1><p><code>GPU</code>：Nvidia Tesla P100-PCIE-16GB 1.3285GHz</p><p><code>GPU连续使用时间</code>：6h</p><p><code>CPU Frequency</code>： 2.3GHz</p><p><code>RAM</code>：14GB</p><p><code>Disk</code>：5.2GB</p><h1 id="使用教程"><a href="#使用教程" class="headerlink" title="使用教程"></a>使用教程</h1><h2 id="基本介绍"><a href="#基本介绍" class="headerlink" title="基本介绍"></a>基本介绍</h2><ol><li><p>登陆<code>Kaggle</code><a href="https://www.kaggle.com/" target="_blank" rel="noopener">官网</a>，注册账号并登陆。在<code>Kaggle</code>注册账号是免费的。</p></li><li><p>点击导航栏的<code>Kernels</code></p></li></ol><p><img src="2.png" alt></p><ol><li>点击页面上部的<code>New Kernel</code>来创建一个新的<code>Kernel</code>。粗略地说<code>Kernel</code>就是一个代码的工程项目。</li></ol><p><img src="3.png" alt></p><ol><li>点击左边的<code>Script</code>来创建一个脚本。这个脚本就是你项目运行的主要文件。</li></ol><p><img src="4.png" alt></p><ol><li>顶部的标题栏的功能。</li></ol><p><img src="5.png" alt></p><ol><li>侧边状态栏的主要功能。<code>Sessions</code>显示资源占用状态，<code>Versions</code>显示版本管理，<code>Draft Environment</code>显示你上传数据（注意：上传后该区域只读，不能写），<code>Settings</code>显示设置（如<code>GPU</code>开关、包的管理）</li></ol><p><img src="6.png" alt></p><ol><li>底部状态栏功能。</li></ol><p><img src="7.png" alt></p><p>用完<code>Kernel</code>建议点击类似电源键的按钮关闭<code>Kernel</code>哦（关闭后所有输出文件将会丢失）</p><ol><li>代码输入窗口。它已预先帮你输入一些示例代码，可以删掉重写。</li></ol><p><img src="8.png" alt></p><h2 id="使用示例"><a href="#使用示例" class="headerlink" title="使用示例"></a>使用示例</h2><p>我们以<code>Tensorflow</code>平台的<code>YoloV3-Tiny</code>模型在数据集<code>VOC2007</code>的训练为例，介绍如何使用<code>Kaggle</code>训练我们的模型，并保存结果，将模型下载到本地。</p><p>提示：<code>Kaggle</code>已经为我们准备好常用的环境了，无需我们从头搭建开发环境。一般直接用就好了。</p><h3 id="文件准备"><a href="#文件准备" class="headerlink" title="文件准备"></a>文件准备</h3><p>首先我们要在本地弄好相关文件，再上传到<code>Kaggle</code>上去。</p><p>本地的准备参考这篇：<a href="https://my.oschina.net/u/876354/blog/1927881" target="_blank" rel="noopener">【AI实战】动手训练自己的目标检测模型（YOLO篇）</a></p><p>有关<code>YOLO</code>参考这篇：<a href="https://karbo.online/dl/yolo_starter/#more" target="_blank" rel="noopener">用YOLO实现目标检测</a></p><p>然后按照实际情况修改<code>train.py</code>的相关参数，例如将<code>batch_size</code>改成<code>128</code>，<code>epochs</code>改小一点等等。</p><p>注意训练时间不能超过<code>6</code>个小时，否则<code>Kaggle</code>会自动关闭你的<code>Kernel</code>。</p><p>并且<code>Keras</code>版的<code>YOLO</code>的标签文件与<code>Darknet</code>版的不同，标签文件要重新生成。然后执行：</p><pre><code> cat 2007_train.txt 2007_val.txt &gt; train.txt</code></pre><p>即我们使用验证集和训练集混合起来一起训练，最后替换下路径前缀。</p><p>但有以下几点要注意下：</p><ol><li><p>上传后不能在线修改你上传的东西，只能删除该压缩包（删除方法见第三节：再次训练）后重新上传（如果数据量巨大，重新上传十分费时）。所以最好需要确保第一次上传的东西就没有问题，否则更改会比较繁琐。</p></li><li><p>上传时，建议是分别上传几样东西（分别压缩打包上传）：</p><ul><li>模型的配置文件</li><li>训练的数据文件</li><li>模型<code>.h5</code>文件</li></ul></li></ol><p>上传方式：点击右侧白色的侧边状态栏中的<code>+ Add Data</code>按钮，在弹出的窗口中，点击右上角的<code>Upload</code>，然后选择文件去上传（只能上传单个文件，这就是为什么叫你打包压缩的原因）。</p><p>上传后，<code>Kaggle</code>会自动帮你解压缩，点击右边的文件树，点选其中的一个文件，会在左侧弹出白色的文件管理弹窗，弹窗的上端会显示该文件的路径：</p><p><img src="9.png" alt></p><p>点击中间的那个蓝色的按钮你可以复制路径到剪切板中。</p><p>当你上传了多个压缩包或文件时，路径的命名规则一般是这样的：</p><ul><li>对于上传了文件：<code>../input/数据集的名字/上传的文件名字</code></li><li>对于上传了压缩包：<code>../input/数据集的名字/压缩包的名字/压缩包底下的路径</code></li></ul><ol><li><p>其中上述的 <code>图片路径的</code>.txt<code>文件</code> 不能单纯按照<a href="https://my.oschina.net/u/876354/blog/1927881" target="_blank" rel="noopener">【AI实战】动手训练自己的目标检测模型（YOLO篇）</a>来做，你要将路径替换成上述第二点描述的那样。因为你执行的主脚本文件并不是在你上传的东西里面，你需要使用类似<code>../input/XXX/XXX</code>的格式来调用你上传的东西。</p></li><li><p>对于脚本中的文件路径也是如此，类似于上述的第三点来做。否则会提示会找不到你上传的文件。实际上有关路径的一切东西都要按照上述的路径规则来做，否则就找不到文件。</p></li><li><p>如果提示<code>import</code>时找不到文件，这是因为你上传的包没有加入系统变量，那么你需要：</p><pre class=" language-lang-python"><code class="language-lang-python">import syskaggle_path_prefix = "../input/keras-yolov3tiny-voc2007/keras-yolo3/"sys.path.append(kaggle_path_prefix)</code></pre></li></ol><p>这里请根据你的实际情况修改上述<code>kaggle_path_prefix</code>的值。</p><p>这里<code>kaggle_path_prefix</code>目录下需要包含那个你刚刚上传的压缩包里名叫<code>yolo3</code>的<code>Python</code>包的文件夹。</p><ol><li>保存文件的路径请直接填写文件名，像这样：</li></ol><pre class=" language-lang-python"><code class="language-lang-python">model.save_weights('trained_weights_final.h5')</code></pre><p>这是因为<code>input</code>文件夹是只读的，且保存到其他地方去无法输出下载，你也找不到输出的文件。况且当<code>Kernel</code>关闭后你的一切东西就会丢失。</p><h3 id="运行并提交"><a href="#运行并提交" class="headerlink" title="运行并提交"></a>运行并提交</h3><p>点击顶部标题栏亮起的蓝色<code>Commit</code>按钮，以运行全部代码并保存结果，最后它会保存你输出的文件。</p><p>如果允许的窗口不慎点没了，可以右侧的<code>Versions</code>中，点击：</p><p><img src="10.png" alt></p><p>重新弹出运行的窗口（除非你点了<code>Cancel commit</code>）。</p><p><img src="1.png" alt></p><p>运行完毕后，点击：</p><p><img src="11.png" alt></p><p>来打开<code>Kernel</code>页面。</p><p>如果你有输出文件，在左侧的：</p><p><img src="12.png" alt></p><p>点击<code>Output</code>就可以切换到输出的文件列表，然后就可以下载你输出的文件啦，选中你想要的模型下载即可。</p><p>如果运行出错，请点击上图所示的<code>Log</code>查看错误日志（有必要时点击<code>Download Log</code>按钮下载日志到本地），按照错误提示修复错误即可。</p><h3 id="再次训练"><a href="#再次训练" class="headerlink" title="再次训练"></a>再次训练</h3><p>只需将原本的模型文件数据集删除，然后再添加上传上去，再次<code>Commit</code>就好了。</p><p>删除数据集的步骤：</p><ol><li><p>点击数据集旁边的那个红色的叉叉，将数据集从当前<code>Kernel</code>移除</p></li><li><p>点击自己的头像，进入<code>My Profile</code>页面，然后点击<code>Datasets</code></p></li></ol><p><img src="13.png" alt></p><ol><li>然后点击<code>Settings</code></li></ol><p><img src="14.png" alt></p><ol><li>最后点击<code>Delete Dataset</code>并确认即可</li></ol><p><img src="15.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> dl </category>
          
      </categories>
      
      
        <tags>
            
            <tag> dl </tag>
            
            <tag> kaggle </tag>
            
            <tag> gpu </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>工具</title>
      <link href="/tools/tools/"/>
      <url>/tools/tools/</url>
      
        <content type="html"><![CDATA[<h1 id="数学"><a href="#数学" class="headerlink" title="数学"></a>数学</h1><blockquote><p><a href="https://www.geogebra.org/" target="_blank" rel="noopener">GeoGebra绘图</a><br><a href="https://www.wolframalpha.com/" target="_blank" rel="noopener">Wolfram|Alpha问答系统</a><br><a href="https://calcme.com/a" target="_blank" rel="noopener">CalcMe在线计算</a></p></blockquote><h1 id="图片处理类"><a href="#图片处理类" class="headerlink" title="图片处理类"></a>图片处理类</h1><blockquote><p><a href="https://www.gaoding.com/koutu" target="_blank" rel="noopener">在线抠图（国内）</a><br><a href="https://www.remove.bg/" target="_blank" rel="noopener">在线抠图（国外）</a></p><p><a href="https://zhitu.isux.us/" target="_blank" rel="noopener">图片压缩（国内）</a><br><a href="https://recompressor.com/" target="_blank" rel="noopener">图片压缩（国外）</a></p><p><a href="http://bigjpg.com/" target="_blank" rel="noopener">图片放大（国内）</a><br><a href="https://bulkresizephotos.com/" target="_blank" rel="noopener">图片放大（国外）</a></p><p><a href="https://zh.vectormagic.com/" target="_blank" rel="noopener">位矢转换（国内）</a><br><a href="https://www.vectorization.org/" target="_blank" rel="noopener">位矢转换（国外）</a></p></blockquote><h1 id="特效类"><a href="#特效类" class="headerlink" title="特效类"></a>特效类</h1><blockquote><p><a href="https://pissang.github.io/little-big-city/" target="_blank" rel="noopener">地球城市卡通化</a><br><a href="http://planetmaker.wthr.us/#" target="_blank" rel="noopener">太空看地球仿真</a><br><a href="https://codepen.io/pissang/full/geajpX" target="_blank" rel="noopener">渐变波浪生成</a><br><a href="https://codepen.io/Yakudoo/full/rJjOJx" target="_blank" rel="noopener">彩色流动生成</a><br><a href="https://pissang.github.io/papercut-box-art/" target="_blank" rel="noopener">等高层风格生成</a><br><a href="https://coolbackgrounds.io/" target="_blank" rel="noopener">简洁背景图制作</a><br><a href="https://trianglify.io/" target="_blank" rel="noopener">三角形渐变图生成</a></p></blockquote><h1 id="文字云"><a href="#文字云" class="headerlink" title="文字云"></a>文字云</h1><blockquote><p><a href="https://wordart.com/" target="_blank" rel="noopener">文字云制作1</a><br><a href="https://www.jasondavies.com/wordcloud/" target="_blank" rel="noopener">文字云制作2</a><br><a href="http://word2art.com/" target="_blank" rel="noopener">文字云艺术图制作</a></p></blockquote><h1 id="文件转换"><a href="#文件转换" class="headerlink" title="文件转换"></a>文件转换</h1><blockquote><p><a href="https://cn.office-converter.com/" target="_blank" rel="noopener">文档、视频、音乐、图片等转换（国内）</a><br><a href="https://cloudconvert.com/" target="_blank" rel="noopener">文件转换（国外）</a></p></blockquote><h1 id="PDF操作"><a href="#PDF操作" class="headerlink" title="PDF操作"></a>PDF操作</h1><blockquote><p><a href="https://smallpdf.com/cn" target="_blank" rel="noopener">PDF操作（国内）</a><br><a href="https://pdfcandy.com/" target="_blank" rel="noopener">PDF操作（国外）</a></p></blockquote><h1 id="在线PS"><a href="#在线PS" class="headerlink" title="在线PS"></a>在线PS</h1><blockquote><p><a href="http://ps.xunjiepdf.com/" target="_blank" rel="noopener">在线PS（国内）</a><br><a href="https://www.photopea.com/" target="_blank" rel="noopener">在线PS（国外）</a></p></blockquote><h1 id="二维码工具"><a href="#二维码工具" class="headerlink" title="二维码工具"></a>二维码工具</h1><blockquote><p><a href="https://cli.im/" target="_blank" rel="noopener">二维码工具</a></p></blockquote><h1 id="Windows-Office"><a href="#Windows-Office" class="headerlink" title="Windows/Office"></a>Windows/Office</h1><blockquote><p><a href="https://msdn.itellyou.cn/" target="_blank" rel="noopener">MSDN</a><br><a href="http://kms.cangshui.net/" target="_blank" rel="noopener">KMS激活</a><br><a href="https://otp.landian.vip/zh-cn/" target="_blank" rel="noopener">Office Tool Plus</a></p></blockquote><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><blockquote><p><a href="https://www.zhuangbi.info/" target="_blank" rel="noopener">图片表情包搜索</a><br><a href="http://geektyper.com/" target="_blank" rel="noopener">黑客打代码装逼网页</a><br><a href="https://colordrop.io/" target="_blank" rel="noopener">纯色配色方案</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> tools </category>
          
      </categories>
      
      
        <tags>
            
            <tag> tools </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>用YOLO实现目标检测</title>
      <link href="/dl/yolo-starter/"/>
      <url>/dl/yolo-starter/</url>
      
        <content type="html"><![CDATA[<h1 id="Why-YOLO"><a href="#Why-YOLO" class="headerlink" title="Why YOLO?"></a>Why YOLO?</h1><p>You only look once (YOLO)是顶尖的实时目标检测模型。</p><p>下面是YOLO与其他模型的性能对比。</p><p><img src="https://pjreddie.com/media/image/map50blue.png" alt></p><p>可以看出YOLO 具有耗时较少，准确率不低的优点。</p><h1 id="配置环境"><a href="#配置环境" class="headerlink" title="配置环境"></a>配置环境</h1><p>建议环境：Win10、支持CUDA的Nvidia显卡、Python3、CUDA&gt;=9.0、CUDNN&gt;=7.0、VS2015、OPENCV&lt;4.0</p><p>详细操作步骤参考：<br><a href="https://blog.csdn.net/sinat_26940929/article/details/80342660" target="_blank" rel="noopener">Yolov3+windows10+VS2015部署安装</a><br><a href="https://github.com/AlexeyAB/darknet#how-to-compile-on-windows-legacy-way" target="_blank" rel="noopener">How to compile on Windows (legacy way)</a></p><p>编译时可能遇到形如compute_75的错误，解决方法：用文本的方式打开darknet.vcxproj文件，将所有的compute_75替换为compute_50，将所有的sm_75替换为sm_50，具体替换成什么，请参考<a href="https://github.com/tpruvot/ccminer/wiki/Compatibility" target="_blank" rel="noopener">Compatibility</a></p><h1 id="YOLO初体验"><a href="#YOLO初体验" class="headerlink" title="YOLO初体验"></a>YOLO初体验</h1><h2 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h2><p>这一步我们尝试使用下刚刚编译好的YOLO。</p><p>由于可能缺少模型的权重文件，我们从这里下载<a href="https://pjreddie.com/media/files/yolov3.weights" target="_blank" rel="noopener">YOLO-V3权重文件（236MB）</a></p><p>然后将目录切换到<code>D:\darknet-master\build\darknet\x64</code>，打开命令行，输入以下语句：</p><pre><code>./darknet.exe detect cfg/yolov3.cfg yolov3.weights data/dog.jpg</code></pre><p>正常情况下会得到以下效果：</p><p><img src="1.png" alt></p><p>同时也会得到<code>predictions.jpg</code>保存在相同目录下。</p><p>运行一次模型需要：</p><ul><li>配置文件（.cfg）</li><li>权重文件（.weights）</li><li>被测图片</li></ul><p>同时尝试将上述语句最后的<code>data/dog.jpg</code>分别替换为<code>data/eagle.jpg</code>, <code>data/dog.jpg</code>, <code>data/person.jpg</code>, or <code>data/horses.jpg</code>，查看效果吧。</p><p>上述语句中的<code>detect</code>是一种缩写，上述语句也等同于</p><pre><code>./darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights data/dog.jpg</code></pre><p>当然也可以载入一次模型进行多次预测，输入以下指令（就是去掉图片选项）：</p><pre><code>./darknet.exe detect cfg/yolov3.cfg yolov3.weights</code></pre><p>然后它会提示你输入图片路径：</p><p><img src="2.png" alt></p><p>输入路径后回车，按<code>Ctrl+C</code>退出输入状态。</p><p>除此之外，<code>YOLO</code>还提供设定阈值方法来剔除置信度过低的结果。例如若想显示所有结果则使用以下代码（此处阈值设置为0）：</p><pre><code>./darknet.exe detect cfg/yolov3.cfg yolov3.weights data/dog.jpg -thresh 0</code></pre><p>默认的阈值是0.25。</p><h2 id="Tiny-YOLOv3"><a href="#Tiny-YOLOv3" class="headerlink" title="Tiny YOLOv3"></a>Tiny YOLOv3</h2><p>首先下载<code>Tiny YOLOv3</code>的<a href="https://pjreddie.com/media/files/yolov3-tiny.weights" target="_blank" rel="noopener">权重文件（34MB）</a>，丢到与<code>darknet.exe</code>同级的目录下。</p><p>使用以下命令运行：</p><pre><code>./darknet.exe detect cfg/yolov3-tiny.cfg yolov3-tiny.weights data/dog.jpg</code></pre><p><img src="3.png" alt></p><p>可以看到<code>tiny</code>版本的精度略低，但是速度快。</p><h2 id="使用摄像头或视频"><a href="#使用摄像头或视频" class="headerlink" title="使用摄像头或视频"></a>使用摄像头或视频</h2><p>使用以下命令在摄像头<code>0</code>（OPENCV默认使用摄像头<code>0</code>）运行<code>Tiny YOLOv3</code></p><pre><code>./darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights</code></pre><p>使用参数<code>-c &lt;num&gt;</code>指定使用哪一只摄像头。<br>或者使用以下命令实现<code>Tiny YOLOv3</code>对视频的目标检测：</p><pre><code>./darknet detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights &lt;video file&gt;</code></pre><h1 id="训练自己的YOLO"><a href="#训练自己的YOLO" class="headerlink" title="训练自己的YOLO"></a>训练自己的YOLO</h1><p>这里我们我们使用<code>Pascal VOC2007</code>数据集训练<code>YOLOv3-tiny</code>模型。</p><p>关于该数据集的介绍，可以查看<a href="https://arleyzhang.github.io/articles/1dc20586/" target="_blank" rel="noopener">这篇文章</a></p><p>具体步骤详情：<br><a href="https://github.com/AlexeyAB/darknet#how-to-train-to-detect-your-custom-objects" target="_blank" rel="noopener">How to train (to detect your custom objects)</a><br><a href="https://github.com/AlexeyAB/darknet#how-to-train-tiny-yolo-to-detect-your-custom-objects" target="_blank" rel="noopener">How to train tiny-yolo (to detect your custom objects)</a></p><h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><p>为了训练<code>YOLO</code>我们需要<code>2007</code>年的<code>VOC</code>数据集，可以从<a href="https://pjreddie.com/projects/pascal-voc-dataset-mirror/" target="_blank" rel="noopener">这里</a>下载。下载完后解压，解压完训练数据都在<code>VOCdevkit/</code>文件夹下。</p><p>训练<code>YOLO</code>需要使用特别格式的标签数据文件，它是一个<code>.txt</code>文本文件。<br>这个<code>.txt</code>文件的每一行是一个标签，一个文件对应一张图片，它看起来像这样：<br><code>&lt;object-class&gt; &lt;x&gt; &lt;y&gt; &lt;width&gt; &lt;height&gt;</code><br>注意此处的中心<code>x</code>、中心<code>y</code>、框<code>width</code>和框<code>height</code>是相对于图片宽度和高度的值，都是不大于<code>1</code>的小数。</p><p>转换公式：</p><script type="math/tex; mode=display">x=\frac{x_{min}+(x_{max}-x_{min})/2}{W_{image}}</script><script type="math/tex; mode=display">y=\frac{y_{min}+(y_{max}-y_{min})/2}{H_{image}}</script><script type="math/tex; mode=display">w=\frac{x_{max}-x_{min}}{W_{image}}</script><script type="math/tex; mode=display">h=\frac{y_{max}-y_{min}}{H_{image}}</script><p>为了得到这些<code>.txt</code>文件，我们可以方便地通过运行一个叫<code>voc_label.py</code>的脚本来生成。<br>脚本的内容如下：</p><pre class=" language-lang-python"><code class="language-lang-python">import xml.etree.ElementTree as ETimport pickleimport osfrom os import listdir, getcwdfrom os.path import joinsets = [('2007', 'train'), ('2007', 'val'), ('2007', 'test')]classes = [    "aeroplane", "bicycle", "bird", "boat", "bottle", "bus", "car", "cat",    "chair", "cow", "diningtable", "dog", "horse", "motorbike", "person",    "pottedplant", "sheep", "sofa", "train", "tvmonitor"]# 位置坐标转换def convert(size, box):    dw = 1. / (size[0])    dh = 1. / (size[1])    x = (box[0] + box[1]) / 2.0 - 1    y = (box[2] + box[3]) / 2.0 - 1    w = box[1] - box[0]    h = box[3] - box[2]    x = x * dw    w = w * dw    y = y * dh    h = h * dh    return (x, y, w, h)# label转换def convert_annotation(year, image_id):    in_file = open('VOC%s/Annotations/%s.xml' % (year, image_id))    out_file = open('VOC%s/labels/%s.txt' % (year, image_id), 'w')    tree = ET.parse(in_file)    root = tree.getroot()    size = root.find('size')    w = int(size.find('width').text)    h = int(size.find('height').text)    for obj in root.iter('object'):        difficult = obj.find('difficult').text        cls = obj.find('name').text        if cls not in classes or int(difficult) == 1:            continue        cls_id = classes.index(cls)        xmlbox = obj.find('bndbox')        b = (float(xmlbox.find('xmin').text), float(xmlbox.find('xmax').text),             float(xmlbox.find('ymin').text), float(xmlbox.find('ymax').text))        bb = convert((w, h), b)        out_file.write(            str(cls_id) + " " + " ".join([str(a) for a in bb]) + '\n')wd = getcwd()for year, image_set in sets:    if not os.path.exists('VOC%s/labels/' % (year)):        os.makedirs('VOC%s/labels/' % (year))    image_ids = open('VOC%s/ImageSets/Main/%s.txt' %                     (year, image_set)).read().strip().split()    list_file = open('%s_%s.txt' % (year, image_set), 'w')    for image_id in image_ids:        list_file.write('%s/VOC%s/JPEGImages/%s.jpg\n' % (wd, year, image_id))        convert_annotation(year, image_id)    list_file.close()os.system("cat 2007_train.txt 2007_val.txt > train.txt")os.system("cat 2007_train.txt 2007_val.txt 2007_test.txt > train.all.txt")</code></pre><p>将脚本保存到与<code>VOC2007</code>文件夹同级的目录，命名为<code>voc_label.py</code>，然后在此目录下打开命令行，执行：</p><pre><code>python voc_label.py</code></pre><p>很快，这个脚本会生成一些必要的文件。它生成了很多标签文件，位于<code>VOCdevkit/VOC2007/labels/</code>路径下。<br>并且在与<code>VOC2007</code>同级的目录下，你应该会看到如下的文件：</p><pre><code>2007_train.txt2007_val.txt2007_test.txttrain.txttrain.all.txt</code></pre><p>如果是自己采集的数据，需要标注，请使用<a href="https://github.com/tzutalin/labelImg" target="_blank" rel="noopener">LabelImg</a>或<a href="https://github.com/AlexeyAB/Yolo_mark" target="_blank" rel="noopener">Yolo_mark</a>工具，以生成<code>YOLO</code>格式的文本文件。然后将图片的路径汇总到一个文本文件，如<code>train.txt</code>、<code>val.txt</code>和<code>test.txt</code>里，一行一个图片路径。</p><h2 id="准备模型"><a href="#准备模型" class="headerlink" title="准备模型"></a>准备模型</h2><p>新建个文件夹，我们用来保存与模型有关的数据。我这里路径为：<code>D:/model/voc_model/</code><br>我这里<code>VOC2007</code>文件夹位于：<code>D:/dataset/VOCdevkit/</code>，<code>Darknet.exe</code>位于<code>D:/darknet-master/build/darknet/x64/</code></p><h3 id="准备权重文件"><a href="#准备权重文件" class="headerlink" title="准备权重文件"></a>准备权重文件</h3><p>首先下载默认的权重文件到你刚刚新建的模型文件夹（我这里是<code>D:/model/voc_model/</code>）：<br><a href="https://pjreddie.com/media/files/yolov3-tiny.weights" target="_blank" rel="noopener">默认权重文件</a></p><p>在模型文件夹运行如下指令，获取预训练的权重文件<code>yolov3-tiny.conv.15</code>，使用如下命令：</p><pre><code>D:/darknet-master/build/darknet/x64/darknet.exe partial D:/darknet-master/build/darknet/x64/cfg/yolov3-tiny.cfg yolov3-tiny.weights yolov3-tiny.conv.15 15</code></pre><p>其他预训练权重可以从<a href="https://pjreddie.com/darknet/imagenet/" target="_blank" rel="noopener">这里</a>下载</p><h3 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h3><ol><li><p>在模型文件夹创建一份<code>VOC2007.names</code>文本文件，其中该文件的每一行都是种类的名字，应该使得行数等于种类数<code>classes</code>的值。</p></li><li><p>在模型文件夹创建一份<code>VOC2007.data</code>文本文件，填入以下内容。<code>classes</code>是种类的个数、<code>train</code>是训练图片路径的文本文件，<code>valid</code>是验证图片路径的文本文件，<code>names</code>是种类名字的文件，<code>backup</code>路径则用于保存备份的权重文件（每迭代<code>100</code>次保存一次文件（带<code>_last</code>后缀），每<code>1000</code>次保存一次文件（带<code>_xxxx</code>后缀））。<br>如果没有验证集，则设置<code>valid</code>为与<code>train</code>相同的值即可，那么将测试在训练集上的精度。</p><pre><code>classes = 20train  = D:/dataset/VOCdevkit/train.txtvalid  = D:/dataset/VOCdevkit/2007_test.txtnames = VOC2007.namesbackup = backup/</code></pre></li><li><p>复制<code>D:/darknet-master/build/darknet/x64/cfg/yolov3-tiny_obj.cfg</code>文件，在模型文件夹另存为<code>yolov3-tiny-obj.cfg</code>，然后按照下述规则修改该文件：</p></li></ol><ul><li>修改使得<code>batch=64</code></li><li>修改使得<code>subdivisions=8</code></li><li>修改所有的<code>classes</code>值为<code>20</code>（这里<code>classes</code>是目标检测物体的种类个数）</li><li>修改所有位于行<code>[yolo]</code>之上的<code>[convolutional]</code>层的<code>filters</code>值为：$ filters = (classes + 5) * 3 $， <code>filters</code>的值需要计算出来再填入。注意，这不是修改所有<code>filters</code>的值，仅仅是修改恰好位于<code>[yolo]</code>这行之上该层的<code>filters</code>的值，可能需要修改多处。</li><li>如果你要修改输入图像的<code>width</code>和<code>height</code>值，请注意这两个值必须能被<code>32</code>整除。</li></ul><h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><p>在模型文件夹运行命令：</p><pre><code>D:/darknet-master/build/darknet/x64/darknet.exe detector train VOC2007.data yolov3-tiny-obj.cfg yolov3-tiny.conv.15</code></pre><ul><li>如果你在<code>avg loss</code>里看到<code>nan</code>，意味着训练失败；在其他地方出现<code>nan</code>则是正常的。</li><li><p>如果出错并显示<code>Out of memory</code>，尝试将<code>.cfg</code>文件的<code>subdivisions</code>值增大（建议为$ 2^n $）。</p></li><li><p>使用附加选项<code>-dont_show</code>来关闭训练时默认显示的损失曲线窗口</p></li><li><p>使用附加选项<code>-map</code>来显示<code>mAP</code>值</p></li><li><p>训练完成后的权重将保存于你在<code>.data</code>文件中设置的<code>backup</code>值路径下</p></li><li><p>你可以从<code>backup</code>值的路径下找到你的备份权重文件，并以此接着训练模型</p></li><li><p>多GPU训练：<a href="https://github.com/AlexeyAB/darknet#how-to-train-with-multi-gpu" target="_blank" rel="noopener">How to train with multi-GPU</a></p></li><li><p>训练完成后使用命令<code>darknet.exe detector test data/obj.data yolo-obj.cfg yolo-obj_8000.weights</code>针对输入的图片查看识别结果。</p></li></ul><h1 id="在COCO上训练YOLO"><a href="#在COCO上训练YOLO" class="headerlink" title="在COCO上训练YOLO"></a>在COCO上训练YOLO</h1><p>从<a href="http://cocodataset.org/#overview" target="_blank" rel="noopener">这里</a>下载<code>COCO</code>数据集。<br>也可以使用位于<code>scripts/get_coco_dataset.sh</code>的脚本来下载<code>COCO</code>数据集。</p><pre><code>cp scripts/get_coco_dataset.sh datacd databash get_coco_dataset.sh # 会下载到data文件夹下</code></pre><p>然后修改<code>cfg/coco.data</code>文件，指定你的数据路径。<br>接着修改<code>cfg/yolo.cfg</code>文件，配置训练使用的参数。<br>然后训练：</p><pre><code>./darknet detector train cfg/coco.data cfg/yolov3.cfg darknet53.conv.74</code></pre><p>如果你想使用4个GPU来跑，在上述语句附加参数<code>-gpus 0,1,2,3</code>即可<br>如果你想从检查点停止或重新运行，使用：</p><pre><code>./darknet detector train cfg/coco.data cfg/yolov3.cfg backup/yolov3.backup</code></pre><h1 id="YOLO进阶"><a href="#YOLO进阶" class="headerlink" title="YOLO进阶"></a>YOLO进阶</h1><h2 id="预训练模型下载地址"><a href="#预训练模型下载地址" class="headerlink" title="预训练模型下载地址"></a>预训练模型下载地址</h2><p><a href="https://github.com/AlexeyAB/darknet#pre-trained-models" target="_blank" rel="noopener">GitHub预训练模型</a><br><a href="https://pjreddie.com/darknet/yolo/" target="_blank" rel="noopener">官网</a> 的 <code>Performance on the COCO Dataset</code>部分</p><h2 id="配置文件地址"><a href="#配置文件地址" class="headerlink" title="配置文件地址"></a>配置文件地址</h2><p>位于路径<code>darknet/cfg/</code>下<br><a href="https://pjreddie.com/darknet/yolo/" target="_blank" rel="noopener">官网</a> 的 <code>Performance on the COCO Dataset</code>部分</p><h2 id="特殊模型"><a href="#特殊模型" class="headerlink" title="特殊模型"></a>特殊模型</h2><p><a href="https://github.com/AlexeyAB/darknet/blob/master/cfg/yolov3-tiny_xnor.cfg" target="_blank" rel="noopener">XNOR-net (2到4倍的性能)</a><br><a href="https://github.com/AlexeyAB/yolo2_light" target="_blank" rel="noopener">INT8-quantization (快30%)</a></p><h2 id="命令行语法"><a href="#命令行语法" class="headerlink" title="命令行语法"></a>命令行语法</h2><p>在<code>Linux</code>使用 <code>./darknet</code>而不是 <code>darknet.exe</code>，像这样：<code>./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights</code>。在<code>Linux</code>中可执行文件 <code>./darknet</code> 在根目录，而 <code>Windows</code>则在<code>/build/darknet/x64</code>路径下。</p><ul><li><code>Yolo v3 COCO</code> - 图像: <code>darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25</code></li><li>另一种方法 <code>Yolo v3 COCO</code> - 图像： <code>darknet.exe detect cfg/yolov3.cfg yolov3.weights -i 0 -thresh 0.25</code></li><li>输出物体坐标： <code>darknet.exe detector test cfg/coco.data yolov3.cfg yolov3.weights -ext_output dog.jpg</code></li><li><code>Yolo v3 COCO</code> - 视频：<code>darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output test.mp4</code></li><li><code>Yolo v3 COCO</code> - 摄像头<code>0</code>： <code>darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights -c 0</code></li><li><code>Yolo v3 COCO</code> - 网络摄像头： <code>darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights http://192.168.0.80:8080/video?dummy=param.mjpg</code></li><li><code>Yolo v3</code> - 保存结果视频： <code>darknet.exe detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights test.mp4 -out_filename res.avi</code></li><li><code>Yolo v3 Tiny COCO</code> - 视频： <code>darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights test.mp4</code></li><li><code>JSON</code>和<code>MJPEG</code>服务器，允许从您的软件或<code>Web</code>浏览器<code>IP</code>地址：<code>8070</code>和<code>8090</code>进行多个连接： <code>./darknet detector demo ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights test50.mp4 -json_port 8070 -mjpeg_port 8090 -ext_output</code></li><li><code>Yolo v3 Tiny</code> 在<code>GPU1</code>上运行：<code>darknet.exe detector demo cfg/coco.data cfg/yolov3-tiny.cfg yolov3-tiny.weights -i 1 test.mp4</code></li><li>在 <code>Amazon EC2</code> 服务器上训练，使用<code>Chrome</code>或<code>Firefox</code>浏览器，通过像这样（<code>http://ec2-35-160-228-91.us-west-2.compute.amazonaws.com:8090</code>）的链接查看 <code>mAP</code> 和 <code>Loss</code> 曲线图（注：<code>Darknet</code>应该与<code>OpenCV</code>一起编译）： <code>./darknet detector train cfg/coco.data yolov3.cfg darknet53.conv.74 -dont_show -mjpeg_port 8090 -map</code></li><li><code>186 MB Yolo9000</code> - 图像： <code>darknet.exe detector test cfg/combine9k.data cfg/yolo9000.cfg yolo9000.weights</code></li><li>处理 <code>data/train.txt</code>中记载路径的图片，然后保存检测结果到 <code>result.json</code>文件中：<code>darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -ext_output -dont_show -out result.json &lt; data/train.txt</code></li><li>处理 <code>data/train.txt</code>中记载路径的图片，然后保存检测结果到 <code>result.txt</code>中：<br><code>darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -dont_show -ext_output &lt; data/train.txt &gt; result.txt</code></li><li>伪标记 - 识别文本文件 <code>data/new_train.txt</code>中记载路径的图片，然后以<code>YOLO</code>训练数据的格式保存识别结果<code>&lt;image_name&gt;.txt</code>（这样子可以增大训练数据量）： <code>darknet.exe detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights -thresh 0.25 -dont_show -save_labels &lt; data/new_train.txt</code></li><li>计算 <code>anchors</code>：<code>darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416</code></li><li>计算 <code>IoU=50</code>下的 <code>mAP</code>值： <code>darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights</code></li><li>计算 <code>IoU=75</code>下的 <code>mAP</code>值： <code>darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights -iou_thresh 0.75</code></li></ul><h2 id="炼丹技巧"><a href="#炼丹技巧" class="headerlink" title="炼丹技巧"></a>炼丹技巧</h2><h3 id="早停"><a href="#早停" class="headerlink" title="早停"></a>早停</h3><p>粗略来讲，对于每个类别<code>2000</code>次迭代，总迭代次数不低于<code>4000</code>次。</p><p>具体来说：</p><ul><li>多次迭代仍不能降低平均损失值（<code>avg loss</code>）时（<code>avg loss</code>可能最终收敛于<code>0.05 ~ 3.0</code>之间的值）</li><li>早停后，你应该从多个权重文件中选取表现最好的，这样或许可以避免过拟合。使用类似如下的指令来验证训练的好坏：<pre><code>darknet.exe detector map data/obj.data yolo-obj.cfg backup\yolo-obj_7000.weights</code></pre>然后选取<code>mAP</code>最大的或<code>IoU</code>最大的作为最终权重。</li></ul><h3 id="提高精度策略"><a href="#提高精度策略" class="headerlink" title="提高精度策略"></a>提高精度策略</h3><ul><li>在<code>.cfg</code>文件中设置<code>random=1</code>，它会通过对不同分辨率的图片进行训练以提高精度</li><li>使用高分辨率的图像输入。在<code>.cfg</code>文件中设置<code>height</code>和<code>width</code>值。但是你无需重头训练，只需使用回<code>416x416</code>分辨率的权重数据就好了。</li><li>检查数据集标注是否正确符合规范</li><li>检查训练数据集数据量是否过少</li><li>迭代次数推荐不低于<code>2000 * classes</code></li><li>你的训练样本希望包含没有目标物体的图像，即该图像中没有出现目标物体，标签文件是空的文本。</li><li>如果图片里有很多数量的目标物体，那么在<code>.cfg</code>文件中最后的<code>[yolo]</code>层或<code>[region]</code>层中添加参数<code>max=200</code>，这也可以设定成更高的值。</li><li>如果目标物体很小（缩放成<code>416x416</code>尺寸后小于<code>16x16</code>），那么将<a href="https://github.com/AlexeyAB/darknet/blob/6390a5a2ab61a0bdf6f1a9a6b4a739c16b36e0d7/cfg/yolov3.cfg#L720" target="_blank" rel="noopener">第720行</a>设置为<code>layers = -1, 11</code>，将<a href="https://github.com/AlexeyAB/darknet/blob/6390a5a2ab61a0bdf6f1a9a6b4a739c16b36e0d7/cfg/yolov3.cfg#L717" target="_blank" rel="noopener">第717行</a>设置为<code>stride=4</code></li><li>如果目标物体有些很大有些又很小，那么请使用修改后的模型：<br><a href="https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov3_5l.cfg" target="_blank" rel="noopener">Full-model: 5 yolo layers</a><br><a href="https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov3-tiny_3l.cfg" target="_blank" rel="noopener">Tiny-model: 3 yolo layers</a><br><a href="https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov3-spp.cfg" target="_blank" rel="noopener">Spatial-full-model: 3 yolo layers</a></li><li>如果你的模型需要区分左右手性，例如区分左手和右手、左转和右转，那么需要关闭翻转数据增强选项，即添加<code>flip=0</code>到<a href="https://github.com/AlexeyAB/darknet/blob/3d2d0a7c98dbc8923d9ff705b81ff4f7940ea6ff/cfg/yolov3.cfg#L17" target="_blank" rel="noopener">这里</a></li><li>如果想要模型具有尺度的鲁棒性，则必须训练样本中包含多尺度的照片。这是因为<code>YOLO</code>不具有尺度变化的适应性。</li><li>要想加速模型的训练（但会降低预测精度）应该使用<code>Fine-Tuning</code>而不是<code>Transfer-Learning</code>，需要在<a href="https://github.com/AlexeyAB/darknet/blob/6d44529cf93211c319813c90e0c1adb34426abe5/cfg/yolov3.cfg#L548" target="_blank" rel="noopener">这里</a>设置参数<code>stopbackward=1</code>，然后运行<code>./darknet partial cfg/yolov3.cfg yolov3.weights yolov3.conv.81 81</code>，这会创建文件<code>yolov3.conv.81</code>，然后使用该文件<code>yolov3.conv.81</code>训练。</li><li>复杂物体应该使用复杂的神经网络来训练</li><li>你可以修改<code>anchors</code>的大小。略。</li></ul><h2 id="如何计算mAP"><a href="#如何计算mAP" class="headerlink" title="如何计算mAP"></a>如何计算mAP</h2><p><a href="https://github.com/AlexeyAB/darknet#how-to-calculate-map-on-pascalvoc-2007" target="_blank" rel="noopener">How to calculate mAP on PascalVOC 2007</a></p><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><h3 id="Open-Images数据集"><a href="#Open-Images数据集" class="headerlink" title="Open Images数据集"></a>Open Images数据集</h3><pre><code>wget https://pjreddie.com/media/files/yolov3-openimages.weights./darknet detector test cfg/openimages.data cfg/yolov3-openimages.cfg yolov3-openimages.weights</code></pre><h3 id="Yolo9000"><a href="#Yolo9000" class="headerlink" title="Yolo9000"></a>Yolo9000</h3><p>能够检测多达<code>9000</code>个物体，需要<code>4G</code>显存<br><a href="https://github.com/AlexeyAB/darknet#using-yolo9000" target="_blank" rel="noopener">Using Yolo9000</a></p><h3 id="如何以库的形式调用YOLO"><a href="#如何以库的形式调用YOLO" class="headerlink" title="如何以库的形式调用YOLO"></a>如何以库的形式调用YOLO</h3><p><a href="https://github.com/AlexeyAB/darknet#how-to-use-yolo-as-dll-and-so-libraries" target="_blank" rel="noopener">How to use Yolo as DLL and SO libraries</a></p><h1 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h1><ul><li>类似教程：<br><a href="https://chtseng.wordpress.com/2018/09/01/%E5%BB%BA%E7%AB%8B%E8%87%AA%E5%B7%B1%E7%9A%84yolo%E8%BE%A8%E8%AD%98%E6%A8%A1%E5%9E%8B-%E4%BB%A5%E6%9F%91%E6%A9%98%E8%BE%A8%E8%AD%98%E7%82%BA%E4%BE%8B/" target="_blank" rel="noopener">建立自己的YOLO辨識模型 – 以柑橘辨識為例</a><br><a href="https://medium.com/%E9%9B%9E%E9%9B%9E%E8%88%87%E5%85%94%E5%85%94%E7%9A%84%E5%B7%A5%E7%A8%8B%E4%B8%96%E7%95%8C/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-ml-note-yolo-%E5%88%A9%E7%94%A8%E5%BD%B1%E5%83%8F%E8%BE%A8%E8%AD%98%E5%81%9A%E7%89%A9%E4%BB%B6%E5%81%B5%E6%B8%AC-object-detection-%E7%9A%84%E6%8A%80%E8%A1%93-3ad34a4cac70" target="_blank" rel="noopener">YOLO!!!如何簡單使用YOLO訓練出自己的物件偵測!!! (Windows+Anaconda)</a><br><a href="https://blog.liebes.top/2018/01/30/ML-darknet-1/index.html" target="_blank" rel="noopener">机器学习之 darknet YOLO 训练 VOC 数据集</a></li></ul><ul><li>参考文献：<br><a href="https://pjreddie.com/darknet/yolo/" target="_blank" rel="noopener">YOLO: Real-Time Object Detection</a><br><a href="https://github.com/AlexeyAB/darknet" target="_blank" rel="noopener">Yolo-v3 and Yolo-v2 for Windows and Linux</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> dl </category>
          
      </categories>
      
      
        <tags>
            
            <tag> dl </tag>
            
            <tag> yolo </tag>
            
            <tag> darknet </tag>
            
            <tag> detection </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>正确选择ML算法</title>
      <link href="/ml/mlcomparsion/"/>
      <url>/ml/mlcomparsion/</url>
      
        <content type="html"><![CDATA[<p>本文教你如何选择合适自己的机器学习算法。</p><p><img src="20.png" alt></p><h1 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h1><h2 id="逻辑斯蒂回归（Logistic-regression）"><a href="#逻辑斯蒂回归（Logistic-regression）" class="headerlink" title="逻辑斯蒂回归（Logistic regression）"></a>逻辑斯蒂回归（Logistic regression）</h2><blockquote><p>属于判别式模型，有很多正则化模型的方法（L0，L1，L2，etc），而且你不必像在用朴素贝叶斯那样担心你的特征是否相关。与决策树与SVM机相比，你还会得到一个不错的概率解释，你甚至可以轻松地利用新数据来更新模型（使用在线梯度下降算法，onlinegradientdescent）。如果你需要一个概率架构（比如，简单地调节分类阈值，指明不确定性，或者是要获得置信区间），或者你希望以后将更多的训练数据快速整合到模型中去，那么使用它吧。<br><img src="1.png" alt></p><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ul><li>实现简单，广泛的应用于工业问题上；</li><li>分类时计算量非常小，速度很快，存储资源低；</li><li>便利的观测样本概率分数；</li><li>对逻辑回归而言，多重共线性并不是问题，它可以结合L2正则化来解决该问题；</li></ul><p><strong>表现最好：当特征没有相关性，最终分类结果是线性的，且特征维度远小于数据量的时候效果好。</strong></p><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul><li>当特征空间很大时，逻辑回归的性能不是很好；</li><li>容易欠拟合，一般准确度不太高</li><li>不能很好地处理大量多类特征或变量；</li><li>只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分；</li><li>对于非线性特征，需要进行转换；</li></ul><p><strong>表现最差：当特征相关性比较强时，表现会很差。</strong></p><h3 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h3><p><a href="http://www.cnblogs.com/chenice/p/7202045.html" target="_blank" rel="noopener">机器学习之良/恶性乳腺癌肿瘤预测</a><br><a href="https://zhuanlan.zhihu.com/p/25327755" target="_blank" rel="noopener">机器学习算法集锦：从贝叶斯到深度学习及各自优缺点</a></p></blockquote><h2 id="朴素贝叶斯（Naive-Bayes）"><a href="#朴素贝叶斯（Naive-Bayes）" class="headerlink" title="朴素贝叶斯（Naive Bayes）"></a>朴素贝叶斯（Naive Bayes）</h2><blockquote><p>朴素贝叶斯属于生成式模型（关于生成模型和判别式模型，主要还是在于是否是要求联合分布），非常简单，你只是做了一堆计数。如果注有条件独立性假设（一个比较严格的条件），朴素贝叶斯分类器的收敛速度将快于判别模型，如逻辑回归，所以你只需要较少的训练数据即可。即使NB条件独立假设不成立，NB分类器在实践中仍然表现的很出色。它的主要缺点是它不能学习特征间的相互作用，用mRMR中R来讲，就是特征冗余。引用一个比较经典的例子，比如，虽然你喜欢BradPitt和TomCruise的电影，但是它不能学习出你不喜欢他们在一起演的电影。<br><img src="2.png" alt></p><h3 id="优点-1"><a href="#优点-1" class="headerlink" title="优点"></a>优点</h3><ul><li>朴素贝叶斯模型发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率。</li><li>对小规模的数据表现很好，能个处理多分类任务，适合增量式训练。</li><li>对缺失数据不太敏感，算法也比较简单，常用于文本分类。</li></ul><h3 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h3><ul><li>需要计算先验概率；</li><li>分类决策存在错误率；</li><li>对输入数据的表达形式很敏感。</li></ul><h3 id="链接-1"><a href="#链接-1" class="headerlink" title="链接"></a>链接</h3><p><a href="https://scikit-learn.org/stable/modules/naive_bayes.html" target="_blank" rel="noopener">朴素贝叶斯的sklearn实现</a></p></blockquote><h2 id="最近邻算法（KNN）"><a href="#最近邻算法（KNN）" class="headerlink" title="最近邻算法（KNN）"></a>最近邻算法（KNN）</h2><blockquote><p>其主要过程为：</p><ol><li>计算训练样本和测试样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）；</li><li>对上面所有的距离值进行排序；</li><li>选前k个最小距离的样本；</li><li>根据这k个样本的标签进行投票，得到最后的分类类别；<br>如何选择一个最佳的K值，这取决于数据。一般情况下，在分类时较大的K值能够减小噪声的影响。但会使类别之间的界限变得模糊。一个较好的K值可通过各种启发式技术来获取，比如，交叉验证。另外噪声和非相关性特征向量的存在会使K近邻算法的准确性减小。<br>近邻算法具有较强的一致性结果。随着数据趋于无限，算法保证错误率不会超过贝叶斯算法错误率的两倍。对于一些好的K值，K近邻保证错误率不会超过贝叶斯理论误差率。<br><img src="3.png" alt></li></ol><h3 id="优点-2"><a href="#优点-2" class="headerlink" title="优点"></a>优点</h3><ul><li>理论成熟，思想简单，既可以用来做分类也可以用来做回归；</li><li>可用于非线性分类；</li><li>训练时间复杂度为O(n)；</li><li>对数据没有假设，准确度高，对outlier不敏感</li></ul><h3 id="缺点-2"><a href="#缺点-2" class="headerlink" title="缺点"></a>缺点</h3><ul><li>计算量大；</li><li>样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）；</li><li>需要大量的内存；</li></ul></blockquote><h2 id="支持向量机（SVM）"><a href="#支持向量机（SVM）" class="headerlink" title="支持向量机（SVM）"></a>支持向量机（SVM）</h2><blockquote><p>支持向量机使用一个名为核函数的技巧，来将非线性问题变换为线性问题，其本质是计算两个观测数据的距离。支持向量机算法所寻找的是能够最大化样本间隔的决策边界，因此又被称为大间距分类器。<br>举例来说，使用线性核函数的支持向量机类似于逻辑回归，但更具稳健性。因而在实践中，支持向量机最大用处是用非线性核函数来对非线性决策边界进行建模。<br><img src="4.png" alt></p><h3 id="优点-3"><a href="#优点-3" class="headerlink" title="优点"></a>优点</h3><ul><li>支持向量机能对非线性决策边界建模，又有许多可选的核函数。在面对过拟合时，支持向量机有着极强的稳健性，尤其是在高维空间中。</li></ul><h3 id="缺点-3"><a href="#缺点-3" class="headerlink" title="缺点"></a>缺点</h3><ul><li>不过，支持向量机是内存密集型算法，选择正确的核函数就需要相当的j技巧，不太适用较大的数据集。在当前的业界应用中，随机森林的表现往往要优于支持向量机。</li></ul><h3 id="链接-2"><a href="#链接-2" class="headerlink" title="链接"></a>链接</h3><p><a href="https://scikit-learn.org/stable/modules/svm.html#classification" target="_blank" rel="noopener">svm的sklearn实现</a></p></blockquote><h2 id="决策树（Decision-tree）"><a href="#决策树（Decision-tree）" class="headerlink" title="决策树（Decision tree）"></a>决策树（Decision tree）</h2><blockquote><p>易于解释。它可以毫无压力地处理特征间的交互关系并且是非参数化的，因此你不必担心异常值或者数据是否线性可分（举个例子，决策树能轻松处理好类别A在某个特征维度x的末端，类别B在中间，然后类别A又出现在特征维度x前端的情况）。它的缺点之一就是不支持在线学习，于是在新样本到来后，决策树需要全部重建。另一个缺点就是容易出现过拟合，但这也就是诸如随机森林RF（或提升树boostedtree）之类的集成方法的切入点。另外，随机森林经常是很多分类问题的赢家（通常比支持向量机好上那么一丁点），它训练快速并且可调，同时你无须担心要像支持向量机那样调一大堆参数，所以在以前都一直很受欢迎。<br>决策树中很重要的一点就是选择一个属性进行分枝，因此要注意一下信息增益的计算公式，并深入理解它。<br><img src="5.png" alt></p><h3 id="优点-4"><a href="#优点-4" class="headerlink" title="优点"></a>优点</h3><ul><li>计算简单，易于理解，可解释性强；</li><li>比较适合处理有缺失属性的样本；</li><li>能够处理不相关的特征；</li><li>在相对短的时间内能够对大型数据源做出可行且效果良好的结果。</li></ul><h3 id="缺点-4"><a href="#缺点-4" class="headerlink" title="缺点"></a>缺点</h3><ul><li>容易发生过拟合（随机森林可以很大程度上减少过拟合）；</li><li>忽略了数据之间的相关性；</li><li>对于那些各类别样本数量不一致的数据，在决策树当中,信息增益的结果偏向于那些具有更多数值的特征（只要是使用了信息增益，都有这个缺点，如RF）。</li></ul></blockquote><h2 id="随机森林（Random-Forests）"><a href="#随机森林（Random-Forests）" class="headerlink" title="随机森林（Random Forests）"></a>随机森林（Random Forests）</h2><blockquote><p>集成的方法，如随机森林（RF）或梯度提升树（GBM），则能结合许多独立训练树的预测。我们在这里不会详述其中的机制，但在实践中，随机森林一般都有很出色的表现，梯度提升树则较难调参，但往往能有更高的性能上限。<br><img src="6.png" alt></p><h3 id="优点-5"><a href="#优点-5" class="headerlink" title="优点"></a>优点</h3><ul><li>在数据集上表现良好，两个随机性的引入，使得随机森林不容易陷入过拟合。</li><li>在当前的很多数据集上，相对其他算法有着很大的优势，两个随机性的引入，使得随机森林具有很好的抗噪声能力。</li><li>它能够处理很高维度（feature很多）的数据，并且不用做特征选择，对数据集的适应能力强：既能处理离散型数据，也能处理连续型数据，数据集无需规范化。</li><li>在创建随机森林的时候，对generlization error使用的是无偏估计。</li><li>训练速度快，可以得到变量重要性排序。</li><li>在训练过程中，能够检测到feature间的互相影响。</li><li>容易做成并行化方法。</li><li>实现比较简单</li></ul><h3 id="缺点-5"><a href="#缺点-5" class="headerlink" title="缺点"></a>缺点</h3><ul><li>由于无约束，单棵树容易过拟合，这是因为单棵树可保留分支直至记住训练的数据。不够，集成方法可以弱化这一缺点。</li></ul><h3 id="链接-3"><a href="#链接-3" class="headerlink" title="链接"></a>链接</h3><p><a href="https://scikit-learn.org/stable/modules/ensemble.html#random-forests" target="_blank" rel="noopener">随机森林的实现</a></p></blockquote><h2 id="深度学习（DL）"><a href="#深度学习（DL）" class="headerlink" title="深度学习（DL）"></a>深度学习（DL）</h2><blockquote><p>深度学习是指能够学习极端复杂模式的多层神经网络。它们在输入层和输出层之间使用隐藏层来对数据的中间表征建模，这一点是其他算法很难做到的。<br>深度学习还有几个重要的机制，如卷积、漏失等，这使该算法可以有效学习高维数据。然而，相对于其他算法，深度学习需要更多的数据来进行训练，因为该模型需要估算更大数量级的参数。<br><img src="7.png" alt></p><h3 id="优点-6"><a href="#优点-6" class="headerlink" title="优点"></a>优点</h3><p>深度学习是当前特定领域的最先进技术，如计算机视觉与语音识别。深度神经网络在图像、音频和文本数据上表现优异，也很容易通过反向传播算法来更新数据模型。它们的架构（即层级的数量和结构）能适用于多种问题，同时隐藏层还能降低算法对特征工程的依赖。</p><h3 id="缺点-6"><a href="#缺点-6" class="headerlink" title="缺点"></a>缺点</h3><p>深度学习算法往往不适合用于通用目的，因为它们需要大量的数据。事实上，对于经典的机器学习问题，深度学习的表现并不比集成方法好。此外，由于训练所需的密集型计算，它们需要更多的专门知识才能进行调参（如设定架构与超参数）。</p><h3 id="链接-4"><a href="#链接-4" class="headerlink" title="链接"></a>链接</h3><p><a href="https://keras.io/" target="_blank" rel="noopener">Keras深度学习</a></p></blockquote><hr><h1 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h1><p><img src="11.png" alt></p><h2 id="K均值（K-means）"><a href="#K均值（K-means）" class="headerlink" title="K均值（K-means）"></a>K均值（K-means）</h2><blockquote><p>K 均值是基于样本点间的几何距离来度量聚类的通用目的算法。由于集群围绕在聚类中心，结果会接近于球状并具有相似的大小。<br>我们之所以推荐该算法给初学者，是因为它不仅足够简单，而且足够灵活，对于大多数问题都能给出合理的结果。<br><img src="8.png" alt></p><h3 id="优点-7"><a href="#优点-7" class="headerlink" title="优点"></a>优点</h3><ul><li>K均值是最为流行的聚类算法，因为它足够快速、足够简单，如果你的预处理数据和特征工程都做得十分有效，那它将具备令人惊叹的灵活性。</li></ul><h3 id="缺点-7"><a href="#缺点-7" class="headerlink" title="缺点"></a>缺点</h3><ul><li>该算法需要指定集群的数量，而 K 值的选择通常都不是那么容易确定的。另外，如果训练数据中的真实集群并不是类球状的，那么 K 均值聚类会得出一些比较差的集群。</li></ul><h3 id="链接-5"><a href="#链接-5" class="headerlink" title="链接"></a>链接</h3><p><a href="https://scikit-learn.org/stable/modules/clustering.html#k-means" target="_blank" rel="noopener">K均值的sklearn实现</a></p></blockquote><h2 id="仿射传播（Affinity-Propagation）"><a href="#仿射传播（Affinity-Propagation）" class="headerlink" title="仿射传播（Affinity Propagation）"></a>仿射传播（Affinity Propagation）</h2><blockquote><p>仿射传播是一种相对较新的聚类算法，它基于两个样本点之间的图形距离来确定集群，其结果倾向于更小且大小不等的集群。<br><img src="9.png" alt></p><h3 id="优点-8"><a href="#优点-8" class="headerlink" title="优点"></a>优点</h3><ul><li>仿射传播不需要指出明确的集群数量，但需要指定“sample preference”和“damping”等超参数。</li></ul><h3 id="缺点-8"><a href="#缺点-8" class="headerlink" title="缺点"></a>缺点</h3><ul><li>仿射传播的主要缺点是训练速度较慢，且需要大量内存，因而难于扩展到大数据集。此外，该算法同样在假定潜在的集群要接近于球状。</li></ul><h3 id="链接-6"><a href="#链接-6" class="headerlink" title="链接"></a>链接</h3><p><a href="https://scikit-learn.org/stable/modules/clustering.html#affinity-propagation" target="_blank" rel="noopener">仿射传播的sklearn实现</a></p></blockquote><h2 id="层次聚类（Hierarchical-clustering）"><a href="#层次聚类（Hierarchical-clustering）" class="headerlink" title="层次聚类（Hierarchical clustering）"></a>层次聚类（Hierarchical clustering）</h2><blockquote><p>其算法基于以下概念来实现：</p><ol><li>每一个集群都从一个数据点开始；</li><li>每一个集群都可基于相同的标准进行合并；</li><li>重复这一过程，直至你仅剩下一个集群，这就获得了集群的层次结构。<br><img src="10.png" alt></li></ol><h3 id="优点-9"><a href="#优点-9" class="headerlink" title="优点"></a>优点</h3><ul><li>层次聚类的最主要优点，是集群不再假定为类球形。此外，它可以很容易扩展到大数据集。</li></ul><h3 id="缺点-9"><a href="#缺点-9" class="headerlink" title="缺点"></a>缺点</h3><ul><li>类似于 K 均值，该算法需要选定集群的数量，即算法完成后所要保留的层次。</li></ul></blockquote><h2 id="DBSCAN"><a href="#DBSCAN" class="headerlink" title="DBSCAN"></a>DBSCAN</h2><blockquote><p>DBSCAN 是一种基于密度的聚类算法，它将样本点的密集区域组成集群；其最新进展是HDBSCAN，它允许集群的密度可变。<br><img src="12.png" alt></p><h3 id="优点-10"><a href="#优点-10" class="headerlink" title="优点"></a>优点</h3><ul><li>DBSCAN 不需要假定类球形集群，其性能可以扩展。此外，它不需要每个点都被分配到集群中，这就降低了集群的噪音。</li></ul><h3 id="缺点-10"><a href="#缺点-10" class="headerlink" title="缺点"></a>缺点</h3><ul><li>用户必须要调整“epsilon”和“min_sample”这两个超参数来定义集群密度。DBSCAN 对此非常敏感。</li></ul><h3 id="链接-7"><a href="#链接-7" class="headerlink" title="链接"></a>链接</h3><p><a href="https://scikit-learn.org/stable/modules/clustering.html#dbscan" target="_blank" rel="noopener">DBSCAN的sklearn实现</a></p></blockquote><hr><h1 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h1><h2 id="线性回归（Linear-regression）"><a href="#线性回归（Linear-regression）" class="headerlink" title="线性回归（Linear regression）"></a>线性回归（Linear regression）</h2><blockquote><p>线性回归是回归任务最常用的算法。它最简的形式，是用一个连续的超平面来拟合数据集（比如，当你仅有两个变量时就用一条直线）。如果数据集内的变量存在线性关系，拟合程度就相当高。<br>在实践中，简单线性回归通常会被其正则化形式（LASSO、Ridge 及弹性网络）所取代。正则化是对过多回归系数所采取的一种避免过拟合的惩罚技巧，同时，惩罚的强度需要被平衡好。<br><img src="13.png" alt></p><h3 id="优点-11"><a href="#优点-11" class="headerlink" title="优点"></a>优点</h3><ul><li>线性回归的理解和解释都非常直观，还能通过正则化来避免过拟合。此外，线性模型很容易通过随机梯度下降来更新数据模型。</li></ul><h3 id="缺点-11"><a href="#缺点-11" class="headerlink" title="缺点"></a>缺点</h3><ul><li>线性回归在处理非线性关系时非常糟糕，在识别复杂的模式上也不够灵活，而添加正确的相互作用项或多项式又极为棘手且耗时。</li></ul><h3 id="链接-8"><a href="#链接-8" class="headerlink" title="链接"></a>链接</h3><p><a href="https://scikit-learn.org/stable/modules/linear_model.html" target="_blank" rel="noopener">sklearn的实现方法</a></p></blockquote><hr><h1 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h1><blockquote><p><strong>特征选取</strong>与<strong>特征提取</strong>的关键区别在于：特征选取是从原特征集中选取一个子特征集，而特称提取则是在原特征集的基础上重新构造出一些(一个或多个)全新的特征。</p></blockquote><h2 id="方差阈值（Variance-Threshold）"><a href="#方差阈值（Variance-Threshold）" class="headerlink" title="方差阈值（Variance Threshold）"></a>方差阈值（Variance Threshold）</h2><blockquote><ul><li>方差阈值会摒弃掉观测样本那些观测值改变较小的特征(即，它们的方差小于某个设定的阈值)。这样的特征的价值极小。</li><li>举例来说，如果你有一份公共健康数据，其中96%的人都是35岁的男性，那么去掉“年龄”和“性别”的特征也不会损失重要信息。</li><li>由于方差阈值依赖于特征值的数量级，你应该对特征值先做归一化处理。<br><img src="14.png" alt></li></ul><h3 id="优点-12"><a href="#优点-12" class="headerlink" title="优点"></a>优点</h3><p>使用方差阈值方式进行数据降维只需一个非常可靠的直觉：特征值不怎么改变的特征，不会带来什么有用的信息。这是在你建模初期进行数据降维相对安全的一种方式。</p><h3 id="缺点-12"><a href="#缺点-12" class="headerlink" title="缺点"></a>缺点</h3><p>如果你正在解决的问题并不需要进行数据降维，即便使用了方差阈值也几乎没有什么作用。此外，你需要手工设置、调整方差阈值，这个过程相当具有技术含量。我们建议从一个保守(也就是，较低)的阈值开始。</p><h3 id="链接-9"><a href="#链接-9" class="headerlink" title="链接"></a>链接</h3><p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html" target="_blank" rel="noopener">方差阈值的sklearn实现</a></p></blockquote><h2 id="相关性阈值"><a href="#相关性阈值" class="headerlink" title="相关性阈值"></a>相关性阈值</h2><blockquote><ul><li>相关性阈值会去掉那些高度相关的特征(亦即，这些特征的特征值变化与其他特征非常相似)。它们提供的是冗余信息。</li><li>举例来说，如果你有一个房地产数据，其中两个特征分别是“房屋面积（单位：平方英尺）”和“房屋面积（单位：平方米）”，那么，你就可以去掉其中的任何一个（这非常安全，也不会给你的模型带来任何负面影响）。</li><li>问题是，你该去掉哪一个特征呢？首先，你应该计算所有特征对的相关系数。而后，如果某个特征对的相关系数大于设定的阈值，那你就可以去掉其中平均绝对相关系数较大的那一个。<br><img src="15.png" alt></li></ul><h3 id="优点-13"><a href="#优点-13" class="headerlink" title="优点"></a>优点</h3><p>使用相关性阈值同样只需一个可靠的直觉：相似的特征提供了冗余的信息。对于某些含有强相关性特征较多的数据集，有些算法的稳健性并不好，因此，去掉它们可以提升整个模型的性能(计算速度、模型准确度、模型稳健性，等等)。</p><h3 id="缺点-13"><a href="#缺点-13" class="headerlink" title="缺点"></a>缺点</h3><p>同样，你还是必须手动去设置、调整相关性阈值，这同样是个棘手且复杂的过程。此外，如果你设置的阈值过低，那么你将会丢失掉一些有用的信息。无论在什么时候，我们都更倾向于使用那些内置了特征选取的算法。对于没有内置特征提取的算法，主成分分析是一个很好的备用方案。</p><h3 id="链接-10"><a href="#链接-10" class="headerlink" title="链接"></a>链接</h3><p><a href="https://gist.github.com/Swarchal/881976176aaeb21e8e8df486903e99d6" target="_blank" rel="noopener">相关性阈值的py实现</a></p></blockquote><h2 id="遗传算法（GA）"><a href="#遗传算法（GA）" class="headerlink" title="遗传算法（GA）"></a>遗传算法（GA）</h2><blockquote><p>遗传算法是可用于不同任务的一大类算法的统称。它们受进化生物学与自然选择的启发，结合变异与交叉，在解空间内进行高效的遍历搜索。<br>在机器学习领域，遗传算法主要有两大用处。</p><ul><li>用于最优化，比如去找神经网络的最佳权重。</li><li>是用于监督式特征提取。<br>这一用例中，“基因”表示单个特征，同时“有机体”表示候选特征集。“种群体”内的每一个有机体都会基于其适应性进行评分，正如在测试数据集上进行模型性能测试。最能适应环境的有机体将会生存下来，并不断繁衍，一直迭代，直至最终收敛于某个最优的解决方案。<br><img src="16.png" alt></li></ul><h3 id="优点-14"><a href="#优点-14" class="headerlink" title="优点"></a>优点</h3><p>在穷举搜索不可行的情况下，对高维数据集使用遗传算法会相当有效。当你的算法需要预处理数据却没有内置的特征选取机制(如最近邻分类算法），而你又必须保留最原始的特征(也就是不能用任何主成分分析算法)，遗传算法就成了你最好的选择。这一情况在要求透明、可解释方案的商业环境下时有发生。</p><h3 id="缺点-14"><a href="#缺点-14" class="headerlink" title="缺点"></a>缺点</h3><p>遗传算法为你解决方案的实施带来了更高的复杂度，而多数情况下它们都是不必要的麻烦。如果可能的话，主成分分析或其它内置特征选取的算法将会更加高效和简洁。</p><h3 id="链接-11"><a href="#链接-11" class="headerlink" title="链接"></a>链接</h3><p><a href="https://pypi.org/project/deap/" target="_blank" rel="noopener">遗传算法的py实现</a></p></blockquote><h2 id="主成分分析（PCA）"><a href="#主成分分析（PCA）" class="headerlink" title="主成分分析（PCA）"></a>主成分分析（PCA）</h2><blockquote><p>主成分分析是一个非监督式算法，它用来创造原始特征的线性组合。新创造出来的特征他们之间都是正交的，也就是没有关联性。具体来说，这些新特征是按它们本身变化程度的大小来进行排列的。第一个主成分代表了你的数据集中变化最为剧烈的特征，第二个主成分代表了变化程度排在第二位的特征，以此类推。<br>因此，你可以通过限制使用主成分的个数来达到数据降维的目的。例如，你可以仅采用能使累积可解释方差为90%的主成分数量。<br>你需要在使用主成分分析之前，对数据进行归一化处理。否则，原始数据中特征值数量级最大的那个特征将会主导你新创造出来的主成分特征。<br><img src="17.png" alt></p><h3 id="优点-15"><a href="#优点-15" class="headerlink" title="优点"></a>优点</h3><p>主成分分析是一项多用途技术，实用效果非常好。它部署起来快速、简单，也就是说，你可以很方便地测试算法性能，无论使用还是不使用主成分分析。此外，主成分分析还有好几种变体和扩展(如：核主成分分析(kernel PCA)，稀疏主成分分析(sparse PCA))，用以解决特定的问题。</p><h3 id="缺点-15"><a href="#缺点-15" class="headerlink" title="缺点"></a>缺点</h3><p>新创造出来的主成分并不具备可解释性，因而在某些情况下，新特征与应用实际场景之间很难建立起联系。此外，你仍然需要手动设置、调整累积可解释方差的阈值。</p><h3 id="链接-12"><a href="#链接-12" class="headerlink" title="链接"></a>链接</h3><p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" target="_blank" rel="noopener">PCA的sklearn实现</a></p></blockquote><h2 id="线性判别分析（LDA）"><a href="#线性判别分析（LDA）" class="headerlink" title="线性判别分析（LDA）"></a>线性判别分析（LDA）</h2><blockquote><ul><li>线性判别分析不是隐含狄利克雷分布，它同样用来构造原始特征集的线性组合。但与主成分分析不同，线性判别分析不会最大化可解释方差，而是最大化类别间的分离程度。</li><li>因此，线性判别分析是一种监督式学习方式，它必须使用有标记的数据集。那么，线性判别分析与主成分分析，到底哪种方法更好呢？这要视具体的情况而定，“没有免费的午餐”原理在这里同样适用。</li><li>线性判别分析同样依赖于特征值的数量级，你同样需要先对特征值做归一化处理。<br><img src="18.png" alt></li></ul><h3 id="优点-16"><a href="#优点-16" class="headerlink" title="优点"></a>优点</h3><p>线性判别分析是一种监督式学习，基于这种方式获取到的特征可以(但并不总是能)提升模型性能。此外，线性判别分析还有一些变体(如二次线性判别分析)，可用来解决特定的问题。</p><h3 id="缺点-16"><a href="#缺点-16" class="headerlink" title="缺点"></a>缺点</h3><p>与主成分分析一样，新创造出来的特征不具有可解释性。而且，你同样要手动设置、调整需要保留的特征数量。线性判别分析需要已经标记好的数据，因此，这也让它更加接地气儿。</p><h3 id="链接-13"><a href="#链接-13" class="headerlink" title="链接"></a>链接</h3><p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis" target="_blank" rel="noopener">LDA的sklearn实现</a><br><a href="https://www.choyang.me/zh/post/liner-discriminant-analysis/" target="_blank" rel="noopener">线性判别分析LDA</a></p></blockquote><h2 id="自编码机"><a href="#自编码机" class="headerlink" title="自编码机"></a>自编码机</h2><blockquote><ul><li>自编码机是一种人工神经网络，它是用来重新构建原始输入的。例如，图像自编码机是训练来重新表征原始数据的，而非用以区分图片里面的小猫、小狗。</li><li>但这有用吗？这里的关键，是在隐含层搭建比输入层和输出层更少数量的神经元。这样，隐含层就会不断学习如何用更少的特征来表征原始图像。</li><li>因为是用输入图像来作为目标输出，自编码机被视为无监督学习。它们可被直接使用（如：图像压缩）或按顺序堆叠使用（如：深度学习）。<br><img src="19.png" alt></li></ul><h3 id="优点-17"><a href="#优点-17" class="headerlink" title="优点"></a>优点</h3><p>自编码机是人工神经网络中的一种，这表示它们对某些特定类型的数据表现会非常好，比如图像和语音数据。</p><h3 id="缺点-17"><a href="#缺点-17" class="headerlink" title="缺点"></a>缺点</h3><p>自编码机是一种人工神经网络。这就是说，它们的优化需要更多的数据来进行训练。它们并不能作为一般意义上的数据降维算法来用。</p><h3 id="链接-14"><a href="#链接-14" class="headerlink" title="链接"></a>链接</h3><p><a href="https://ynuwm.github.io/2017/05/14/%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8AutoEncoder/" target="_blank" rel="noopener">自编码器AutoEncoder</a></p></blockquote><hr><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><blockquote><ol><li><a href="https://zhuanlan.zhihu.com/p/31711537" target="_blank" rel="noopener">机器学习算法汇总及选择</a></li></ol></blockquote>]]></content>
      
      
      <categories>
          
          <category> ml </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ml </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux-入门速成</title>
      <link href="/linux/linux-quickstart/"/>
      <url>/linux/linux-quickstart/</url>
      
        <content type="html"><![CDATA[<p>本文实用地介绍了 Linux 指令的用法，和 Linux 系统的相关知识。</p><p>（大多内容选自 <a href="http://billie66.github.io/TLCL/book/index.html" target="_blank" rel="noopener">The Linux Command Line 的中文版</a> ）</p><p><img src="1.png" alt></p><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><h2 id="一个故事"><a href="#一个故事" class="headerlink" title="一个故事"></a>一个故事</h2><blockquote><p>我想给大家讲个故事。<br>故事内容不是 Linus Torvalds 在1991年怎样写了 Linux 内核的第一个版本， 因为这些内容你可以在许多 Linux 书籍中读到。我也不是来告诉你，更早之前，Richard Stallman 是如何开始 GNU 项目，设计了一个免费的类Unix 的操作系统。那也是一个很有意义的故事， 但大多数 Linux 书籍也讲到了它。<br>我想告诉大家一个你如何才能夺回计算机管理权的故事。<br>在20世纪70年代末，我刚开始和计算机打交道时，正进行着一场革命，那时的我还是一名大学生。 微处理器的发明，使普通老百姓（就如你和我）真正拥有一台计算机成为可能。今天， 人们难以想象，只有大企业和强大的政府才能够拥有计算机的世界，是怎样的一个世界。 简单说，你做不了多少事情。<br>今天，世界已经截然不同了。计算机遍布各个领域，从小手表到大型数据中心，及大小介于它们之间的每件东西。 除了随处可见的计算机之外，我们还有一个无处不在的连接所有计算机的网络。这已经开创了一个奇妙的， 个人授权和创作自由的新时代，但是在过去的二三十年里，正在发生另一些事情。一个大公司不断地把它的 管理权强加到世界上绝大多数的计算机上，并且决定你对计算机的操作权力。幸运地是，来自世界各地的人们， 正积极努力地做些事情来改变这种境况。通过编写自己的软件，他们一直在为维护电脑的管理权而战斗着。 他们建设着 Linux。<br>一提到 Linux，许多人都会说到“自由”，但我不认为他们都知道“自由”的真正涵义。“自由”是一种权力， 它决定你的计算机能做什么，同时能够拥有这种“自由”的唯一方式就是知道计算机正在做什么。 “自由”是指一台没有任何秘密的计算机，你可以从它那里了解一切，只要你用心的去寻找。</p></blockquote><h2 id="为什么使用命令行"><a href="#为什么使用命令行" class="headerlink" title="为什么使用命令行"></a>为什么使用命令行</h2><blockquote><p>你是否注意到，在电影中一个“超级黑客”坐在电脑前，从不摸一下鼠标， 就能够在30秒内侵入到超安全的军用计算机中。这是因为电影制片人意识到， 作为人类，本能地知道让计算机圆满完成工作的唯一途径，是用键盘来操纵计算机。<br>现在，大多数的计算机用户只是熟悉图形用户界面（GUI），并且产品供应商和此领域的学者会灌输给用户这样的思想， 命令行界面（CLI）是过去使用的一种很恐怖的东西。这就很不幸，因为一个好的命令行界面， 是用来和计算机进行交流沟通的非常有效的方式，正像人类社会使用文字互通信息一样。人们说，“图形用户界面让简单的任务更容易完成， 而命令行界面使完成复杂的任务成为可能”，到现在这句话仍然很正确。<br>因为 Linux 是以 Unix 家族的操作系统为模型写成的，所以它分享了 Unix 丰富的命令行工具。 Unix 在20世纪80年代初显赫一时(虽然，开发它在更早之前），结果，在普遍地使用图形界面之前， 开发了一种广泛的命令行界面。事实上，很多人选择 Linux（而不是其他的系统，比如说 Windows NT）是因为其可以使“完成复杂的任务成为可能”的强大的命令行界面。</p></blockquote><h2 id="Linux-版本选择"><a href="#Linux-版本选择" class="headerlink" title="Linux 版本选择"></a>Linux 版本选择</h2><blockquote><ol><li>适合初学者的最佳Linux发行版：Linux Mint</li><li>老旧硬件的最佳Linux发行版：Ubuntu MATE</li><li>安全行业的最佳Linux发行版：Kali Linux</li><li>专属游戏的Linux发行版：Steam OS</li><li>用于编程的Linux发行版：Debian</li><li>美丽的Linux发行版：elementary OS</li><li>儿童专属Linux发行版：Ubermix</li><li>隐私和匿名的Linux发行版：Tails</li><li>Linux服务器发行版：CentOS</li><li>强大PC和笔记本电脑推荐的Linux发行版：Ubuntu</li></ol></blockquote><h1 id="学习-shell"><a href="#学习-shell" class="headerlink" title="学习 shell"></a>学习 shell</h1><h2 id="什么是-shell"><a href="#什么是-shell" class="headerlink" title="什么是 shell"></a>什么是 shell</h2><blockquote><p>一说到命令行，我们真正指的是 shell。shell 就是一个程序，它接受从键盘输入的命令， 然后把命令传递给操作系统去执行。几乎所有的 Linux 发行版都提供一个名为 bash 的 来自 GNU 项目的 shell 程序。“bash” 是 “Bourne Again SHell” 的首字母缩写， 所指的是这样一个事实，bash 是最初 Unix 上由 Steve Bourne 写成 shell 程序 sh 的增强版。</p><p><code>[root@izwz9biz2m4sd3tu00600pz ~]#</code>这叫做 shell 提示符，无论何时当 shell 准备好了去接受输入时，它就会出现。然而， 它可能会以各种各样的面孔显示，这则取决于不同的 Linux 发行版， 它通常包括你的用户名@主机名，紧接着当前工作目录（稍后会有更多介绍）和一个美元符号。<br>如果提示符的最后一个字符是“#”, 而不是“$”, 那么这个终端会话就有超级用户权限。 这意味着，我们或者是以 root 用户的身份登录，或者是我们选择的终端仿真器提供超级用户（管理员）权限。</p><p>下面尝试使用下面指令来体会下 Linux 操作系统吧 (≧∀≦)ゞ</p><p>显示时间日期</p></blockquote><pre><code>[root@izwz9biz2m4sd3tu00600pz ~]# dateFri Mar  1 10:44:55 CST 2019</code></pre><blockquote><p>显示当前月份的日历</p></blockquote><pre><code>[root@izwz9biz2m4sd3tu00600pz ~]# cal     March 2019Su Mo Tu We Th Fr Sa                1  2 3  4  5  6  7  8  910 11 12 13 14 15 1617 18 19 20 21 22 2324 25 26 27 28 29 3031</code></pre><blockquote><p>查看磁盘剩余空间</p></blockquote><pre><code>[root@izwz9biz2m4sd3tu00600pz ~]# dfFilesystem     1K-blocks    Used Available Use% Mounted on/dev/vda1       41151808 4606104  34432272  12% /devtmpfs          931516       0    931516   0% /devtmpfs             941860       0    941860   0% /dev/shmtmpfs             941860   16708    925152   2% /runtmpfs             941860       0    941860   0% /sys/fs/cgrouptmpfs             188376       0    188376   0% /run/user/0</code></pre><blockquote><p>显示空闲内存</p></blockquote><pre><code>[root@izwz9biz2m4sd3tu00600pz ~]# free              total        used        free      shared  buff/cache   availableMem:        1883724      128840      470492       16712     1284392     1537728Swap:             0           0           0</code></pre><blockquote><p>结束终端会话（退出）</p></blockquote><pre><code>[root@izwz9biz2m4sd3tu00600pz ~]# exit</code></pre><h2 id="文件系统中跳转"><a href="#文件系统中跳转" class="headerlink" title="文件系统中跳转"></a>文件系统中跳转</h2><blockquote><p>下面是常用的指令</p></blockquote><pre><code>pwd  # 显示当前的工作目录（print working directory的缩写）cd   # 切换目录ls   # 列出目录内容</code></pre><blockquote><p>当我们首次登录系统（或者启动终端仿真器会话）后，当前工作目录是我们的家目录。 每个用户都有他自己的家目录，当用户以普通用户的身份操控系统时，家目录是唯一允许用户写入文件的地方。</p></blockquote><h3 id="cd-指令"><a href="#cd-指令" class="headerlink" title="cd 指令"></a>cd 指令</h3><blockquote><p>其他有用的 cd 指令：</p></blockquote><pre><code>cd -        # 切换到先前的工作目录cd          # 切换到自己的家目录（等同于&quot;cd ~&quot;）cd ~karbo    # 切换到karbo的家目录</code></pre><h3 id="ls-指令"><a href="#ls-指令" class="headerlink" title="ls 指令"></a>ls 指令</h3><blockquote><p>其他有用的 ls 指令：</p></blockquote><pre><code>ls -a                          # 显示所有文件（包括以”.“开头的隐藏文件）ls -l                          # 显示文件细节（以长模式输出）ls -lt                         # 显示文件细节并按照时间顺序排序（时间正序指最近修改的文件是第一行）ls &lt;想查看的目录&gt;                # 显示 &lt;想查看的目录&gt; 下的文件ls &lt;想查看的目录1&gt; &lt;想查看的目录2&gt; # 显示 &lt;想查看的目录1&gt; 和 &lt;想查看的目录2&gt; 下的文件</code></pre><blockquote><p>ls 命令选项：</p></blockquote><div class="table-container"><table><thead><tr><th>选项</th><th>长选项</th><th>描述</th></tr></thead><tbody><tr><td>-a</td><td>—all</td><td>列出所有文件，甚至包括文件名以圆点开头的默认会被隐藏的隐藏文件。</td></tr><tr><td>-d</td><td>—directory</td><td>通常，如果指定了目录名，ls 命令会列出这个目录中的内容，而不是目录本身。 把这个选项与 -l 选项结合使用，可以看到所指定目录的详细信息，而不是目录中的内容。</td></tr><tr><td>-F</td><td>—classify</td><td>这个选项会在每个所列出的名字后面加上一个指示符。例如，如果名字是 目录名，则会加上一个’/‘字符。</td></tr><tr><td>-h</td><td>—human-readable</td><td>当以长格式列出时，以人们可读的格式，而不是以字节数来显示文件的大小。</td></tr><tr><td>-l</td><td></td><td>以长格式显示结果。</td></tr><tr><td>-r</td><td>—reverse</td><td>以相反的顺序来显示结果。通常，ls 命令的输出结果按照字母升序排列。</td></tr><tr><td>-S</td><td></td><td>命令输出结果按照文件大小来排序。</td></tr><tr><td>-t</td><td></td><td>按照修改时间来排序。</td></tr></tbody></table></div><blockquote><p>ls 长格式列表的字段（以<code>-rw-r--r-- 1 root root   32059 2007-04-03 11:05 oo-cd-cover.odf</code>为例）</p></blockquote><div class="table-container"><table><thead><tr><th>字段</th><th>含义</th></tr></thead><tbody><tr><td>-rw-r—r—</td><td>对于文件的访问权限。第一个字符指明文件类型。在不同类型之间， 开头的“－”说明是一个普通文件，“d”表明是一个目录。其后三个字符是文件所有者的 访问权限，再其后的三个字符是文件所属组中成员的访问权限，最后三个字符是其他所 有人的访问权限。这个字段的完整含义将在第十章讨论。</td></tr><tr><td>1</td><td>文件的硬链接数目。参考随后讨论的关于链接的内容。</td></tr><tr><td>root</td><td>文件所有者的用户名。</td></tr><tr><td>root</td><td>文件所属用户组的名字。</td></tr><tr><td>32059</td><td>以字节数表示的文件大小。</td></tr><tr><td>2007-04-03 11:05</td><td>上次修改文件的时间和日期。</td></tr><tr><td>oo-cd-cover.odf</td><td>文件名。</td></tr></tbody></table></div><blockquote><p>注意事项：</p><ol><li>文件名和命令名是<strong>大小写敏感</strong>的。文件名 “File1” 和 “file1” 是指两个不同的文件名。</li><li>Linux <strong>没有“文件扩展名”的概念</strong>，不像其它一些系统。可以用你喜欢的任何名字 来给文件起名。文件内容或用途由其它方法来决定。虽然类 Unix 的操作系统， 不用文件扩展名来决定文件的内容或用途，但是有些应用程序会。</li><li>虽然 Linux 支持长文件名，文件名可能包含空格，标点符号，但标点符号仅限 使用 “.”，“－”，下划线。最重要的是，不要在文件名中使用空格。如果你想表示词与词间的空格，<strong>用下划线字符来代替</strong>。有些时候，你会感激自己这样做。</li></ol></blockquote><h2 id="探究操作系统"><a href="#探究操作系统" class="headerlink" title="探究操作系统"></a>探究操作系统</h2><blockquote><p>既然我们已经知道了如何在文件系统中跳转，是时候开始 Linux 操作系统之旅了。然而在开始之前，我们先学习一些对研究 Linux 系统有帮助的命令。</p></blockquote><pre><code>file &lt;文件名&gt;  # 确定文件类型less &lt;文件名&gt;  # 浏览文件内容</code></pre><blockquote><p>在进行进一步探究之前，有必要介绍下命令的组成格式。<br>大多数命令看起来像这样：<code>command -options arguments</code><br>大多数命令使用的选项，是由一个中划线加上一个字符组成，例如，“-l”。<br>但是许多命令，包括来自于 GNU 项目的命令，也支持长选项，长选项由两个中划线加上一个字组成。当然， 许多命令也允许把多个短选项串在一起使用。以这个例子<code>ls -lt --reverse</code>为例，代表列出当前目录下的文件详细信息，并按照时间逆序排序。</p></blockquote><h3 id="file-指令"><a href="#file-指令" class="headerlink" title="file 指令"></a>file 指令</h3><blockquote><p>当调用 file 命令后，file 命令会打印出文件内容的简单描述。我们将用 file 命令来确定文件的类型。<br>例如我们想要知道 l2tp.log 文件是用来干什么的，就这样做：</p></blockquote><pre><code>[root@izwz9biz2m4sd3tu00600pz ~]# file l2tp.logl2tp.log: UTF-8 Unicode text, with escape sequences</code></pre><blockquote><p>有许多种类型的文件。事实上，在类 Unix 操作系统中比如说 Linux 中，有个普遍的观念就是“一切皆文件”。 随着课程的进行，我们将会明白这句话是多么的正确。</p></blockquote><h3 id="less-指令"><a href="#less-指令" class="headerlink" title="less 指令"></a>less 指令</h3><blockquote><p>less 命令是一个用来浏览文本文件的程序。纵观 Linux 系统，有许多人类可读的文本文件。less 程序为我们检查文本文件提供了方便。<br>一旦运行起来，less 程序允许你前后滚动文件。例如，要查看一个定义了系统中全部用户身份的文件，输入以下命令：</p></blockquote><pre><code>[root@izwz9biz2m4sd3tu00600pz ~]# less /etc/passwd</code></pre><blockquote><p>一旦 less 程序运行起来，我们就能浏览文件内容了。如果文件内容多于一页，那么我们可以按上、下键滚动文件。按下“q”键， 退出 less 程序。<br>下表列出了 less 程序最常使用的键盘命令。</p></blockquote><div class="table-container"><table><thead><tr><th>命令</th><th>行为</th></tr></thead><tbody><tr><td>Page UP or b</td><td>向上翻滚一页</td></tr><tr><td>Page Down or space</td><td>向下翻滚一页</td></tr><tr><td>UP Arrow</td><td>向上翻滚一行</td></tr><tr><td>Down Arrow</td><td>向下翻滚一行</td></tr><tr><td>G</td><td>移动到最后一行</td></tr><tr><td>1G or g</td><td>移动到开头一行</td></tr><tr><td>/charaters</td><td>向前查找指定的字符串</td></tr><tr><td>n</td><td>向前查找下一个出现的字符串，这个字符串是之前所指定查找的</td></tr><tr><td>h</td><td>显示帮助屏幕</td></tr><tr><td>q</td><td>退出 less 程序</td></tr></tbody></table></div><blockquote><p>less 程序是早期 Unix 程序 more 的改进版。“less” 这个名字，对习语 “less is more” 开了个玩笑， 这个习语是现代主义建筑师和设计者的座右铭。<br>less 属于”页面调度器”类程序，这些程序允许以逐页方式轻松浏览长文本文档。 more 程序只能向前翻页，而 less 程序允许前后翻页，此外还有很多其它的特性。</p></blockquote><h3 id="符号链接-软链接"><a href="#符号链接-软链接" class="headerlink" title="符号链接(软链接)"></a>符号链接(软链接)</h3><blockquote><p>在我们到处查看时，我们可能会看到一个目录，列出像这样的一条信息：<br><code>lrwxrwxrwx 1 root root 11 2007-08-11 07:34 libc.so.6 -&gt; libc-2.6.so</code><br>注意看，为何这条信息第一个字符是“l”，并且有两个文件名呢？ 这是一个特殊文件，叫做符号链接（也称为软链接或者 symlink ）。 在大多数“类 Unix” 系统中， 有可能一个文件被多个文件名所指向。虽然这种特性的意义并不明显，但它真的很有用。<br>描绘一下这样的情景：一个程序要求使用某个包含在名为“foo”文件中的共享资源，但是“foo”经常改变版本号。 这样，在文件名中包含版本号，会是一个好主意，因此管理员或者其它相关方，会知道安装了哪个“foo”版本。 这会导致另一个问题。如果我们更改了共享资源的名字，那么我们必须跟踪每个可能使用了 这个共享资源的程序，当每次这个资源的新版本被安装后，都要让使用了它的程序去寻找新的资源名。 这听起来很没趣。<br>这就是符号链接存在至今的原因。比方说，我们安装了文件 “foo” 的 2.6 版本，它的 文件名是 “foo-2.6”，然后创建了叫做 “foo” 的符号链接，这个符号链接指向 “foo-2.6”。 这意味着，当一个程序打开文件 “foo” 时，它实际上是打开文件 “foo-2.6”。 现在，每个人都很高兴。依赖于 “foo” 文件的程序能找到这个文件，并且我们能知道安装了哪个文件版本。 当升级到 “foo-2.7” 版本的时候，仅添加这个文件到文件系统中，删除符号链接 “foo”， 创建一个指向新版本的符号链接。这不仅解决了版本升级问题，而且还允许在系统中保存两个不同的文件版本。 假想 “foo-2.7” 有个错误（该死的开发者！），那我们得回到原来的版本。 一样的操作，我们只需要删除指向新版本的符号链接，然后创建指向旧版本的符号链接就可以了。</p></blockquote><h3 id="硬链接"><a href="#硬链接" class="headerlink" title="硬链接"></a>硬链接</h3><blockquote><p>讨论到链接问题，我们需要提一下，还有一种链接类型，叫做硬链接。硬链接同样允许文件有多个名字， 但是硬链接以不同的方法来创建多个文件名。</p></blockquote><h3 id="Linux-系统中的目录"><a href="#Linux-系统中的目录" class="headerlink" title="Linux 系统中的目录"></a>Linux 系统中的目录</h3><div class="table-container"><table><thead><tr><th>目录</th><th>评论</th></tr></thead><tbody><tr><td>/</td><td>根目录，万物起源。</td></tr><tr><td>/bin</td><td>包含系统启动和运行所必须的二进制程序。</td></tr><tr><td>/boot</td><td>包含 Linux 内核、初始 RAM 磁盘映像（用于启动时所需的驱动）和 启动加载程序。有趣的文件：/boot/grub/grub.conf or menu.lst， 被用来配置启动加载程序。/boot/vmlinuz，Linux 内核。</td></tr><tr><td>/dev</td><td>这是一个包含设备结点的特殊目录。“一切都是文件”，也适用于设备。 在这个目录里，内核维护着所有设备的列表。</td></tr><tr><td>/etc</td><td>这个目录包含所有系统层面的配置文件。它也包含一系列的 shell 脚本， 在系统启动时，这些脚本会开启每个系统服务。这个目录中的任何文件应该是可读的文本文件。有趣的文件：虽然/etc 目录中的任何文件都有趣，但这里只列出了一些我一直喜欢的文件：/etc/crontab， 定义自动运行的任务。/etc/fstab，包含存储设备的列表，以及与他们相关的挂载点。/etc/passwd，包含用户帐号列表。</td></tr><tr><td>/home</td><td>在通常的配置环境下，系统会在/home 下，给每个用户分配一个目录。普通用户只能在自己的目录下写文件。这个限制保护系统免受错误的用户活动破坏。</td></tr><tr><td>/lib</td><td>包含核心系统程序所使用的共享库文件。这些文件与 Windows 中的动态链接库相似。</td></tr><tr><td>/lost+found</td><td>每个使用 Linux 文件系统的格式化分区或设备，例如 ext3文件系统， 都会有这个目录。当部分恢复一个损坏的文件系统时，会用到这个目录。这个目录应该是空的，除非文件系统 真正的损坏了。</td></tr><tr><td>/media</td><td>在现在的 Linux 系统中，/media 目录会包含可移动介质的挂载点， 例如 USB 驱动器，CD-ROMs 等等。这些介质连接到计算机之后，会自动地挂载到这个目录结点下。</td></tr><tr><td>/mnt</td><td>在早些的 Linux 系统中，/mnt 目录包含可移动介质的挂载点。</td></tr><tr><td>/opt</td><td>这个/opt 目录被用来安装“可选的”软件。这个主要用来存储可能 安装在系统中的商业软件产品。</td></tr><tr><td>/proc</td><td>这个/proc 目录很特殊。从存储在硬盘上的文件的意义上说，它不是真正的文件系统。 相反，它是一个由 Linux 内核维护的虚拟文件系统。它所包含的文件是内核的窥视孔。这些文件是可读的， 它们会告诉你内核是怎样监管计算机的。</td></tr><tr><td>/root</td><td>root 帐户的家目录。</td></tr><tr><td>/sbin</td><td>这个目录包含“系统”二进制文件。它们是完成重大系统任务的程序，通常为超级用户保留。</td></tr><tr><td>/tmp</td><td>这个/tmp 目录，是用来存储由各种程序创建的临时文件的地方。一些配置导致系统每次 重新启动时，都会清空这个目录。</td></tr><tr><td>/usr</td><td>在 Linux 系统中，/usr 目录可能是最大的一个。它包含普通用户所需要的所有程序和文件。</td></tr><tr><td>/usr/bin</td><td>/usr/bin 目录包含系统安装的可执行程序。通常，这个目录会包含许多程序。</td></tr><tr><td>/usr/lib</td><td>包含由/usr/bin 目录中的程序所用的共享库。</td></tr><tr><td>/usr/local</td><td>这个/usr/local 目录，是非系统发行版自带程序的安装目录。 通常，由源码编译的程序会安装在/usr/local/bin 目录下。新安装的 Linux 系统中会存在这个目录， 并且在管理员安装程序之前，这个目录是空的。</td></tr><tr><td>/usr/sbin</td><td>包含许多系统管理程序。</td></tr><tr><td>/usr/share</td><td>/usr/share 目录包含许多由/usr/bin 目录中的程序使用的共享数据。 其中包括像默认的配置文件、图标、桌面背景、音频文件等等。</td></tr><tr><td>/usr/share/doc</td><td>大多数安装在系统中的软件包会包含一些文档。在/usr/share/doc 目录下， 我们可以找到按照软件包分类的文档。</td></tr><tr><td>/var</td><td>除了/tmp 和/home 目录之外，相对来说，目前我们看到的目录是静态的，这是说， 它们的内容不会改变。/var 目录存放的是动态文件。各种数据库，假脱机文件， 用户邮件等等，都位于在这里。</td></tr><tr><td>/var/log</td><td>这个/var/log 目录包含日志文件、各种系统活动的记录。这些文件非常重要，并且 应该时时监测它们。其中最重要的一个文件是/var/log/messages。注意，为了系统安全，在一些系统中， 你必须是超级用户才能查看这些日志文件。</td></tr></tbody></table></div><h2 id="操作文件和目录"><a href="#操作文件和目录" class="headerlink" title="操作文件和目录"></a>操作文件和目录</h2><blockquote><p>此时此刻，我们已经准备好了做些真正的工作！这一章节将会介绍以下命令：</p></blockquote><pre><code>cp &lt;src&gt; &lt;dst&gt; # 复制文件和目录mv &lt;src&gt; &lt;dst&gt; # 移动/重命名文件和目录mkdir &lt;folder_name&gt; # 创建目录rm &lt;dir/file&gt; # 删除文件和目录ln # 创建硬链接和符号链接</code></pre><blockquote><p>这五个命令属于最常使用的 Linux 命令之列。它们用来操作文件和目录。<br>现在，坦诚地说，用图形文件管理器来完成一些由这些命令执行的任务会更容易些。使用文件管理器， 我们可以把文件从一个目录拖放到另一个目录、剪贴和粘贴文件、删除文件等等。那么， 为什么还使用早期的命令行程序呢？<br>答案是命令行程序，功能强大灵活。虽然图形文件管理器能轻松地实现简单的文件操作，但是对于 复杂的文件操作任务，则使用命令行程序比较容易完成。例如，怎样拷贝一个目录下所有的HTML文件 ——这些文件在目标目录不存在或者版本比目标目录的文件更新——到目标目录呢？ 要完成这个任务，使用文件管理器相当难，使用命令行相当容易：<code>cp -u *.html destination</code></p></blockquote><h3 id="通配符"><a href="#通配符" class="headerlink" title="通配符"></a>通配符</h3><blockquote><p>在开始使用命令之前，我们需要介绍一个使命令行如此强大的 shell 特性。因为 shell 频繁地使用 文件名，shell 提供了特殊字符来帮助你快速指定一组文件名。这些特殊字符叫做通配符。 使用通配符（也以文件名代换著称）允许你依据字符的组合模式来选择文件名。下表列出这些通配符 以及它们所选择的对象：</p></blockquote><div class="table-container"><table><thead><tr><th>通配符</th><th>意义</th></tr></thead><tbody><tr><td>*</td><td>匹配任意多个字符（包括零个或一个）</td></tr><tr><td>?</td><td>匹配任意一个字符（不包括零个）</td></tr><tr><td>[characters]</td><td>匹配任意一个属于字符集中的字符</td></tr><tr><td>[!characters]</td><td>匹配任意一个不是字符集中的字符</td></tr><tr><td>[[:class:]]</td><td>匹配任意一个属于指定字符类中的字符</td></tr></tbody></table></div><blockquote><p>下表列出了最常使用的字符类：</p></blockquote><div class="table-container"><table><thead><tr><th>字符类</th><th>意义</th></tr></thead><tbody><tr><td>[:alnum:]</td><td>匹配任意一个字母或数字</td></tr><tr><td>[:alpha:]</td><td>匹配任意一个字母</td></tr><tr><td>[:digit:]</td><td>匹配任意一个数字</td></tr><tr><td>[:lower:]</td><td>匹配任意一个小写字母</td></tr><tr><td>[:upper:]</td><td>匹配任意一个大写字母</td></tr></tbody></table></div><blockquote><p>借助通配符，为文件名构建非常复杂的选择标准成为可能。下面是一些类型匹配的范例:</p></blockquote><div class="table-container"><table><thead><tr><th>模式</th><th>匹配对象</th></tr></thead><tbody><tr><td>*</td><td>所有文件</td></tr><tr><td>g*</td><td>文件名以“g”开头的文件</td></tr><tr><td>b*.txt</td><td>以”b”开头，中间有零个或任意多个字符，并以”.txt”结尾的文件</td></tr><tr><td>Data???</td><td>以“Data”开头，其后紧接着3个字符的文件</td></tr><tr><td>[abc]*</td><td>文件名以”a”,”b”,或”c”开头的文件</td></tr><tr><td>[[:upper:]]*</td><td>以大写字母开头的文件</td></tr><tr><td>[![:digit:]]*</td><td>不以数字开头的文件</td></tr><tr><td>*[[:lower:]123]</td><td>文件名以小写字母结尾，或以 “1”，“2”，或 “3” 结尾的文件</td></tr></tbody></table></div><h3 id="mkdir-指令"><a href="#mkdir-指令" class="headerlink" title="mkdir 指令"></a>mkdir 指令</h3><blockquote><p>mkdir 命令是用来创建目录的。它这样工作：</p></blockquote><pre><code>mkdir directory...</code></pre><blockquote><p>注意表示法: 在描述一个命令时（如上所示），当有三个圆点跟在一个命令的参数后面， 这意味着那个参数可以重复，就像这样：<code>mkdir dir1 dir2 dir3</code>会创建三个目录，名为 dir1, dir2, dir3。</p></blockquote><h3 id="cp-指令"><a href="#cp-指令" class="headerlink" title="cp 指令"></a>cp 指令</h3><blockquote><p>cp 命令，复制文件或者目录。它有两种使用方法：</p></blockquote><pre><code>cp item1 item2 # 用法一</code></pre><blockquote><p>用法一：复制<code>item1</code>到<code>item2</code>，其中<code>item</code>是文件或者是目录</p></blockquote><pre><code>cp item... directory # 用法二</code></pre><blockquote><p>用法二：复制多个<code>item</code>（文件或目录）到同一个目录<code>directory</code>下。</p><p>这里列举了 cp 命令一些有用的选项（短选项和等效的长选项）：</p></blockquote><div class="table-container"><table><thead><tr><th>选项</th><th>意义</th></tr></thead><tbody><tr><td>-a, —archive</td><td>复制文件和目录，以及它们的属性，包括所有权和权限。 通常，副本具有用户所操作文件的默认属性。</td></tr><tr><td>-i, —interactive</td><td>在重写已存在文件之前，提示用户确认。如果这个选项不指定， cp 命令会默认重写文件。</td></tr><tr><td>-r, —recursive</td><td>递归地复制目录及目录中的内容。当复制目录时， 需要这个选项（或者-a 选项）。</td></tr><tr><td>-u, —update</td><td>当把文件从一个目录复制到另一个目录时，仅复制 目标目录中不存在的文件，或者是文件内容新于目标目录中已经存在的文件。</td></tr><tr><td>-v, —verbose</td><td>显示翔实的命令操作信息</td></tr></tbody></table></div><blockquote><p>应用例子：</p></blockquote><div class="table-container"><table><thead><tr><th>命令</th><th>运行结果</th></tr></thead><tbody><tr><td>cp file1 file2</td><td>复制文件 file1 内容到文件 file2。如果 file2 已经存在， file2 的内容会被 file1 的内容重写。如果 file2 不存在，则会创建 file2。</td></tr><tr><td>cp -i file1 file2</td><td>这条命令和上面的命令一样，除了如果文件 file2 存在的话，在文件 file2 被重写之前， 会提示用户确认信息。</td></tr><tr><td>cp file1 file2 dir1</td><td>复制文件 file1 和文件 file2 到目录 dir1。目录 dir1 必须存在。</td></tr><tr><td>cp dir1/* dir2</td><td>使用一个通配符，在目录 dir1 中的所有文件都被复制到目录 dir2 中。 dir2 必须已经存在。</td></tr><tr><td>cp -r dir1 dir2</td><td>复制目录 dir1 中的内容到目录 dir2。如果目录 dir2 不存在， 创建目录 dir2，操作完成后，目录 dir2 中的内容和 dir1 中的一样。 如果目录 dir2 存在，则目录 dir1 (和目录中的内容)将会被复制到 dir2 中。</td></tr></tbody></table></div><h3 id="mv-指令"><a href="#mv-指令" class="headerlink" title="mv 指令"></a>mv 指令</h3><blockquote><p>mv 命令可以执行文件移动和文件命名任务，这依赖于你怎样使用它。任何一种 情况下，完成操作之后，原来的文件名不再存在。mv 使用方法与 cp 很相像：</p></blockquote><pre><code>mv item1 item2 # 方式一</code></pre><blockquote><p>方式一：把文件或目录 “item1” 移动或重命名为 “item2”, 或者：</p></blockquote><pre><code>mv item... directory # 方式二</code></pre><blockquote><p>方式二：把一个或多个条目从一个目录移动到另一个目录中。</p><p>mv 与 cp 共享了很多一样的选项：</p></blockquote><div class="table-container"><table><thead><tr><th>选项</th><th>意义</th></tr></thead><tbody><tr><td>-i —interactive</td><td>在重写一个已经存在的文件之前，提示用户确认信息。 <strong>如果不指定这个选项，mv 命令会默认重写文件内容。</strong></td></tr><tr><td>-u —update</td><td>当把文件从一个目录移动另一个目录时，只是移动不存在的文件， 或者文件内容新于目标目录相对应文件的内容。</td></tr><tr><td>-v —verbose</td><td>当操作 mv 命令时，显示翔实的操作信息。</td></tr></tbody></table></div><blockquote><p>操作实例：</p></blockquote><div class="table-container"><table><thead><tr><th>mv file1 file2</th><th>移动 file1 到 file2。 <strong>如果 file2 存在，它的内容会被 file1 的内容重写。如果 file2 不存在，则创建 file2。这两种情况下，file1 都不再存在。</strong></th></tr></thead><tbody><tr><td>mv -i file1 file2</td><td>除了如果 file2 存在的话，在 file2 被重写之前，用户会得到 提示信息外，这个和上面的选项一样。</td></tr><tr><td>mv file1 file2 dir1</td><td>移动 file1 和 file2 到目录 dir1 中。dir1 必须已经存在。</td></tr><tr><td>mv dir1 dir2</td><td>如果目录 dir2 不存在，创建目录 dir2，并且移动目录 dir1 的内容到 目录 dir2 中，同时删除目录 dir1。如果目录 dir2 存在，移动目录 dir1（及它的内容）到目录 dir2。</td></tr></tbody></table></div><h3 id="rm-指令"><a href="#rm-指令" class="headerlink" title="rm 指令"></a>rm 指令</h3><blockquote><p>rm 命令用来移除（删除）文件和目录：</p></blockquote><pre><code>rm item...</code></pre><blockquote><p>“item”代表一个或多个文件或目录。</p><p>下表是一些普遍使用的 rm 选项：</p></blockquote><div class="table-container"><table><thead><tr><th>选项</th><th>意义</th></tr></thead><tbody><tr><td>-i, —interactive</td><td>在删除已存在的文件前，提示用户确认信息。 <strong>如果不指定这个选项，rm 会默默地删除文件</strong></td></tr><tr><td>-r, —recursive</td><td>递归地删除文件，这意味着，如果要删除一个目录，而此目录 又包含子目录，那么子目录也会被删除。要删除一个目录，必须指定这个选项。</td></tr><tr><td>-f, —force</td><td>忽视不存在的文件，不显示提示信息。这选项覆盖了“—interactive”选项。</td></tr><tr><td>-v, —verbose</td><td>在执行 rm 命令时，显示翔实的操作信息。</td></tr></tbody></table></div><blockquote><p>操作实例：</p></blockquote><div class="table-container"><table><thead><tr><th>命令</th><th>运行结果</th></tr></thead><tbody><tr><td>rm file1</td><td>默默地删除文件</td></tr><tr><td>rm -i file1</td><td>除了在删除文件之前，提示用户确认信息之外，和上面的命令作用一样。</td></tr><tr><td>rm -r file1 dir1</td><td>删除文件 file1, 目录 dir1，及 dir1 中的内容。</td></tr><tr><td>rm -rf file1 dir1</td><td>同上，除了如果文件 file1，或目录 dir1 不存在的话，rm 仍会继续执行。</td></tr></tbody></table></div><blockquote><p><strong>小心 rm!</strong><br>类 Unix 的操作系统，比如说 Linux，没有复原命令。一旦你用 rm 删除了一些东西， 它就消失了。Linux 假定你很聪明，你知道你在做什么。<br>尤其要小心通配符。思考一下这个经典的例子。假如说，你只想删除一个目录中的 HTML 文件。输入：<code>rm *.html</code><br>这是正确的，如果你不小心在 “<em>” 和 “.html” 之间多输入了一个空格，就像这样：`rm </em> .html`<br>这个 rm 命令会删除目录中的所有文件，还会抱怨没有文件叫做 “.html”。<br><strong>[小贴士]</strong> 当你使用带有通配符的rm命令时（除了仔细检查输入的内容外）， 先用 ls 命令来测试通配符。这会让你看到将要被删除的文件是什么。然后按下上箭头按键，重新调用 刚刚执行的命令，用 rm 替换 ls。</p></blockquote><h3 id="ln-指令"><a href="#ln-指令" class="headerlink" title="ln 指令"></a>ln 指令</h3><blockquote><p>ln 命令既可创建硬链接，也可以创建符号链接。</p></blockquote><pre><code>ln file link # 创建硬链接ln -s item link # 创建符号链接，”item” 可以是一个文件或是一个目录。</code></pre><blockquote><p>与更加现代的符号链接相比，硬链接是最初 Unix 创建链接的方式。每个文件默认会有一个硬链接， 这个硬链接给予文件名字。我们每创建一个硬链接，就为一个文件创建了一个额外的目录项。 硬链接有两个重要局限性：</p><ol><li>一个硬链接不能关联它所在文件系统之外的文件。这是说一个链接不能关联与链接本身不在同一个磁盘分区上的文件。</li><li>一个硬链接不能关联一个目录。<br>一个硬链接和文件本身没有什么区别。不像符号链接，当你列出一个包含硬链接的目录内容时，你会看到没有特殊的链接指示说明。当一个硬链接被删除时，这个链接 被删除，但是文件本身的内容仍然存在（这是说，它所占的磁盘空间不会被重新分配）， 直到所有关联这个文件的链接都删除掉。知道硬链接很重要，因为你可能有时 会遇到它们，但现在实际中更喜欢使用符号链接，下一步我们会讨论符号链接。</li></ol><p>创建符号链接是为了克服硬链接的局限性。符号链接生效，是通过创建一个 特殊类型的文件，这个文件包含一个关联文件或目录的文本指针。在这一方面， 它们和 Windows 的快捷方式差不多，当然，符号链接早于 Windows 的快捷方式 很多年 ;-)<br>一个符号链接指向一个文件，而且这个符号链接本身与其它的符号链接几乎没有区别。 例如，如果你往一个符号链接里面写入东西，那么相关联的文件也被写入。然而， 当你删除一个符号链接时，只有这个链接被删除，而不是文件自身。如果先于符号链接 删除文件，这个链接仍然存在，但是不指向任何东西。在这种情况下，这个链接被称为 坏链接。在许多实现中，ls 命令会以不同的颜色展示坏链接，比如说红色，来显示它们 的存在。</p><p>关于链接的概念，看起来很迷惑，但不要胆怯。我们将要试着练习 这些命令，希望，它变得清晰起来。</p></blockquote><h3 id="创建游戏场（实战演习）"><a href="#创建游戏场（实战演习）" class="headerlink" title="创建游戏场（实战演习）"></a>创建游戏场（实战演习）</h3><blockquote><p>下面我们将要做些真正的文件操作，让我们先建立一个安全地带， 来玩一下文件操作命令。首先，我们需要一个工作目录。在我们的 家目录下创建一个叫做“playground”的目录。</p></blockquote><pre><code>[root@izwz9biz2m4sd3tu00600pz ~]# cd[root@izwz9biz2m4sd3tu00600pz ~]# mkdir playground[root@izwz9biz2m4sd3tu00600pz ~]# cd playground[root@izwz9biz2m4sd3tu00600pz playground]# mkdir dir1 dir2[root@izwz9biz2m4sd3tu00600pz playground]# cp /etc/passwd .[root@izwz9biz2m4sd3tu00600pz playground]# ls -ltotal 12drwxr-xr-x 2 root root 4096 Mar  8 23:19 dir1drwxr-xr-x 2 root root 4096 Mar  8 23:19 dir2-rw-r--r-- 1 root root 1320 Mar  8 23:20 passwd[root@izwz9biz2m4sd3tu00600pz playground]# mv fun dir1[root@izwz9biz2m4sd3tu00600pz playground]# ls dir1fun[root@izwz9biz2m4sd3tu00600pz playground]# mv dir1/fun .[root@izwz9biz2m4sd3tu00600pz playground]# lsdir1  dir2  fun</code></pre><blockquote><p>现在，我们试着创建链接。首先是硬链接。我们创建一些关联我们数据文件的链接：</p></blockquote><pre><code>[root@izwz9biz2m4sd3tu00600pz playground]# ln fun fun-hard[root@izwz9biz2m4sd3tu00600pz playground]# ln fun dir1/fun-hard[root@izwz9biz2m4sd3tu00600pz playground]# ln fun dir2/fun-hard[root@izwz9biz2m4sd3tu00600pz playground]# ls -ltotal 16drwxr-xr-x 2 root root 4096 Mar  8 23:26 dir1drwxr-xr-x 2 root root 4096 Mar  8 23:26 dir2-rw-r--r-- 4 root root 1320 Mar  8 23:21 fun-rw-r--r-- 4 root root 1320 Mar  8 23:21 fun-hard</code></pre><blockquote><p>注意到一件事，列表中，文件 fun 和 fun-hard 的第二个字段是”4”，这个数字 是文件”fun”的硬链接数目。你要记得一个文件至少有一个硬链接，因为文件 名就是由链接创建的。那么，我们怎样知道实际上 fun 和 fun-hard 是同一个文件呢？ 在这个例子里，ls 不是很有用。虽然我们能够看到 fun 和 fun-hard 文件大小一样 （第五字段），但我们的列表没有提供可靠的信息来确定（这两个文件一样）。 为了解决这个问题，我们更深入的研究一下。<br>当考虑到硬链接的时候，我们可以假设文件由两部分组成：包含文件内容的数据部分和持有文件名的名字部分 ，这将有助于我们理解这个概念。当我们创建文件硬链接的时候，实际上是为文件创建了额外的名字部分， 并且这些名字都关联到相同的数据部分。这时系统会分配一连串的磁盘块给所谓的索引节点，然后索引节点与文 件名字部分相关联。因此每一个硬链接都关系到一个具体的包含文件内容的索引节点。<br>ls 命令有一种方法，来展示（文件索引节点）的信息。在命令中加上”-i”选项：</p></blockquote><pre><code>[root@izwz9biz2m4sd3tu00600pz playground]# ls -litotal 16131435 drwxr-xr-x 2 root root 4096 Mar  8 23:26 dir1131436 drwxr-xr-x 2 root root 4096 Mar  8 23:26 dir2131437 -rw-r--r-- 4 root root 1320 Mar  8 23:21 fun131437 -rw-r--r-- 4 root root 1320 Mar  8 23:21 fun-hard</code></pre><blockquote><p>在这个版本的列表中，第一字段表示文件索引节点号，正如我们所见到的， fun 和 fun-hard 共享一样的索引节点号，这就证实这两个文件是同一个文件。</p><p>建立符号链接的目的是为了克服硬链接的两个缺点：硬链接不能跨越物理设备， 硬链接不能关联目录，只能是文件。符号链接是文件的特殊类型，它包含一个指向 目标文件或目录的文本指针。<br>符号链接的建立过程相似于创建硬链接：</p></blockquote><pre><code>[root@izwz9biz2m4sd3tu00600pz playground]# ln -s fun fun-sym[root@izwz9biz2m4sd3tu00600pz playground]# ln -s ../fun dir1/fun-sym[root@izwz9biz2m4sd3tu00600pz playground]# ln -s ../fun dir2/fun-sym</code></pre><blockquote><p>第一个例子相当直接，在 ln 命令中，简单地加上”-s”选项就可以创建一个符号链接， 而不是一个硬链接。下面两个例子又是怎样呢？ 记住，当我们创建一个符号链接 的时候，会建立一个目标文件在哪里和符号链接有关联的文本描述。如果我们看看 ls 命令的输出结果，比较容易理解。</p></blockquote><pre><code>[root@izwz9biz2m4sd3tu00600pz playground]# ls -l dir1total 4-rw-r--r-- 4 root root 1320 Mar  8 23:21 fun-hardlrwxrwxrwx 1 root root    6 Mar  8 23:34 fun-sym -&gt; ../fun</code></pre><blockquote><p>目录 dir1 中，fun-sym 的列表说明了它是一个符号链接，通过在第一字段中的首字符”l” 可知，并且它还指向”../fun”，也是正确的。相对于 fun-sym 的存储位置，fun 在它的 上一个目录。同时注意，符号链接文件的长度是6，这是字符串”../fun”所包含的字符数， 而不是符号链接所指向的文件长度。<br>当建立符号链接时，你既可以使用绝对路径名：<code>ln -s /home/me/playground/fun dir1/fun-sym</code>，也可用相对路径名，正如前面例题所展示的。使用相对路径名更令人满意， 因为它允许一个包含符号链接的目录重命名或移动，而不会破坏链接。<br>除了普通文件，符号链接也能关联目录：</p></blockquote><pre><code>[root@izwz9biz2m4sd3tu00600pz playground]# ln -s dir1 dir1-sym[root@izwz9biz2m4sd3tu00600pz playground]# ls -ltotal 16drwxr-xr-x 2 root root 4096 Mar  8 23:34 dir1lrwxrwxrwx 1 root root    4 Mar  8 23:40 dir1-sym -&gt; dir1drwxr-xr-x 2 root root 4096 Mar  8 23:34 dir2-rw-r--r-- 4 root root 1320 Mar  8 23:21 fun-rw-r--r-- 4 root root 1320 Mar  8 23:21 fun-hardlrwxrwxrwx 1 root root    3 Mar  8 23:34 fun-sym -&gt; fun</code></pre><blockquote><p>正如我们之前讨论的，rm 命令被用来删除文件和目录。我们将要使用它 来清理一下我们的游戏场。首先，删除一个硬链接：</p></blockquote><pre><code>[root@izwz9biz2m4sd3tu00600pz playground]# rm fun-hardrm: remove regular file ‘fun-hard’? y[root@izwz9biz2m4sd3tu00600pz playground]# ls -ltotal 12drwxr-xr-x 2 root root 4096 Mar  8 23:34 dir1lrwxrwxrwx 1 root root    4 Mar  8 23:40 dir1-sym -&gt; dir1drwxr-xr-x 2 root root 4096 Mar  8 23:34 dir2-rw-r--r-- 3 root root 1320 Mar  8 23:21 funlrwxrwxrwx 1 root root    3 Mar  8 23:34 fun-sym -&gt; fun[root@izwz9biz2m4sd3tu00600pz playground]# rm fun-sym dir1-symrm: remove symbolic link ‘fun-sym’? yrm: remove symbolic link ‘dir1-sym’? y[root@izwz9biz2m4sd3tu00600pz playground]# lsdir1  dir2  fun</code></pre><blockquote><p>对于符号链接，有一点值得记住，执行的大多数文件操作是针对链接的对象，而不是链接本身。 而 rm 命令是个特例。当你删除链接的时候，删除链接本身，而不是链接的对象。</p><p>最后，我们将删除我们的游戏场。为了完成这个工作，我们将返回到 我们的家目录，然后用 rm 命令加上选项(-r)，来删除目录 playground， 和目录下的所有内容，包括子目录：</p></blockquote><pre><code>[root@izwz9biz2m4sd3tu00600pz playground]# cd[root@izwz9biz2m4sd3tu00600pz ~]# rm -r  playgroundrm: descend into directory ‘playground’? yrm: descend into directory ‘playground/dir1’? yrm: remove symbolic link ‘playground/dir1/fun-sym’? yrm: remove regular file ‘playground/dir1/fun-hard’? yrm: remove directory ‘playground/dir1’? yrm: descend into directory ‘playground/dir2’? yrm: remove symbolic link ‘playground/dir2/fun-sym’? yrm: remove regular file ‘playground/dir2/fun-hard’? yrm: remove directory ‘playground/dir2’? yrm: remove regular file ‘playground/fun’? yrm: remove directory ‘playground’? y</code></pre><blockquote><p>在这一章中，我们已经研究了许多基础知识。我们得花费一些时间来全面地理解。 反复练习 playground 例题，直到你觉得它有意义。能够良好地理解基本文件操作 命令和通配符，非常重要。随意通过添加文件和目录来拓展 playground 练习， 使用通配符来为各种各样的操作命令指定文件。关于链接的概念，在刚开始接触 时会觉得有点迷惑，花些时间来学习它们是怎样工作的。它们能成为真正的救星。</p></blockquote><p><strong>TO BE CONTINUED…</strong></p>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> grammar </tag>
            
            <tag> linux </tag>
            
            <tag> programing </tag>
            
            <tag> os </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Markdown-快速入门</title>
      <link href="/markdown/markdown-beginner/"/>
      <url>/markdown/markdown-beginner/</url>
      
        <content type="html"><![CDATA[<p>Markdown 是什么？它是一门标记语言，它能实现与 Microsoft Word 文字排版相似的功能。而这篇文章正是用 Markdown 语言编写的，我们使用它来进行文字的排版。</p><p><img src="Markdown-mark.svg" alt></p><h1 id="行文结构"><a href="#行文结构" class="headerlink" title="行文结构"></a>行文结构</h1><h2 id="标题"><a href="#标题" class="headerlink" title="标题"></a>标题</h2><blockquote><p>使用井号 “#” 来调整标题的大小。实例代码如下：</p></blockquote><pre><code># 一级大标题## 二级标题### 三级标题#### 以此类推</code></pre><blockquote><p>效果如下：<br><img src="1.png" alt></p></blockquote><h2 id="分割线"><a href="#分割线" class="headerlink" title="分割线"></a>分割线</h2><blockquote><p>使用三个横线 “---” 来实现分割线，实例代码如下</p></blockquote><pre><code>---</code></pre><blockquote><p>效果如下：</p><hr></blockquote><h1 id="段落文字"><a href="#段落文字" class="headerlink" title="段落文字"></a>段落文字</h1><h2 id="粗体"><a href="#粗体" class="headerlink" title="粗体"></a>粗体</h2><blockquote><p>使用两个星号 “**” 包围住想要加粗的字体，代码如下：（其中的符号 “**” 也可以用 “__” 替代，注意不带空格）</p></blockquote><pre><code>**我是粗体**</code></pre><blockquote><p>效果如下：<br><strong>我是粗体</strong></p><h2 id="斜体"><a href="#斜体" class="headerlink" title="斜体"></a>斜体</h2><p>使用一个星号 “*” 包围住想要加粗的字体，代码如下：（其中的符号 “*” 也可以用 “_” 替代，注意不带空格）</p></blockquote><pre><code>*我是斜体*</code></pre><blockquote><p>效果如下：<br><em>我是斜体</em><br>如果想要粗斜体，则使用三个星号 “***” 包围即可。</p></blockquote><h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><blockquote><p>此处的引用指的是在段落的前方出现一条竖线，代码如下：</p></blockquote><pre><code> &gt; Markdown is a lightweight markup language with plain text formatting syntax. Its design allows it to be converted to many output formats, but the original tool by the same name only supports HTML.</code></pre><blockquote><p> 效果如下：</p><blockquote><p>Markdown is a lightweight markup language with plain text formatting syntax. Its design allows it to be converted to many output formats, but the original tool by the same name only supports HTML.</p></blockquote></blockquote><h2 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h2><blockquote><p>列表分为有序列表和无序列表，代码如下：（其中的符号 “-” 也可以用 “*” 或 “+” 替代）</p></blockquote><pre><code>- 无序列表A- 无序列表B1. 有序列表A2. 有序列表B</code></pre><blockquote><p>效果如下：</p><ul><li>无序列表A</li><li>无序列表B</li></ul><ol><li>有序列表A</li><li>有序列表B</li></ol></blockquote><h2 id="选择框"><a href="#选择框" class="headerlink" title="选择框"></a>选择框</h2><blockquote><p>选择框类似windows的勾选框框，代码如下：</p></blockquote><pre><code>- [ ] 不勾选- [x] 勾选</code></pre><blockquote><p>效果如下：</p><ul><li>[ ] 不勾选</li><li>[x] 勾选</li></ul></blockquote><h2 id="超链接"><a href="#超链接" class="headerlink" title="超链接"></a>超链接</h2><blockquote><p>代码如下：</p></blockquote><pre><code>[Karbo的博客](https://blog.karbo.online)</code></pre><blockquote><p>效果如下：<br><a href="https://blog.karbo.online">Karbo的博客</a></p></blockquote><h1 id="特殊部件"><a href="#特殊部件" class="headerlink" title="特殊部件"></a>特殊部件</h1><h2 id="代码段"><a href="#代码段" class="headerlink" title="代码段"></a>代码段</h2><blockquote><p>代码段一般支持高亮功能，用于显示代码，示例代码如下：</p></blockquote><p><img src="2.png" alt></p><blockquote><p>代码段被三个反引号包括（即数字1左边的那个符号），在其后可以指定编程语言的类型。<br>效果如下：</p></blockquote><pre class=" language-lang-c++"><code class="language-lang-c++">int main(int argc, char *argv[]){    if (argc == 2)    {        cout << argv[1] << endl;    }}</code></pre><h2 id="表格"><a href="#表格" class="headerlink" title="表格"></a>表格</h2><blockquote><p>第一列左对齐、第二列居中、第三列右对齐的表格，实现代码如下：</p></blockquote><pre><code>| 1    |  2   |    3 || :--- | :--: | ---: || 4    |  5   |    6 || 7    |  8   |    9 |</code></pre><blockquote><p>效果如下：</p></blockquote><div class="table-container"><table><thead><tr><th style="text-align:left">1</th><th style="text-align:center">2</th><th style="text-align:right">3</th></tr></thead><tbody><tr><td style="text-align:left">4</td><td style="text-align:center">5</td><td style="text-align:right">6</td></tr><tr><td style="text-align:left">7</td><td style="text-align:center">8</td><td style="text-align:right">9</td></tr></tbody></table></div><h2 id="图片"><a href="#图片" class="headerlink" title="图片"></a>图片</h2><blockquote><p>可以在 Markdown 中嵌入图片，示例代码如下：</p></blockquote><pre><code>![图片的描述](图片的路径 鼠标悬浮于图片时显示的文字)</code></pre><blockquote><ol><li>其中 “图片的描述” 为用来描述图片的关键词，可以不写。最初的本意是当图片因为某种原因不能被显示时而出现的替代文字，后来又被用于SEO，可以方便搜索引擎根据描述关键词搜索到图片。</li><li>其中 “图片的路径” 可以使用绝对路径、相对路径，或是图片网址。<br> 如果你的 Markdown 编辑器是 Typora，则可以通过在 Front Matter 中配置 typora-root-url 的值来指定寻找图片的根路径。</li><li>其中 “鼠标悬浮于图片时显示的文字” 为可选选项，可以不填。<br>除此之外，插入图片还有类似于 reference 的方式，示例代码如下：</li></ol></blockquote><pre><code>以下是我们的图片啦：![GitHub][github]以上为我们的图片了。[github]: https://avatars2.githubusercontent.com/u/3265208?v=3&amp;s=100 &quot;GitHub,Social Coding&quot;</code></pre><blockquote><p>上述代码则会在 “以下是我们的图片啦：” 和 “以上为我们的图片了。” 之间插入图片。<br>相当于为图片创建了一个id，叫 github，以后使用图片只需要<code>![][github]</code>这样就可以啦。而<code>[github]: https://avatars2.githubusercontent.com/u/3265208?v=3&amp;s=100 &quot;GitHub,Social Coding&quot;</code>这句话则不会显示，因为它是用来指明 github 是什么图片的语句。</p><p>效果如下：</p><p>以下是我们的图片啦：<br><img src="https://avatars2.githubusercontent.com/u/3265208?v=3&amp;s=100" alt="GitHub" title="GitHub,Social Coding"><br>以上为我们的图片了。</p></blockquote><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><blockquote><p>Markdown 还支持画流程图、时序图、甘特图等<br>因为不常用，这里就不一一列出使用方法了，网上有教程。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> markdown </category>
          
      </categories>
      
      
        <tags>
            
            <tag> grammar </tag>
            
            <tag> programing </tag>
            
            <tag> markdown </tag>
            
            <tag> writing </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数学资源</title>
      <link href="/math/mathlearn/"/>
      <url>/math/mathlearn/</url>
      
        <content type="html"><![CDATA[<p>本文搜集了一些网络上优秀的数学学习资源。</p><p><img src="1.jpg" alt></p><h1 id="3Blue1Brown"><a href="#3Blue1Brown" class="headerlink" title="3Blue1Brown"></a>3Blue1Brown</h1><blockquote><p>3blue1brown的创始人是毕业于斯坦福大学的Grant Sanderson，他用“可以看见”的动画方式，通俗易懂、深入浅出的解释了数学原理。这是一种“让你懂”的解释，而不是一种“仅他懂”的证明。3blue1brown是我见过的最好的数学科普网站，没有之一。</p></blockquote><p><img src="2.png" alt></p><p><img src="3.png" alt></p><blockquote><p>链接：</p><ul><li><a href="https://www.3blue1brown.com/" target="_blank" rel="noopener">3Blue1Brown（油管）</a></li><li><a href="https://space.bilibili.com/88461692/" target="_blank" rel="noopener">3Blue1Brown（B站）</a></li></ul></blockquote><h1 id="沉浸式线性代数"><a href="#沉浸式线性代数" class="headerlink" title="沉浸式线性代数"></a>沉浸式线性代数</h1><blockquote><p>本网站试图用交互式的动图来解释线性代数，一图胜千言。</p></blockquote><ul><li><a href="http://immersivemath.com/ila/index.html" target="_blank" rel="noopener">immersive linear algebra</a></li></ul><p><img src="4.png" alt></p><p><img src="5.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> math </category>
          
      </categories>
      
      
        <tags>
            
            <tag> math </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
