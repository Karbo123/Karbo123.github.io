<!DOCTYPE HTML>
<html lang="zh-CN">


<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">
    <meta name="keywords" content="3D结构重建, 任我行">
    <meta name="description" content="3D重建介绍3D重建的各种模型框架方法等。论文日期以Arxiv上第一版的日期为准。
3D数据常用的表示方法：

Rasterized forms例如voxels and multi-view RGB(D) images.
Geometric">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <title>3D结构重建 | 任我行</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/font-awesome.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">
    <style type="text/css">
        
    </style>

    <script src="/libs/jquery/jquery-2.2.0.min.js"></script>
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head>


<body>

<header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">任我行</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fa fa-navicon"></i></a>
<ul class="right">
    
    <li class="hide-on-med-and-down">
        <a href="/" class="waves-effect waves-light">
            
            <i class="fa fa-home"></i>
            
            <span>首页</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/tags" class="waves-effect waves-light">
            
            <i class="fa fa-tags"></i>
            
            <span>标签</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/categories" class="waves-effect waves-light">
            
            <i class="fa fa-bookmark"></i>
            
            <span>分类</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/archives" class="waves-effect waves-light">
            
            <i class="fa fa-archive"></i>
            
            <span>归档</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/about" class="waves-effect waves-light">
            
            <i class="fa fa-user-circle-o"></i>
            
            <span>关于</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/friends" class="waves-effect waves-light">
            
            <i class="fa fa-address-book"></i>
            
            <span>友情链接</span>
        </a>
    </li>
    
    <li>
        <a href="#searchModal" class="modal-trigger waves-effect waves-light">
            <i id="searchIcon" class="fa fa-search" title="搜索"></i>
        </a>
    </li>
</ul>

<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">任我行</div>
        <div class="logo-desc">
            
            可以任我走怎么到头来又随着大队走，人群是那么像羊群
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li>
            <a href="/" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-home"></i>
                
                首页
            </a>
        </li>
        
        <li>
            <a href="/tags" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-tags"></i>
                
                标签
            </a>
        </li>
        
        <li>
            <a href="/categories" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-bookmark"></i>
                
                分类
            </a>
        </li>
        
        <li>
            <a href="/archives" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-archive"></i>
                
                归档
            </a>
        </li>
        
        <li>
            <a href="/about" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-user-circle-o"></i>
                
                关于
            </a>
        </li>
        
        <li>
            <a href="/friends" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-address-book"></i>
                
                友情链接
            </a>
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Karbo123" class="waves-effect waves-light" target="_blank">
                <i class="fa fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>

        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Karbo123" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>





<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/17.jpg')">
    <div class="container">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <div class="description center-align post-title">
                        3D结构重建
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>



<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #toc-content .is-active-link::before {
        background-color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 20px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/3d/" target="_blank">
                                <span class="chip bg-color">3d</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fa fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/3d/" class="post-category" target="_blank">
                                3d
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                <div class="post-date info-break-policy">
                    <i class="fa fa-calendar-minus-o fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2019-10-08
                </div>

                
                    
                    <div class="info-break-policy">
                        <i class="fa fa-file-word-o fa-fw"></i>文章字数:&nbsp;&nbsp;
                        20.6k
                    </div>
                    

                    
                    <div class="info-break-policy">
                        <i class="fa fa-clock-o fa-fw"></i>阅读时长:&nbsp;&nbsp;
                        87 分
                    </div>
                    
                
				
				
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="fa fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <h1 id="3D重建"><a href="#3D重建" class="headerlink" title="3D重建"></a>3D重建</h1><p>介绍3D重建的各种模型框架方法等。论文日期以Arxiv上第一版的日期为准。</p>
<p>3D数据常用的表示方法：</p>
<ul>
<li>Rasterized forms例如voxels and multi-view RGB(D) images.</li>
<li>Geometric forms例如point clouds, polygon meshes, and sets of primitives</li>
<li>其他：geometry images、depth images、classification boundaries、signed distance function</li>
</ul>
<p>常用数据集（出自文章Mask R-CNN）：</p>
<ul>
<li>ShapeNet is a large-scale dataset of CAD models which are rendered to give synthetic images</li>
<li>IKEA dataset aligns CAD models of IKEA objects to real-world images</li>
<li>Pix3D extends this idea to a larger set of images and models</li>
<li>Pascal3D aligns CAD models to real-world images, but it is unsuitable for shape reconstruction since its train and test sets share the same small set of models</li>
<li>KITTI annotates outdoor street scenes with 3D bounding boxes, but does not provide shape annotations</li>
</ul>
<p>概念：</p>
<ul>
<li>Geometry：例如顶点的位移，顶点的形变</li>
<li>Topology：例如亏格的改变</li>
</ul>
<p>常见的loss：</p>
<ul>
<li>chamfer loss（点集之间）；normal loss（点集之间）</li>
<li>edge regularizer；Laplacian regularizer（smoothness）；boundary regularizer</li>
</ul>
<h2 id="3D-R2N2（2016-4）"><a href="#3D-R2N2（2016-4）" class="headerlink" title="3D-R2N2（2016.4）"></a>3D-R2N2（2016.4）</h2><blockquote>
<p>ECCV2016《3D-R2N2: A Unified Approach for Single and Multi-view 3D Object Reconstruction》</p>
<p>paper下载：<a href="https://arxiv.org/abs/1604.00449" target="_blank" rel="noopener">https://arxiv.org/abs/1604.00449</a></p>
</blockquote>
<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>提出3D-R2N2网络，能够输入一张/多张图片并重建出3D occupancy grid，该方法在训练/测试阶段不需要任何的图像标注和物体类别标签（但需要bounding box）。在某些情形中使用传统SFM/SLAM方法失效而本方法却能够重建出来（因为这些情形缺少纹理，或者摄像机的拍摄视角变化大）。3D-R2N2是自动学习的、end-to-end的。性能超越了SOTA。</p>
<p>创新点：使用RNN来实现允许单图片/多图片的输入</p>
<p>注：</p>
<ul>
<li>3D-R2N2 == 3D Recurrent Reconstruction Neural Network</li>
<li><p>先前方法的要求条件（因为SFM/SLAM这类方法假设了features can be matched across views）：</p>
<ul>
<li><p>观察的视角密集，即相机的位置变化比较小</p>
</li>
<li><p>表面是Lambertian reflectance，即均匀的漫反射</p>
</li>
<li><p>表面的纹理丰富，不单一</p>
</li>
</ul>
</li>
<li>所使用的数据集：ShapeNet、PASCAL 3D、Online Products、MVS CAD Models</li>
<li>所对比的模型：Kar et al.（《Category-Specific Object Reconstruction from a Single Image》）、Multi View Stereo method（MVS）</li>
</ul>
<h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p><img src="1_1.png" alt></p>
<p>利用RNN的“可接受任意长的序列输入”的性质，来达到单视图/多视图输入的统一。</p>
<p>首先将图片利用2D conv编码成1024维的特征向量。然后丢到3D Convolutional LSTM中，输出是$4\times 4\times 4$大小的voxels（4D tensor），每个voxel位置上都是一个vector（其实是排列成$4\times 4\times 4$的64个LSTM单元，然后取hidden state作为每个voxel的特征向量）。然后将4D tensor丢到3D conv中上采样，得到$32\times 32\times 32$的3D occupancy grid（每个voxel位置上是这个voxel取到的概率，所以需要一个threshold来确定最终的输出）。</p>
<p>随着输入图片变多，得到的结果也逐渐变得精细。</p>
<p>视觉上解释LSTM带来的优点：LSTM的遗忘门/输入门机制对应于自动纠错错误重建的部分/自动重建未看见的部分（有效的处理物体遮挡问题）</p>
<h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><p>分为三部分：2D Convolutional Neural Network (2D-CNN), a novel architecture named 3D Convolutional LSTM (3D-LSTM), and a 3D Deconvolutional Neural Network (3D-DCNN)</p>
<ol>
<li><p>Encoder: 2D-CNN</p>
<p>可以使用普通的浅CNN（上面那个），或者是使用残差结构的深CNN（下面那个）。实验表明残差结构更好。</p>
<p><img src="1_2.png" alt></p>
</li>
<li><p>Recurrence: 3D Convolutional LSTM</p>
<p><img src="1_3.png" alt></p>
<p>在实施例中，有64个不同的LSTM单元，排列成$4\times 4\times 4$的形状。经过某种计算之后，我们取每个LSTM单元的$N_h$维的hidden state作为输出，于是得到$4\times 4\times 4 \times N_h$的4D tensor输出。计算过程如下：</p>
<p><img src="1_4.png" alt></p>
<p>其中$f_t$是遗忘门，$i_t$是输入门，$s_t$是memory cell，$h_t$是hidden state。</p>
<p>先考虑在一个位置上的计算：$U*h_{t-1}$表示周围邻居（红色）的$t-1$时刻的hidden state的线性加权和$\sum_{k \in \mathcal{N}_{index}} u_k h_{t-1}[k]$，$\mathcal{T(x_t)}$表示$t$时刻的1024维向量输入。那么输出$h_t$就是一个向量。</p>
<p>如果考虑全部64个LSTM单元，那么输出$h_t$就是一个4D tensor。</p>
<p>作者在这里没有使用outputs gate，节省了参数量。这里的kernel size是3。</p>
<p>当然也有基于GRU（Gated Recurrent Unit）的实现方法，实验表明GRU效果好于LSTM，kernel size为3时好于为1时。</p>
</li>
<li><p>Decoder: 3D Deconvolutional Neural Network</p>
<p>可以用普通的3D卷积，也可以用残差结构的3D卷积。</p>
<p>最后将$32\times 32\times\times 32 \times 2$的输出套用softmax，得到voxel-wise的概率值（尺寸$32\times 32\times \times 32$）。然后用threshold截断概率便得到最后的结果。</p>
<p>损失函数是the sum of voxel-wise cross-entropy</p>
</li>
<li><p>训练和效果</p>
<ul>
<li>Data augmentation： augmented the input images with random crops、tinted the color、randomly translated the images、all viewpoints were sampled randomly</li>
<li>Training： variable length inputs ranging from one image to an arbitrary number of images</li>
<li>Metrics： voxel Intersection-over-Union (IoU)、cross-entropy loss</li>
</ul>
</li>
</ol>
<h2 id="PSG（2016-12）"><a href="#PSG（2016-12）" class="headerlink" title="PSG（2016.12）"></a>PSG（2016.12）</h2><blockquote>
<p>CVPR2017《A Point Set Generation Network for 3D Object Reconstruction from a Single Image》</p>
<p>paper下载：<a href="http://openaccess.thecvf.com/content_cvpr_2017/html/Fan_A_Point_Set_CVPR_2017_paper.html" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2017/html/Fan_A_Point_Set_CVPR_2017_paper.html</a></p>
</blockquote>
<h3 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h3><p>作者是第一个用深度学习重建点云的人。输入单张图片，输出点云的3D坐标。他们构造的模型是“conditional shape sampler”（即有个输入变量$r$可以控制生成的多样性，用于描述重建的不确定性），可以输出多种可能的点云（取决于$r$）。单图输入的情况下性能超越SOTA（与3D-R2N2对比）。也可以用于3D补全，以及多可能的输出。</p>
<p>似乎没深究如何生成点云以达到无序性输出，文中好像是在fc/conv这样的结构化排列中输出的？</p>
<p>难点：</p>
<ul>
<li>如何输出点云</li>
<li>仅靠输入的单张图片难以直接确定3D形状（即inherent ambiguity in groundtruth）</li>
</ul>
<p>特点：</p>
<ul>
<li>引入随机变量$r$可以从输入操控生成点云的多样性</li>
</ul>
<p>作者的Discussion：</p>
<ul>
<li>First, how to generate an orderless set of entities. Towards building generative models for more sophisticated combinatorial data structures such as graphs, knowing how to generate a set may be a good starting point.</li>
<li>Second, how to capture the ambiguity of the groundtruth in a regression problem. Other than 3D reconstruction, many regression problems may have such inherent ambiguity. Our construction of the MoN loss by wrapping existing loss functions may be generalizable to these problems.</li>
</ul>
<p>TODO IDEA：能否不使用“Distance Metric between Point Sets”，转而使用能直接处理点云的网络？能否定向操控$r$各个分量所对应的3D属性？</p>
<h3 id="实现-1"><a href="#实现-1" class="headerlink" title="实现"></a>实现</h3><p>主要由三部分组成：architecture, loss function and learning paradigm</p>
<p>注：architecture的作用是根据图片和随机向量$r$来生成点云；loss function的作用是度量点云相似度；learning paradigm作用是使得随机变量$r$能在网络中起作用，相当于在loss function外部包上了一层“MoN函数”。</p>
<ol>
<li><p>Architecture - Point Set Prediction Network（此网络简记作$\mathbb{G}$）</p>
<p><img src="2_1.png" alt></p>
<p>作者设计了3种网络的结构：vanilla version、two prediction branch version和hourglass version</p>
<p>每种网络都有encoder stage和predictor stage。encoder stage用于将图片$I$和输入的随机向量$r$映射到embedding space；而predictor则输出一个$N \times 3$大小的矩阵$M$（$N$是重建出的点云的点数）。A random vector $r$ is subsumed so that it perturbs the prediction from the image $I$（随机向量$r$的用法在“Generation of Multiple Plausible Shapes”会讲到）</p>
<ul>
<li>vanilla version：最原始的版本，细节见图（r.v.就是随机向量$r$）。Though simple, this version works reasonably well in practice.</li>
<li>two prediction branch version：目的是better accommodate large and smooth surfaces which are common in natural objects。它的predictor有两个并行的分支：a fully-connected (fc) branch and a deconvolution (deconv) branch。fc branch预测$N_1=256$个点（fc输出256*3个nodes），deconv branch预测输出$H \times W=24\times 32$个点（排列成$24\times 32$然后一个像素对应一个3D的点，是3通道的）。那么总共输出$N = 24 \times 32 + 256=1024$个点（不知道如何处理无序性呢？？）。Their predictions are later merged together to form the whole set of points in $M$. Multiple skip links are added to boost information flow across encoder and predictor。fc branch功能：high flexibility、可describe intricate structures。deconvolution branch功能：节省参数、more friendly to large smooth surfaces, due to the spatial continuity induced by deconv and conv。</li>
<li>hourglass version：This deep network conducts the encoding-decoding operations recurrently, thus has stronger representation power and can mix global and local information better.</li>
</ul>
</li>
<li><p>Loss function - Distance Metric between Point Sets</p>
<p>对Loss的要求：可微、高效计算、对异常点鲁棒</p>
<p>那么需要找到一种衡量两个点云$S_i^{pred}$和$S_i^{gt}$相似度的距离函数$d(S_i^{pred}，S_i^{gt})$，那么损失函数就是$L = \sum_i d(S_i^{pred}，S_i^{gt})$（其中$i$是training samples的index。这个损失函数还不是最终的损失函数，不能实际应用）</p>
<p>作者建议使用Chamfer distance（CD）或Earth Mover’s distance（EMD）</p>
<p><img src="2_2.png" alt></p>
<p>Facing the inherent inability to resolve the shape precisely, neural networks tend to predict a “mean” shape averaging out the space of uncertainty. The mean shape carries the characteristics of the distance itself.</p>
</li>
<li><p>Learning paradigm - Generation of Multiple Plausible Shapes</p>
<p>The ambiguity of groundtruth shape may significantly affect the trained predictor, as the loss function induces our model to predict the mean of possible shapes. So We expect that the random<br>variable $r$ passed to “Point Set Prediction Network” would help it explore the groundtruth distribution. 如果跳过这步不做的话，就会导致the loss minimization will nullify the randomness.</p>
<p>我们可以使用最小化下面的损失函数（称作Min-of-N loss （MoN））的方法来解决：</p>
<p><img src="2_3.png" alt></p>
<p>意思是尝试$n $个random variable的取值，然后取最小距离作为距离值。（$n=2$足矣）</p>
<p>另一种方法是使用Conditional VAE（这样好像$\mathbb{G}$里就不用$r$了），详情略。</p>
<p>似乎也能用Conditional GAN，但是作者没深究。</p>
</li>
</ol>
<h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>用ShapeNet dataset，从CAD模型中渲染出图片来</p>
<ol>
<li><p>3D Shape Reconstruction from RGB Images</p>
<p>与3D-R2N2作比较。将3D-R2N2输出的体数据用最远点采样获得点数据，然后计算CD、EMD值；将本模型输出的点数据经过后期处理得到体数据，然后计算IoU值。</p>
<p>实验表明本方法在单张图片输入时，在所有种类上精度远超3D-R2N2。同时大部分种类的精度远超5张图片输入的3D-R2N2。而且本方法不存在“薄弱细小结构”不能重建的缺点。</p>
</li>
<li><p>3D Shape Completion from RGBD Images</p>
<p>当是RGBD输入时，还可以起到3D补全的作用。</p>
<p><img src="2_4.png" alt></p>
</li>
<li><p>Predicting Multiple Plausible Shapes</p>
<p>改变$r$值可以产生多种输出。</p>
<p><img src="2_5.png" alt></p>
</li>
<li><p>Network Design Analysis</p>
<ul>
<li><p>Effect of combining deconv and fc branches for reconstruction</p>
<ul>
<li><p>deconv的引入提升了精度；Stacking another hourglass level也提升了精度</p>
</li>
<li><p>In the deconv branch the network learns to use the convolution structure to constructs a 2D surface that warps around the object. In the fully connected branch the output is less organized as the channels are not ordered.</p>
</li>
<li><p>The deconv branch is in general good at capturing the “main body” of the object, while the fully connected branch complements the shape with more detailed components</p>
<p><img src="2_6.png" alt></p>
</li>
</ul>
</li>
<li><p>Analysis of distance metrics</p>
<p>The network trained by CD tends to scatter a few points in its uncertain area (e.g. behind the door) but is able to better preserve the detailed shape of the grip. In contrast, the network trained by EMD produces more compact results but sometimes overly shrinks local structures. This is in line with experiment on synthetic data.</p>
</li>
</ul>
</li>
<li><p>More results and application to real world data</p>
<p>也能用于现实生活中照片的3D重建，但需要从背景分割出物体来</p>
</li>
<li><p>Analysis of human ability for single view 3D reconstruction</p>
<p>某些图片上超越人类</p>
</li>
<li><p>Analysis of failure cases</p>
<p>仅仅是主观分析，略</p>
</li>
</ol>
<h2 id="OGN-（2017-3）👍"><a href="#OGN-（2017-3）👍" class="headerlink" title="OGN （2017.3）👍"></a>OGN （2017.3）👍</h2><blockquote>
<p>ICCV2017《Octree Generating Networks: Efficient Convolutional Architectures for High-resolution 3D Outputs》</p>
<p>paper下载：<a href="http://openaccess.thecvf.com/content_iccv_2017/html/Tatarchenko_Octree_Generating_Networks_ICCV_2017_paper.html" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_iccv_2017/html/Tatarchenko_Octree_Generating_Networks_ICCV_2017_paper.html</a></p>
</blockquote>
<h3 id="概述-2"><a href="#概述-2" class="headerlink" title="概述"></a>概述</h3><p>使用八叉树（octree）的表示法，生成高分辨率的3D体素数据。本文中的OGN是deep convolutional decoder（即3D体素生成器）。近似平方的复杂度（而不是立方复杂度），大大节省了内存和计算量消耗。低分辨率时与普通的voxel方法精度差不多，而且它还能生成高分辨率体素，甚至可以生成$512^3$这么大的分辨率。可以用在多种场合，如3D convolutional autoencoders、reconstruction from high-level representations or a single image</p>
<p><img src="3_3.png" alt></p>
<p>注意：</p>
<ul>
<li>使用octree意味着可以使用不同大小的cell size，不断细分大的cell来达到高分辨率。即不断精细化体素来达到高分辨率</li>
<li>OGN是在八叉树上操作。用binary occupancy maps来表示生成的形状</li>
<li>同时OGN也是灵活的，即可以任意指定层数和层的配置</li>
<li>OGN是end-to-end的，可用反向传播计算</li>
</ul>
<h3 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h3><p><img src="3_1.png" alt></p>
<ul>
<li><p>最左边的“dense block”由普通的3D卷积层组成，输出$d_1 \times d_2 \times d_3 \times c$大小的4D tensor。</p>
</li>
<li><p>然后这个4D tensor被转换为键值对（index-value pairs）存储在hash table中，value就是一个voxel内的特征向量。</p>
</li>
<li><p>然后接下来的octree block就会预测octree新生成的结构、和相应生成的内容（特征向量）。</p>
</li>
<li><p>多次通过octree block处理就会生成高精度的体素表示。</p>
</li>
</ul>
<h3 id="实现-2"><a href="#实现-2" class="headerlink" title="实现"></a>实现</h3><ol>
<li><p>Octree编码法</p>
<p>作用：将体素空间用octree表示，那么对voxel grid的操作就可以转换为对octree的操作</p>
<p>基于哈希表的Octree编码（可以实现常数时间的元素获取）：</p>
<ul>
<li><p>Octree cell的空间坐标$\mathtt{x}=(x,y,z)$，其在树中的层级（分辨率等级）为$l$，其内容为$v$。</p>
</li>
<li><p>将其转换为索引$m=\mathcal{Z}(\mathtt{x},l)$，其中$\mathcal{Z}(\cdot)$是Z-order curve。</p>
</li>
<li><p>那么就形成了键值对$(m,v)$。于是八叉树就是$O=\{(m,v)\}$</p>
</li>
</ul>
<p>那么数据操作和更新都在hash table中进行。</p>
<p>查值函数（从八叉树$O$中获取值）：</p>
<p><img src="3_2.png" alt></p>
</li>
<li><p>Octree Generating Networks</p>
<p>binary occupancy values $v=\{0,1\}$</p>
<p>由普通3D卷积生成$d_1 \times d_2 \times d_3 \times c$大小的4D tensor，然后被转换成octree，并将数据存储在hash table中。</p>
<p><img src="3_4.png" alt></p>
<p>图中为了示意的简便起见，用2D voxel替代3D voxel来展示。</p>
<p>propagated features是指需要处理或细分的网格特征</p>
<p>empty是指不是实体的网格</p>
<p>filled是指被占据、是实体的网格</p>
<p>mixed是指需要细分的网格（与GT相比既有empty又有filled）</p>
<ul>
<li><p>OGN-Conv</p>
<p>OGN-Conv需要处理由哈希表表示的特征图。OGN-Conv支持步长卷积和上采样卷积。</p>
<p>基本原理（类似于“im2col”和“col2im”函数）：将hash table转变为feature matrix，然后与权重矩阵相乘，再复原回hash table</p>
</li>
<li><p>OGN-Loss</p>
<p>predictions就是判断voxel是empty/filled/mixed中的哪种，是3分类问题。用$1^3$卷积和softmax实现即可。</p>
<p>最小化predictions与the cell state of the ground truth的交叉熵：</p>
<p><img src="3_5.png" alt></p>
<p>$p_m^i$是输出概率值，$M_l$是第$l$层的叶子集合。那么总目标函数就是全部层的$\mathcal{L_l}$的求和。</p>
</li>
<li><p>OGN-Prop</p>
<p>将预测出来是“mixed”的取出来作为输出。可能还要取出一些其他的邻居，以便用于接下来的卷积计算。</p>
<p>根据在测试阶段是否知道Octree的GT，传播方式分为：Prop-Known方法（例如语义分割，结构不变，只需要与GT做对比即可）和Prop-Pred方法（例如三维重建，结构需要你预测，需要训练分类模型来判断每个voxel是empty/filled/mixed中的哪种）。不详述。</p>
</li>
</ul>
</li>
</ol>
<h2 id="MarrNet（2017-11）"><a href="#MarrNet（2017-11）" class="headerlink" title="MarrNet（2017.11）"></a>MarrNet（2017.11）</h2><blockquote>
<p>NIPS2017《MarrNet: 3D Shape Reconstruction via 2.5D Sketches》</p>
<p>paper下载：<a href="http://papers.nips.cc/paper/6657-marrnet-3d-shape-reconstruction-via-25d-sketches" target="_blank" rel="noopener">http://papers.nips.cc/paper/6657-marrnet-3d-shape-reconstruction-via-25d-sketches</a></p>
</blockquote>
<h3 id="概述-3"><a href="#概述-3" class="headerlink" title="概述"></a>概述</h3><p>从单张图片重构出3D voxel-based reconstruction。从RGB图得到2.5D表示，再转换成3D形状（two-step）。MarrNet可以在synthetic data数据上训练然后在real data上进行self-supervised的fine-tune（一定程度上解决了domain adaptation问题）。MarrNet是end-to-end的可训练模型，达到了SOTA。</p>
<p>2D → 2.5D → 3D的优点：</p>
<ul>
<li>First, compared to full 3D shape, 2.5D sketches are much easier to be recovered from a 2D image; models that recover 2.5D sketches are also more likely to transfer from synthetic to real data.</li>
<li>Second, for 3D reconstruction from 2.5D sketches, systems can learn purely from synthetic data.</li>
<li>Third, we derive differentiable projective functions from 3D shape to 2.5D sketches; the framework is therefore end-to-end trainable on real images, requiring no human annotations</li>
</ul>
<p>注：</p>
<ul>
<li>取名MarrNet是因为与David Marr’s theory of perception相近</li>
<li>intrinsic images（描述物体内在固有属性的可分离的本质图像）：例如depth、surface normals、silhouette等等</li>
<li>Reprojection Consistency只在fine-tune中使用；预训练使用与GT voxel gird比较的交叉熵。</li>
<li>本文定性分析（图片展示）比较多，定量分析比较少（只有人类肉眼比较结果和一个IoU结论）</li>
<li>我感觉本文的ablation study说服力不足，即不能说明fine-tune是有效的，并不能说明Reprojection Consistency的设计是起到实际作用的。详见实验的第二部分。</li>
</ul>
<h3 id="原理-2"><a href="#原理-2" class="headerlink" title="原理"></a>原理</h3><p><img src="4_1.png" alt></p>
<p>用2D CNN的方法生成2.5D图，然后将2.5D图用encoder-decoder方法生成体素表示。用reprojection consistency loss确保3D与2.5D匹配。</p>
<h3 id="实现-3"><a href="#实现-3" class="headerlink" title="实现"></a>实现</h3><ol>
<li><p>2.5D Sketch Estimation</p>
<p>用encoder-decoder结构生成2.5D表示。用ResNet-18编码，将256x256的RGB转变为8x8x256的特征图。然后用decoder恢复成256x256的depth, surface normal, and silhouette images</p>
</li>
<li><p>3D Shape Estimation</p>
<p>用encoder-decoder结构生成3D体素。输入depth和normal图（施加以silhouette images掩膜），然后用卷积转变为200维向量，然后用3D卷积生成128x128x128的体素表示。</p>
</li>
<li><p>Reprojection Consistency</p>
<p>使得3D表示与2.5D表示相一致，即将2.5D投射到3D空间中，体素也需要具有与2.5D的描述相一致的性质。reprojection consistency loss分为depth reprojection loss和surface normal reprojection loss。并且reprojection consistency loss对voxel可求导，使得可以用反向传播计算和优化。</p>
<p>记3D voxel grid在$(x,y,z)$处存在该voxel的概率为$v_{x,y,z}\in[0,1]$，深度图在$(x,y)$处的值为$d_{x,y}$，法向图在$(x,y)$处的法向为$n_{x,y}=(n_a,n_b,n_c)$。并且假设是<strong>正交投影</strong>（注意这里不是相机成像的投影模型）</p>
<p><img src="4_3.png" alt></p>
<ul>
<li><p>depth reprojection loss的目标是：将depth image的point投影到3D中，使得这个3D point恰好出现在3D voxel reconstruction中，并且该点的视线前方无遮挡。</p>
<p><img src="4_2.png" alt></p>
</li>
<li><p>surface normal reprojection的目标是：使得该点在3D切平面上的邻居存在</p>
<p>已知法向$n_{x,y}=(n_a,b_b,n_c)$必定与$n_x’=(0,-1,n_b/n_c)$、$n_y’=(-1,0,n_a/n_c)$垂直。那么voxel在$(x,y,z)\pm n_x’$和$(x,y,z)\pm n_y’$的值必定为1（当这些点在silhouette image以内时才算）。</p>
<p><img src="4_4.png" alt></p>
</li>
</ul>
</li>
<li><p>Training paradigm</p>
<p>将“2.5D sketch estimation”和“3D shape estimation”分别在synthetic images上预训练：The 3D interpreter is trained using ground truth voxels and a <strong>cross-entropy loss</strong>（注意预训练是用与GT比较的交叉熵！）</p>
<p>然后再在真实图片上fine-tune：The <strong>reprojection consistency loss</strong> is used to fine-tune the 3D estimation component of our model on real images, using the predicted normal, depth, and silhouette。固定decoder of the 3D estimator（因为它包含了3D形状的先验，不希望改变），只fine-tune the encoder（学习映射到decoder合理的特征空间中）。这种fine-tune类似自监督，可以在无任何标注的单张测试图片上进行fine-tune。</p>
</li>
</ol>
<h3 id="实验-1"><a href="#实验-1" class="headerlink" title="实验"></a>实验</h3><ol>
<li><p>3D Reconstruction on ShapeNet</p>
<p>生成的形状具有smoother surfaces and finer details</p>
<p>Our full model achieves a higher IoU (0.57) than the direct prediction baseline (0.52).</p>
<p>direct prediction baseline：不经过2.5D表示，直接基于3D卷积的体素生成</p>
</li>
<li><p>3D Reconstruction on Pascal 3D+</p>
<p>现在ShapeNet上预训练，然后fine-tune them on the PASCAL 3D+ dataset</p>
<p><img src="4_5.png" alt></p>
<p>上图的ablation study表明，需要固定decoder of the 3D estimator进行fine-tune才能取得更好的fine-tune效果。</p>
<p>但感觉本文的ablation study说服力不足，文中说The model trained on synthetic data provides a reasonable shape estimate和Our final model, fine-tuned with the decoder fixed, keeps the shape prior and provides more details of the shape，这只是作者自己的观点，并没有数据支撑，并且“human studies”中并没有fine-tune前后的“preferences”对比，并不能说明fine-tune是有效的，即并不能说明Reprojection Consistency的设计是有作用的。</p>
<p>作者认为IoU指标不好，不能描述精细结构等，提出“human studies”，即人类肉眼比较MarrNet和竞争对手DRC哪个效果比较好。效果当然是说MarrNet比较好啦。</p>
<p><img src="4_6.png" alt></p>
<p>MarrNet不能复原复杂的、细弱的结构，而且silhouette mask不能精确估计。</p>
</li>
<li><p>3D Reconstruction on IKEA</p>
<p>IKEA furniture数据集，our model can deal with mild occlusions in real life scenarios</p>
</li>
<li><p>Extensions</p>
<p>We further train MarrNet jointly on all three object categories, and our model successfully recovers shapes of different categories</p>
</li>
</ol>
<h2 id="LSM（2017-8）"><a href="#LSM（2017-8）" class="headerlink" title="LSM（2017.8）"></a>LSM（2017.8）</h2><blockquote>
<p>NIPS2017《Learning a Multi-View Stereo Machine》</p>
<p>paper下载：<a href="http://papers.nips.cc/paper/6640-learning-a-multi-view-stereo-machine" target="_blank" rel="noopener">http://papers.nips.cc/paper/6640-learning-a-multi-view-stereo-machine</a></p>
</blockquote>
<h3 id="概述-4"><a href="#概述-4" class="headerlink" title="概述"></a>概述</h3><p>利用n幅图像（n≥1）以及这些摄像机的内外参（camera poses），求出物体的体素重建（Voxel LSM）或相应的n幅深度图（Depth LSM）。LSM端到端可微分。核心思想：利用投射关系将2D排列的特征与3D排列的特征相互转换，且能此过程可微分，使其利用了空间投影关系的几何先验。</p>
<p>注：</p>
<ul>
<li>多视角立体视觉（Multi-view stereopsis，简称MVS）：给定物体的多幅图像、这些图像所分别拍摄的摄像机的姿态（内参和外参），试求物体的几何表示（3D结构/深度图等）</li>
<li>LSM == Learnt Stereo Machine</li>
<li>our system is able to better use camera pose information leading to significantly large improvements while adding more views（然而对比的网路（3D-R2N2）是add pose information in a fully connected manner，并没有专门对此设计过）</li>
<li>可能作者调参和优化了很久。。。</li>
</ul>
<p>TODO IDEA：将几何投影等关系编码且使得可微分，使得可以反向传播。</p>
<h3 id="实现-4"><a href="#实现-4" class="headerlink" title="实现"></a>实现</h3><p><img src="5_1.png" alt></p>
<p>简述：输入n幅图像，然后经过2D CNN特征提取出2D的特征图。再分别使用“unprojection”操作得到3D排列的体特征$\mathcal{G}^f_i$。然后使用RNN将其全部整合成一个体特征$\mathcal{G}^p$。再将其用3D卷积得到$\mathcal{G}^o$。如果想得到体素表示，则直接使用3D卷积生成即可；如果想得到相应的深度图，则使用“projection”操作转换为2D特征图之后再使用2D卷积上采样即可。</p>
<ol>
<li><p>2D Image Encoder</p>
<p>用UNet生成特征图。从图像$\{I_i\}^n_{i=1}$转变为特征图$\{\mathcal{F}_i\}^n_{i=1}$</p>
</li>
<li><p>Differentiable Unprojection</p>
<p>输入特征图和相机姿态，输出3D排列的特征。</p>
<p>即从特征图$\{\mathcal{F}_i\}^n_{i=1}$转变为3D gird$\{\mathcal{G}_i^f\}^n_{i=1}$</p>
<p><img src="5_2.png" alt></p>
<p>上图只是方便图示而已，实际上是3D gird和2D feature map。</p>
<p>实际实现过程：将3D gird center投影到image plane得到连续的坐标，然后根据discrete grid的feature value用bilinear sampling插值得到该3D gird的feature</p>
<p>为了可以支持单张图片输入，我们还往每个3D gird中添加几何特征（例如depth value和ray direction）</p>
<p>这个过程是可微分的，使得可以端到端训练。</p>
</li>
<li><p>Recurrent Grid Fusion</p>
<p>从多个3D gird$\{\mathcal{G}_i^f\}^n_{i=1}$到单个$\mathcal{G^p} $</p>
<p>使用3D convolutional variant of the Gated Recurrent Unit (GRU)，类似于3D-R2N2中的做法。</p>
<p>训练时随机打乱输入的顺序，减小输入顺序带来的影响。</p>
</li>
<li><p>3D Grid Reasoning</p>
<p>将$\mathcal{G^p} $转变为$\mathcal{G^o} $</p>
<p>使用3D UNet。目的（感觉乱说。。）：use shape cues present in $\mathcal{G^p} $ such as feature matches and silhouettes as well as build in shape priors like smoothness and symmetries and knowledge about object classes enabling it to produce complete shapes even when only partial information is visible</p>
</li>
<li><p>Differentiable Projection</p>
<p>从$\mathcal{G^o}$转变为多个2D feature map$\{\mathcal{O}_i\}^n_{i=1}$</p>
<p>输入：$\mathcal{G^o}$和相机姿态。输出：2D feature map$\mathcal{O}$</p>
<p><img src="5_3.png" alt></p>
<p>上图只是方便图示而已，实际上是3D gird和2D feature map。</p>
<p>实际实现过程：有多个平行于相机平面的等间距平面（图中的z=1,2,3平面），视线ray是穿过2D feature map的离散格点中心的。由此横穿经过的3D gird的特征值就作为特征值。在3D gird中是nearest neighbor interpolation而不是trilinear interpolation。详情见上图。</p>
<p>最后使用1x1的卷积可以减少channels的数量。</p>
</li>
<li><p>Architecture Details and Experiments</p>
<p>卷积均使用instance normalization和layer normalization</p>
<ul>
<li><p>Voxel LSM (VLSM)：最后使用3D卷积上采样。softmax + binary cross entropy loss</p>
</li>
<li><p>Depth LSM (D-LSM)：使用Projection将$\mathcal{G^o}$转变为多个2D feature map$\{\mathcal{O}_i\}^n_{i=1}$，然后分别进行1x1卷积和deconvolution上采样，其中deconvolution有skip connections来自之前的image encoder。</p>
</li>
</ul>
<p>Experiments性能：</p>
<p><img src="5_4.png" alt></p>
</li>
</ol>
<h2 id="IM-NET（2018-12）👍"><a href="#IM-NET（2018-12）👍" class="headerlink" title="IM-NET（2018.12）👍"></a>IM-NET（2018.12）👍</h2><blockquote>
<p>CVPR2019《Learning Implicit Fields for Generative Shape Modeling》</p>
<p>paper下载：<a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Learning_Implicit_Fields_for_Generative_Shape_Modeling_CVPR_2019_paper.html" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Learning_Implicit_Fields_for_Generative_Shape_Modeling_CVPR_2019_paper.html</a></p>
</blockquote>
<h3 id="概述-5"><a href="#概述-5" class="headerlink" title="概述"></a>概述</h3><p>使用隐式场的方法来生成3D点云，后处理后的形成的mesh具有superior visual quality（higher surface quality）。并在插值时表现出良好的过度性质。所使用的implicit field decoder称作IM-NET，可以用于3D形状自编码（即shape representation learning）（IM-AE）、形状生成（IM-GAN）、形状插值、单视图3D重建等。其中encoder不属于本文设计范畴，只要能得到shape feature的都可以。</p>
<p>注：</p>
<ul>
<li>Our work is the first to introduce a deep network for learning implicit fields for generative shape modeling</li>
<li>缺点：训练时间长（因为the decoder needs to be applied on each point in the training set），或许可以考虑只生成物体表面的点来提高速度</li>
</ul>
<p>TODO IDEA：后期可以考虑用decoder来生成其他的属性，例如颜色、纹理等。或许还可以用来做part segmentation</p>
<h3 id="原理-3"><a href="#原理-3" class="headerlink" title="原理"></a>原理</h3><p>decoder的关键是怎么根据shape feature得到出3D数据（点云）。这通过判断给定点$p$是处于shape的内部还是外部来实现的，那么中间的分界等值面就是shape的边界。</p>
<p><img src="6_1.png" alt></p>
<p>构造一个MLP，输入是shape feature和point coordinate，输出是判断内外部的二分类（sigmoid函数）。</p>
<p><img src="6_2.png" alt></p>
<p>其中输入的point coordinate对空间的均匀采样得到的，因而输出形状的分辨率可以任意高。</p>
<p>The skip connections（copy and concatenate）可以使训练稳定、加快训练。</p>
<h3 id="实现-5"><a href="#实现-5" class="headerlink" title="实现"></a>实现</h3><ol>
<li><p>用decoder $f_\theta(p)$为每个点$p$计算“是内部点的概率”（$f_\theta(p):[0,1]^3 \rightarrow [0,1]$），然后计算等值面（如3D形状则使用Marching Cubes方法，2D图像则使用applying thresholding方法），从而得到mesh表示。对于闭合形状，GT为$\mathcal{F}(p)$内部取1外部取0。</p>
</li>
<li><p>decoder训练方法（可采取的方法有两种）（作者的实现方式是一个decoder只对应一个物体种类）：</p>
<ul>
<li>A naive sampling：将训练形状的空间离散化（voxelize or rasterize），然后均匀采样。多分辨率采样依次得到$16^3,32^3,64^3,128^3$个点。先用低分辨率训练再逐渐用高分辨率训练（train the model progressively）。大致是立方复杂度。</li>
<li>A more efficient approach：采样更多的靠近表面的点，对离表面比较远的点不采样。然后对采样的每个点都有个权重$w_p$，是该点采样密度的倒数，用以补偿采样密度的变化。大致是平方复杂度。</li>
</ul>
</li>
<li><p>损失函数（$S$是采样得到的点集）：</p>
</li>
</ol>
<script type="math/tex; mode=display">
\mathcal{L}(\theta)=\frac{\sum_{p \in S}|f_\theta(p)-\mathcal{F}(p)|^2 \cdot w_p}{\sum_{p \in S} w_p}</script><ol>
<li><p>IM-NET的应用举例</p>
<ul>
<li><p>Auto-Encoding：使用3D卷积从$64^3$的voxels中提取128维特征，然后decoder使用IM-NET。使用progressive training。</p>
</li>
<li><p>3D shape generation：使用autoencoder中训练好的encoder的输出作为latent-GAN的输入</p>
</li>
<li><p>single-view 3D reconstruction（SVR）：用ResNet将$128\times128$的图像编码成128维的特征向量。使用autoencoder中训练好的decoder，固定其参数不变，只训练ResNet的参数（encoder）去最小化mean squared loss</p>
</li>
</ul>
</li>
</ol>
<h3 id="实验-2"><a href="#实验-2" class="headerlink" title="实验"></a>实验</h3><ol>
<li><p>获取point cloud之后，转换为mesh（如Marching Cubes方法），最后用Poisson-disk Sampling获取10000个表面的点。</p>
</li>
<li><p>作者觉得chamfer distance（CD）、mean squared error（MSE）、IoU并不能很好的描述物体表面的视觉性质（例如桌子面发生一点点上下的移动将剧烈地改变IoU的值，但是视觉上的效果却差不多；并且IoU指标并不抵制表面的凹凸起伏），提出使用计算机图形领域的light field descriptor（LFD）指标。以及coverage score（COV-LFD）、Minimum Matching Distance（MMD-LFD）指标。</p>
</li>
<li><p>插值效果</p>
<p><img src="6_3.png" alt></p>
<p>优点：cleaner surface boundaries、smooth part movements、handles topology changes</p>
<p>但是不知道如何控制这种变化的过程</p>
</li>
</ol>
<h2 id="DISN（2019-5）👍"><a href="#DISN（2019-5）👍" class="headerlink" title="DISN（2019.5）👍"></a>DISN（2019.5）👍</h2><blockquote>
<p>NIPS2019《DISN: Deep Implicit Surface Network for High-quality Single-view 3D Reconstruction》</p>
<p>paper下载：<a href="https://arxiv.org/abs/1905.10711" target="_blank" rel="noopener">https://arxiv.org/abs/1905.10711</a></p>
</blockquote>
<h3 id="概述-6"><a href="#概述-6" class="headerlink" title="概述"></a>概述</h3><p>提出了一种单视图重建的网络，能生成高质量的3D形状。核心是预测“signed distance field”。不仅结合了“global image features”，还结合了“local features from the patch”，因而能够生成更多细节的高质量3D形状（例如细小结构、孔洞）。在单视图重建方面达到了SOTA。但是只能用于图片背景干净的输入。</p>
<p>注：</p>
<ul>
<li>Signed Distance Functions（SDF）是属于隐式的surface表示；传统的方法是显式的surface表示（例如mesh）</li>
<li>DISN == Deep Implicit Surface Network</li>
<li>原始生成的3D形状是点云，然后用Marching Cubes方法来确定iso-surface（以获得3D mesh）</li>
<li>缺点：要生成很多3D points才能确定等值面（最后需要从dense 3D grid中采样格点作为输入）。</li>
<li>优点：生成的分辨率可以任意高</li>
<li>还可以扩展到multi-view reconstruction和shape interpolation应用中</li>
<li>实现细粒度可能的原因是Local Feature是从images中的对应区域抠特征子图实现的。Local Feature预测出来的SDF值可以认为是总的SDF值的残差，残差对应“细粒度/细节”。</li>
</ul>
<p>TODO IDEA：</p>
<ul>
<li>IDEA1：生成空间分布，然后在分布中采样得到物体表面的点。</li>
<li>IDEA2：使用无符号的Distance Functions（记作$f(p)$），然后随机初始化一个3维空间点$p\in \mathbb{R}^3$，通过求解优化问题$min_{p} \frac{\partial f(p)}{\partial p}$，满足此问题的解$p$组成的一系列的点集就是物体的表面。相当于用导数来引导point在surface上的移动。这样理想情况下只需要初始化一个随机点就能使得该点遍历整个surface了</li>
<li>IDEA3：能否借鉴U-Net的思想生成高分辨率的细粒度SDF？</li>
</ul>
<h3 id="原理-4"><a href="#原理-4" class="headerlink" title="原理"></a>原理</h3><p>Signed Distance Functions（SDF）描述了空间某点到物体表面的有符号距离。</p>
<p>在物体外部则为正号，内部则为符号，在物体表面则为零。绝对值是到表面的距离。</p>
<p><img src="7_1.png" alt></p>
<p>然后采样很多个点，通过预测SDF的值，再用寻找等值面的方法来求得表面。</p>
<h3 id="实现-6"><a href="#实现-6" class="headerlink" title="实现"></a>实现</h3><p>首先估计相机姿态，目的是把3D点投射到2D plane上，用以确定2D feature map上local patch的位置。然后将point-wise feature、local feature、global feature联合起来推断SDF的值。</p>
<p>最后采样dense 3D grid，为每个格点预测SDF值，然后用Marching Cubes获取3D mesh。</p>
<p>Local Feature是从images中的对应区域提取的，point-wise feature直接就是MLP处理点坐标，global feature直接就是CNN处理整张图像。</p>
<p>整体结构（图中的“+”号是加法，作者实验表明两路的推断（两个decoder）比一路的推断（只使用一个decoder）效果要好）：</p>
<p><img src="7_2.png" alt></p>
<ol>
<li><p>Camera Pose Estimation</p>
<p><img src="7_3.png" alt></p>
<p>将输入图片用CNN推断出位移量$t$（Translation）和6D旋转表示$b$（Rotation）。由$b$可以用公式计算出旋转矩阵$R$，那么点$p$的空间变换就是$Rp+t$。对aligned model space（即world space）施加空间变换得到“预测的变换点云”，再与GT比较就可以计算出Loss。</p>
<p>Loss的计算方法如下（本质是mean squared error，$PC_w$代表world space中的点云，$p_G$代表camera space中的GT）：</p>
<p><img src="7_4.png" alt></p>
</li>
<li><p>SDF Prediction</p>
<ul>
<li><p>Point-wise feature：目的是将位置向量映射到更高维的空间中去（即图中的“Point feature”）</p>
</li>
<li><p>Global feature：利用CNN推断全局的特征向量</p>
</li>
<li><p>Local Feature：利用估计出来的相机参数将3D点$p$投射到2D image plane的点$q$去，然后将相应位置上的特征子图抠出来再concat，由此得到local feature。（因为特征图尺寸不一样，所以先用双线性插值再抠图）</p>
<p><img src="7_5.png" alt></p>
<p>那么SDF的结果就是两个decoder输出结果的和。可以认为是下面这个decoder分支是一种“residual SDF”，即一种残差预测结构。</p>
</li>
<li><p>Loss Functions：使用加权的损失函数，含义是使得更看重GT表面附近的SDF估计误差（其中$m_1&gt;m_2$）</p>
<p><img src="7_6.png" alt></p>
</li>
</ul>
</li>
</ol>
<h3 id="实验-3"><a href="#实验-3" class="headerlink" title="实验"></a>实验</h3><p>使用ShapeNet，并且train a single network on all categories，使用VGG-16作为CNN的提取特征。以CD、EMD、IoU作为指标。与多种模型（如AtlasNet, Pixel2Mesh, 3DN, OccNet and IMNET）进行比较，取得较好的性能。详情略。</p>
<p><img src="7_7.png" alt></p>
<h2 id="Neural-Renderer（2017-11）"><a href="#Neural-Renderer（2017-11）" class="headerlink" title="Neural Renderer（2017.11）"></a>Neural Renderer（2017.11）</h2><blockquote>
<p>CVPR2018《Neural 3D Mesh Renderer》</p>
<p>paper下载：<a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Kato_Neural_3D_Mesh_CVPR_2018_paper.html" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2018/html/Kato_Neural_3D_Mesh_CVPR_2018_paper.html</a></p>
</blockquote>
<h3 id="概述-7"><a href="#概述-7" class="headerlink" title="概述"></a>概述</h3><p>将Renderer的渲染过程近似可微化，使得渲染器（Renderer）可以嵌入到神经网络结构中来。渲染的对象是3D mesh，生成2D图像。由此作者根据Neural Renderer提出了两种应用：single-image 3D mesh reconstruction with silhouette image supervision和gradient-based 3D mesh editing with 2D supervision（such as 2D-to-3D style transfer and 3D DeepDream），展现了将Renderer集成到神经网络中的巨大威力</p>
<p><img src="8_1.png" alt></p>
<p>难点：如何将rasterization的过程可微分</p>
<p>注：</p>
<ul>
<li><p>polygon mesh的优点：compactness（容易表示、参数少）、geometric properties（suitability for geometric transformations. The rotation, translation, and scaling of objects are represented by simple operations on the vertices.）</p>
</li>
<li><p>渲染过程：Rendering consists of <strong>projecting</strong> the vertices of a mesh onto the screen coordinate system, and generating an image through regular grid <strong>sampling</strong>。后者的过程（rasterization）是离散的、不可微分的，需要我们近似梯度，从而实现反向传播算法</p>
</li>
<li><p>Our proposed renderer can flow gradients into texture, lighting, and cameras as well as object shapes.（mesh + texture + lighting + …  = 2D rendered image）</p>
</li>
<li><p>Polygon mesh表示法：3D顶点$\{v_i^o\}$和面${f_j}$（其中$f_j$是3维向量，内容是三角形面的顶点的索引）</p>
</li>
<li><p>2D-to-3D style transfer：</p>
<script type="math/tex; mode=display">
\begin{align*}\frac{\partial Loss}{\partial Mesh}&=\frac{\partial Loss}{\partial Image}\times\frac{\partial Image}{\partial Mesh}\\&=(the\; gradient\; of\; loss) \times (the\; gradient\; of\;renderer)\end{align*}</script><p>风格迁移：将content mesh的“content”和style image的“style”融合到输出的output mesh上。需要用content loss衡量content的迁移损失、style loss衡量style的迁移损失<br>$Mesh \rightarrow Image$提供了对顶点和面纹理的梯度流，因此可以修改面纹理，将风格迁移到面纹理上。</p>
</li>
<li><p>Renderer使得一些对图像地操作（例如风格迁移）迁移到3D数据结构中成为可能。</p>
</li>
</ul>
<h3 id="实现-7"><a href="#实现-7" class="headerlink" title="实现"></a>实现</h3><ol>
<li><p>Rendering pipeline and its derivative</p>
<p>将3D顶点$\{v_i^o\}$投射到2D平面上得到$\{v_i^s\} $，此过程是可微分的</p>
<p>然后将$\{v_i^s\} $和$\{f_j\}$通过rasterization得到2D rendered image：考虑2D rendered image上的一个pixel点$P_j$（其颜色值$I_j$），如果这个pixel的center位于face $f_i$的内部，则此pixel的颜色值$I_j $被赋值（染色）成$I_{ij}$</p>
<p><img src="8_2.png" alt></p>
<p>图中只考虑$x_i$是可变的，其他暂时冻结住不考虑其变化</p>
<p>我们将突变的过程连续化（线性插值），得到“（d）”所示的效果。我们前向传播使用图“（b）”所示的曲线，反向传播的导数使用“（e）”所示的曲线。</p>
<p>其中颜色突变量$\delta_j^I=I(x_1)-I(x_0)$，位移变化量$\delta_j^x=x_1-x_0$，那么斜线的斜率就是$\frac{\delta_j^I}{\delta_j^x}$</p>
<p>考虑反向传播的误差信号$\delta_j^p = \frac{\partial Loss}{\partial I_j}$，当$\delta_j^p&gt;0$时$I_j$会减少，当$\delta_j^I&gt;0$时$I_j$会增大。为了不使得做无用功，我们要求$\delta_j^I \delta_j^p &lt; 0$才可以更新参数，所以：</p>
<p><img src="8_3.png" alt></p>
<p>类似地，当像素点本身就在face的内部时，则如下图所示：</p>
<p><img src="8_4.png" alt></p>
<p>并且其导数如下所示：</p>
<p><img src="8_5.png" alt></p>
<p>当多个faces时，our rasterizer draws only the frontmost face at each pixel, and do not flow gradients if they are occluded by surfaces not including $v_i$</p>
<p>Texture：位于face$\{v_1,v_2,v_3\}$上的点$p$可以被分解为$p=w_1v_1+w_2v_2+w_3v_3$，那么用$(w_1,w_2,w_3)$就可以表示出face$\{v_1,v_2,v_3\}$上的点。其纹理texture就可以利用$(w_1,w_2,w_3)$从 texture image$s_t\times s_t \times s_t$（each face has its own texture image）中查值获得。不难验证必定满足$w_1 + w_2 + w_3 = 1$</p>
<p>Lighting：考虑环境光$l^a$和定向光源$l^d$，那么pixel color$ I_j $经过光源渲染后的颜色就是$I^l_j = (l^a+(n^d\cdot n^j)l^d)I_j$（$n^j$是face上pixel的单位法向，$n^d$是定向光源的单位方向）</p>
</li>
<li><p>Single image 3D reconstruction</p>
<p><img src="8_6.png" alt></p>
<p>思想：match the ground truth silhouettes。有点像encoder-decoder结构</p>
<p>3D generator idea：deform an isotropic sphere with 642 vertices to generate a new mesh, therefore the mesh we use is specified by 642*3 parameters。然后使用silhouette loss（鼓励IoU越大越好）和smoothness loss（鼓励面面夹角越接近180°越好）训练此generation function</p>
<p>将mask作为附加通道加入到RGB图中</p>
</li>
<li><p>Gradient-based 3D mesh editing</p>
<p><img src="8_7.png" alt></p>
<p>We optimize a 3D mesh consisting of vertices, faces, and textures based on its rendered image</p>
<p>In this section, we propose a method to transfer the style of an image onto a mesh</p>
<p>其中content loss确保3D mseh的shape相同；style loss确保风格相同（图中的Loss指的是style loss）；同时使用了regularizer for noise reduction。总Loss就是所有loss的加权和，对顶点和纹理最小化loss的值</p>
<p>类似地，还可以用于3D DeepDream</p>
</li>
</ol>
<h2 id="Pixel2Mesh（2018-4）"><a href="#Pixel2Mesh（2018-4）" class="headerlink" title="Pixel2Mesh（2018.4）"></a>Pixel2Mesh（2018.4）</h2><blockquote>
<p>ECCV2018《Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images》</p>
<p>paper下载：<a href="http://openaccess.thecvf.com/content_ECCV_2018/html/Nanyang_Wang_Pixel2Mesh_Generating_3D_ECCV_2018_paper.html" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_ECCV_2018/html/Nanyang_Wang_Pixel2Mesh_Generating_3D_ECCV_2018_paper.html</a></p>
</blockquote>
<h3 id="概括"><a href="#概括" class="headerlink" title="概括"></a>概括</h3><p>提出了单视图输入的end-to-end框架，能生成三角形网格（triangular mesh）。使用GCN来处理mesh的顶点特征，将椭球形变来拟合表面，同时利用图像的特征。mesh从粗到精细化，顶点数变多。定义了4种loss来优化结果。结果具有更好的精细度，超越了SOTA。</p>
<p>注：</p>
<ul>
<li>形变法的优点：deep network is better at predicting residual。a series of deformations can be added up together, which allows shape to be gradually refined in detail。it provides the chance to encode any prior knowledge to the initial mesh。</li>
<li>限制条件：只能用于亏格（genus）为零的shape，因为椭球没有“孔”</li>
<li>具有上采样层，点数逐渐增多。Deformation block则负责精细化mesh</li>
</ul>
<h3 id="原理-5"><a href="#原理-5" class="headerlink" title="原理"></a>原理</h3><p><img src="9_2.png" alt></p>
<p>将mesh初始化为椭球，然后用GCN对vertex feature施加变换，从而实现对3D vertex point的位置空间变换。然后上采样graph，以增加点数。不断重复此过程，实现从粗到精的过渡。image feature融合进入deformation blocks中来实现全局shape的把握。</p>
<h3 id="实现-8"><a href="#实现-8" class="headerlink" title="实现"></a>实现</h3><p>整体由image feature network和cascaded mesh deformation network组成。</p>
<p>deformation blocks：将图像的2D特征提升到3D vertex的特征中来（1280维），然后cancat（1280+128=1408维），再GCN（输出空间坐标3维和特征128维）。</p>
<p>graph unpooling layers：上采样图，增加顶点数，从而可以实现更精细的结构</p>
<ol>
<li><p>Initial ellipsoid</p>
<ul>
<li><p>ellipsoid with average size placed at the common location</p>
</li>
<li><p>initial feature contains only the 3D coordinate of each vertex（3维。特征提取之后才是128维的feature）</p>
</li>
</ul>
</li>
<li><p>GCN</p>
<p>将mesh用vertex和edge表示，那么就是一张graph。每个vertex上都有个feature。通过一次GCN层，得到变换后的特征：</p>
<p><img src="9_1.png" alt></p>
<p>其中$w_0$和$w_1$都是$d_{l+1}\times d_{l} $尺寸的变换矩阵，即经过一层GCN层其特征向量的维度可能发生变化。我们的特征向量$f_p$是“3D空间坐标”、“局部形状特征”和“输入图像特征”的cancat。使用GCN将变换特征，等效于空间点位置的变换以及特征提取。</p>
</li>
<li><p>Mesh deformation block</p>
<p><img src="9_3.png" alt></p>
<p>将“input image feature”提升到3D空间中的点特征，得到1280维的特征。与128维的形状特征concat得到1408维的特征，再进行GCN，其中一个分支预测点的3维坐标，另一个分支预测128维特征。</p>
<p>提升到3D空间：将3D点投射到2D平面上，然后根据图像中的gird feature进行双线性插值得到该点的特征。其中feature maps取自VGG16，一共有1280个通道，所以输出的特征是1280维。</p>
<p>残差结构的graph based ResNet（G-ResNet）有助于efficient exchange of the information between vertices、提升感受野。</p>
<p>所有的deformation blocks似乎是共用同一个VGG16的feature maps</p>
</li>
<li><p>Graph unpooling layer</p>
<p>目的：增加点的数量。使用图中Edge-based的方法。</p>
<p><img src="9_4.png" alt></p>
<p>This edge-based unpooling uniformly upsamples the vertices, and doesn’t causes the imbalanced vertex degrees.</p>
</li>
<li><p>Losses and Regularizations</p>
<ul>
<li>Chamfer loss：It is reasonably good to regress the vertices close to its correct position, however is not sufficient to produce nice 3D mesh</li>
<li>Normal loss：this loss requires the edge between a vertex with its neighbors to perpendicular to the observation from the ground truth. enforce the consistency of surface normal</li>
<li>Laplacian regularization：prevent the vertices from moving too freely, which potentially avoids mesh self-intersection. encourages neighboring vertices to have the same movement. laplacian regularization to maintain relative location between neighboring vertices during deformation,</li>
<li>Edge length regularization：edge length regularization to prevent outliers. 鼓励edge越短越好</li>
<li>总：就是上面4种的线性加权和</li>
</ul>
</li>
</ol>
<h2 id="Pixel2Mesh-（2019-8）"><a href="#Pixel2Mesh-（2019-8）" class="headerlink" title="Pixel2Mesh++（2019.8）"></a>Pixel2Mesh++（2019.8）</h2><blockquote>
<p>ICCV2019《Pixel2Mesh++: Multi-View 3D Mesh Generation via Deformation》</p>
<p>paper下载：<a href="https://arxiv.org/abs/1908.01491" target="_blank" rel="noopener">https://arxiv.org/abs/1908.01491</a></p>
</blockquote>
<h3 id="概括-1"><a href="#概括-1" class="headerlink" title="概括"></a>概括</h3><p>从多视图输入、已知相机姿态的条件下重建出3D Mesh，是Pixel2Mesh的升级版。同样是利用GCN抽取顶点的特征、对初始mesh进行粗到细的形变。不同的是形变的过程为：对每个顶点分别提出形变的候选点，然后利用projection、GCN操作、结合图像的feature maps变换特征得到每个候选点的重要性系数，进行坐标的线性加权得到形变后的坐标。能够生成视觉效果好的Mesh，泛化能力强，达到SOTA。</p>
<p>注：</p>
<ul>
<li>Pixel2Mesh生成的mesh往往只在single-image的视角方向效果好，再其他视角下效果差</li>
<li>多视图输入的关键原理：使用诸如mean、std、max的统计特征，实现可以接受任意多视图的输入</li>
<li>Pixel2Mesh++由perceptual network（VGG16）、coarse shape generation（Pixel2Mesh）和Multi-View Deformation Network（MDN）组成。</li>
<li>MDN是end-to-end trainable的。可接受任意数量的视图输入</li>
<li>注意在MDN中没有上采样层，不能增加点的数量。</li>
<li>作者主要的贡献是设计了MDN，而MDN的作用更像是微调已生成的mesh</li>
<li>候选点（Hypothesis）本质上是对vertex周围环境的感知，GCN起到类似于local graph的neighborhood之间的特征交互融合、特征提取的作用。最后根据GCN得到的score来make movement decision。而Pixel2Mesh没有感知vertex周围环境的能力。</li>
</ul>
<h3 id="原理-6"><a href="#原理-6" class="headerlink" title="原理"></a>原理</h3><p><img src="10_1.png" alt></p>
<p>基于Pixel2Mesh做初始mesh的生成。从多视图中用VGG16抽取特征，底层级称作geometry feature，高层级称作semantic feature。然后为每个vertex生成形变候选点，利用图像抽取来的特征为每个候选点计算score，最后根据score和3D坐标作线性加权得到最终形变后的位置。多次形变即可得到输出的mesh。</p>
<h3 id="实现-9"><a href="#实现-9" class="headerlink" title="实现"></a>实现</h3><ol>
<li><p>Deformation Hypothesis Sampling</p>
<p>目的：propose deformation hypotheses（形变候选点）for each vertex</p>
<p><img src="10_2.png" alt></p>
<p>实现：在每个vertex的位置上放置一个level-1 icosahedron（具有42个顶点、80个面、120条边），然后为每个vertex构造一个local graph（local graph的边由“level-1 icosahedron”的120条边、vertex到“level-1 icosahedron”的顶点的连线组成。所以一个local graph一共有120+42=162条边，42+1=43个点）。每个顶点都要构造一个local graph，然后喂给接下来的“Cross-View Perceptual Feature Pooling”步骤</p>
</li>
<li><p>Cross-View Perceptual Feature Pooling</p>
<p>目的：assign each node（in the local graph）features from the multiple input color images</p>
<p><img src="10_3.png" alt></p>
<p>大体类似于Pixel2Mesh中的“perceptual feature pooling”操作。feature maps来自VGG16的conv1_2、conv2_2、conv3_3（它们是底层级的geometry feature），一个视图concat起来输出64+128+256=448长度的特征。</p>
<p>考虑到对不同数量的视图输入如果concat的话会得到不同长度的feature。我们改而使用统计特征，实现固定长度的输出。使用mean、max、std这三种统计特征。而且使用统计特征对视图的输入的顺序具有不变性。所以特征长度变为448*3=1344维。再把空间坐标concat进去得到1344+3=1347维度的特征。</p>
</li>
<li><p>Deformation Reasoning</p>
<p>目的：reason an optimal deformation for each vertex from the hypotheses using pooled cross-view perceptual features</p>
<p><img src="10_4.png" alt></p>
<p>实现：将local graph和local graph nodes feature（即pooled cross-view perceptual features）丢到scoring network（本质是GCN）中得到每个nodes的重要性得分$s_i$（softmax输出后自动满足$\sum_{i=1}^{43} s_i=1$）。若候选点的空间位置记作$h_i$（包括vertex，$i=1…43$），那么形变后的位置就是位置的线性加权：$\sum_{i=1}^{43}s_i h_i$。对每个顶点都运行一遍，就得到了最终形变后的结果。</p>
</li>
<li><p>Losses</p>
<p>损失函数照抄Pixel2Mesh的，但是唯一的区别是作者还extends the Chamfer distance loss to a resampled version，即：从mesh的face上均匀采样点，采样的点数正比于face的面积，由此可以额外采样出4000个点，加上2466个原本的mesh的顶点，总共6466个点，根据6466个点计算Chamfer distance loss</p>
<p>re-sample Chamfer loss的功能：helps to remove artifacts in the results，解决了points are not uniformly distributed on the surface的问题</p>
</li>
<li><p>Implementation Details</p>
<p>we use Pixel2Mesh to generate a coarse shape with 2466 vertices → 最终生成的点的数量就是2466个</p>
</li>
</ol>
<h3 id="实验-4"><a href="#实验-4" class="headerlink" title="实验"></a>实验</h3><ul>
<li><p>对比的网络：</p>
<ul>
<li>P2M-M：we directly run single-view Pixel2Mesh on each of the input image and fuse multiple results</li>
<li>MVP2M：we replace the perceptual feature pooling to our cross-view version to enable Pixel2Mesh for the multi-view scenario</li>
<li>LSM：Learnt Stereo Machine</li>
<li>3DR2N2：3D Recurrent Reconstruction Neural Network</li>
</ul>
</li>
<li><p>性能：Our model significantly outperforms previous methods（based on F-score）</p>
</li>
</ul>
<h2 id="AtlasNet（2018-2）👍"><a href="#AtlasNet（2018-2）👍" class="headerlink" title="AtlasNet（2018.2）👍"></a>AtlasNet（2018.2）👍</h2><blockquote>
<p>CVPR2018《A Papier-Mâché Approach to Learning 3D Surface Generation》</p>
<p>paper下载：<a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Groueix_A_Papier-Mache_Approach_CVPR_2018_paper.html" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2018/html/Groueix_A_Papier-Mache_Approach_CVPR_2018_paper.html</a></p>
</blockquote>
<h3 id="概述-8"><a href="#概述-8" class="headerlink" title="概述"></a>概述</h3><p>提出使用基于<a href="https://zh.wikipedia.org/wiki/流形" target="_blank" rel="noopener">流形</a>中Charts和Atlases的概念的3Dshapes的surface生成方法。将surface表示成多个局部小表面的集合（类似于Charts的概念）。提高了精度、泛化强、能生成任意精度的分辨率。AtlasNet的本质是一个3D shape decoder（输入形状描述子输出mesh/pointcloud）。AtlasNet可以应用于形状自编码、3D重建等多个领域。</p>
<p>注：</p>
<ul>
<li>pointcloud的缺点：不能直接表示与邻居的关系（no surface connectivity），使得很难得到光滑、高保真的流形</li>
<li>polygonal mesh的特点：对光滑流形的分块平面近似</li>
<li>Surface parameterization： Establishing a connection between the surface of the 3D shape and a 2D domain</li>
<li>3D重建时输入的对象可以是pointcloud、images等，因为AtlasNet只要求特征向量作为输入，可以使用其他网络提取输入对象的特征向量之后作为AtlasNet的输入。</li>
<li>chart：3D→2D（1个MLP函数）；atlas：a set of charts（多个MLPs）</li>
<li>AtlasNet的功能：learn an atlas for 2-manifold $\mathcal{S}$</li>
<li>缺点：生成的mesh不闭合，有小孔，不同的曲面块之间有overlap</li>
</ul>
<h3 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h3><p><img src="11_1.png" alt></p>
<p>将2D单位正方形平面的点集（$[0,1]\times [0,1]$）使用MLP映射到3D曲面上。有多个MLP，于是就能得到多个小的3D曲面。多个小3D曲面就能组成物体。即，we seek to approximate the target surface locally by<br>mapping a set of squares to the surface of the 3D shape</p>
<script type="math/tex; mode=display">
[0,1]\times[0,1] \leftrightarrow 3D \; surface</script><p>这样1个MLP就能局部生成一个2-manifold（3D曲面）了</p>
<ul>
<li><p>2D→3D：通过MLP得到</p>
</li>
<li><p>3D→2D：UV parameterization of the surface / 2D （UV） embedding to a plane</p>
</li>
</ul>
<p>于是在2D平面上的性质就能迁移到3D曲面中：例如2D平面有纹理，则在3D曲面的对应点上也有相同的纹理</p>
<p>生成mesh的方法：将方形的2Dmesh映射到3D空间中，并保持点与点之间的连接关系</p>
<h3 id="实现-10"><a href="#实现-10" class="headerlink" title="实现"></a>实现</h3><ol>
<li><p>Learning to decode a surface</p>
<p>目标：有一个形状描述子$\mathtt{x}$（形状特征向量），请据此生成the surface of the shape</p>
<p>实现：</p>
<ul>
<li>有N个learnable parameterizations（MLP）$\phi_{\theta_i}$，$i\in\{1,…,N\}$，其中$\theta_{i}$是MLP参数</li>
<li><p>将$\mathtt{x}$和$(0,1)\times(0,1)$采样的点$(x,y)$进行cancat，再送到MLP中</p>
</li>
<li><p>损失函数：Chamfer loss。通过计算两个点集之间的Chamfer Distance来比较。</p>
</li>
</ul>
<p>注意到MLP并没有明确禁止所编码的区域重叠（符合atlas的要求之一），并且最好覆盖整个shape（全覆盖才能使得Chamfer loss变小）</p>
</li>
<li><p>Implementation details</p>
<ul>
<li>auto-encoder中的encoder是PointNet；形状描述子的维度：1024；输入点云的点数：250~2500</li>
<li>reconstruction中的encoder是ResNet-18；只训练encoder，而decoder则是autoencoder中的decoder（固定参数，不训练）</li>
<li>decoder是4层的MLP：1024（ReLU）、512（ReLU）、256（ReLU）、128（tanh）。使用tanh的原因可能是点云经过单位球化了</li>
<li>输出点云的点数：2500</li>
<li>2D采样方法：网格规则采样</li>
</ul>
</li>
<li><p>Mesh generation</p>
<p>有三种方法：</p>
<ul>
<li>Propagate the patch-grid edges to the 3D points：即transfer a regular mesh on the unit square to 3D，使得可以生成高分辨率的mesh，是一种自然的生成mesh的好方法（因为mesh的拓扑连接关系源自于2D grid的edge的连接方式）【AtlasNet专用，效果好】</li>
<li>Generate a highly dense point cloud and use Poisson surface reconstruction（PSR）：生成很多点以及他们的normals，然后用PSR方法来得到mesh【Baseline专用，AtlasNet可用】</li>
<li>Sample points on a closed surface rather than patches：从3D球面采样输入的点，而不是从多个2D单位正方形中采样。可以生成闭合图形，但是似乎只能用一个MLP。【AtlasNet专用，记作“1 sphere”】</li>
</ul>
</li>
</ol>
<h3 id="实验-5"><a href="#实验-5" class="headerlink" title="实验"></a>实验</h3><ol>
<li><p>数据集：standard ShapeNet Core dataset</p>
</li>
<li><p>评价指标：Chamfer distance（CD，用于衡量pointcloud）和Metro（average Euclidean distance between the two meshes，用于衡量mesh）</p>
</li>
<li><p>Baseline/Oracle</p>
<ul>
<li>“Points baseline”：如上一幅图的（a）所示。直接输入1024维度的形状描述子，然后输出2500*3=7500维度，即2500个点（MLP层数为1024（ReLU）、512（ReLU）、256（ReLU）、7500（tanh））。最终输出的是pointcloud，所以没有“Metro”指标。</li>
<li>“Pointsbaseline + normals”：输出空间指标和法向量，一个点是$\mathbb{R}^6$。然后根据切平面来增加点的数量（Augmentation）。然后使用PSR算法得到mesh。最终输出的是mesh。</li>
<li>Oracle：从GT中随机采样点，然后计算CD和Metro的值。代表了模型所能达到的性能的上界（因为是由GT生成的）</li>
</ul>
</li>
<li><p>3D reconstruction结果</p>
<p><img src="11_2.png" alt></p>
</li>
<li><p>其他实验</p>
<ul>
<li><p>Generalization across object categories：略</p>
</li>
<li><p>Singleview reconstruction：略</p>
</li>
<li><p>Shape interpolation</p>
<p><img src="11_3.png" alt></p>
</li>
<li><p>Finding shape correspondences：Notice that we get semantically meaningful correspondences, such as the chair back, seat, and legs without any supervision from the dataset on semantic information.</p>
<p><img src="11_4.png" alt></p>
</li>
<li><p>Mesh parameterization：our inferred atlas usually has relatively high texture distortion. But we can minimize distortion with off-theshelf geometric optimization, yielding small distortion.</p>
<p><img src="11_8.png" alt></p>
</li>
<li><p>Excess of distortion</p>
<p><img src="11_5.png" alt></p>
</li>
<li><p>Topological issues</p>
<p><img src="11_6.png" alt></p>
</li>
<li><p>Deformable shapes</p>
<p><img src="11_7.png" alt></p>
</li>
</ul>
</li>
</ol>
<h2 id="Deep-Marching-Cubes（2018-6）"><a href="#Deep-Marching-Cubes（2018-6）" class="headerlink" title="Deep Marching Cubes（2018.6）"></a>Deep Marching Cubes（2018.6）</h2><blockquote>
<p>CVPR2018《Deep Marching Cubes: Learning Explicit Surface Representations》</p>
<p>paper下载：<a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Liao_Deep_Marching_Cubes_CVPR_2018_paper.html" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2018/html/Liao_Deep_Marching_Cubes_CVPR_2018_paper.html</a></p>
</blockquote>
<h3 id="概述-9"><a href="#概述-9" class="headerlink" title="概述"></a>概述</h3><p>提出了一种end-to-end的surface prediction方法。即“Differentiable Marching Cubes Layer”，使得可作为神经网络的最后一层来输出mesh。并且设计了loss函数使得可以在sparse point supervision上训练。并且能补全形状、分离物体的内外表面，即使在GT是稀疏和不完全的情况下。可以用于从点云中推断形状。可以与各种encoder、shape inference techniques结合使用。</p>
<p>特点：</p>
<ul>
<li>differentiable, end-to-end</li>
<li>predicts explicit surface representations of arbitrary topology</li>
<li>avoids the need for defining auxiliary losses or converting target meshes to implicit distance fields</li>
<li>more accurate reconstructions while also handling noise and missing observations</li>
<li>separating inside from outside even if the ground truth is sparse or not watertight</li>
<li>easily integrating additional priors about the surface（e.g., smoothness）</li>
<li>does not require explicit surface ground truth</li>
</ul>
<p>注：</p>
<ul>
<li>普通的marching cubes的输入是SDF，而deep marching cubes输入是$O$和$X$。所以网络的输出类型也要要求不同。</li>
<li>现有的3D surface prediction不能end-to-end训练，因为他们需要中间表示（例如TSDF），因而需要后处理（例如marching cubes算法）</li>
<li>普通的marching cubes算法不可微分</li>
<li>传统的表面重建方法：<ul>
<li>Voxel based method <ul>
<li>output: voxel occupancy / TSDF</li>
<li>post-processing: Marching cubes</li>
</ul>
</li>
<li>Point based method<ul>
<li>output: points</li>
<li>post-processing: Poisson surface reconstruction / SSD</li>
</ul>
</li>
</ul>
</li>
<li>传统方法的缺点：<ul>
<li>the ground truth of the implicit representation is often hard to obtain, e.g., in the presence of a noisy and incomplete point cloud or when the inside and outside of the object is unknown.</li>
<li>these methods only optimize an auxiliary loss defined on an intermediate representation and require an additional postprocessing step for surface extraction. Thus they are unable to directly constrain the properties of the predicted surface.</li>
</ul>
</li>
</ul>
<h3 id="实现-11"><a href="#实现-11" class="headerlink" title="实现"></a>实现</h3><ol>
<li><p>传统的Marching Cubes（非本文的方法）</p>
<p>分为两步：</p>
<ul>
<li>estimation of the topology：确定cell的拓扑类型（面的顶点位置不确定，只是知道拓扑概念上的位置）</li>
<li>prediction of the vertex locations of the triangles：确定面的顶点的具体位置</li>
</ul>
<p>记号：</p>
<ul>
<li>$D \in \mathbb{R}^{N\times N\times N}$为signed distance field。表示到surface的有向距离。内部为正，外部为负，表面为零。其元素记作$d_n$，其中$n=(i,j,k)\in \mathbb{N}^3$是索引</li>
</ul>
<p>实现：</p>
<ul>
<li><p>首先确定cell’s surface topology T。假设$D\in \mathbb{R}^{3\times 3\times 3}$，那么相当于这27个点分布在二阶魔方的各个角块的顶角上，它只是点。cell就是指二阶魔方的8个角块，它具有体积。然后根据d是否符号改变（穿过零）确定这个cell的拓扑类型。（只是确定了拓扑类型，图中蓝色面的顶点的具体位置并不知道，需要下一步来计算）</p>
<p><img src="12_1.png" alt></p>
</li>
<li><p>然后如果这个cell内部存在蓝色平面，就需要计算蓝色平面顶点的具体位置。使用线性插值得到。（如果是上图中的左上角的第一种类型，那就不用计算了，这种类型往往出现在物体的内部、或者是远离物体的外部）</p>
<p><img src="12_2.png" alt></p>
</li>
</ul>
</li>
<li><p>Differentiable Marching Cubes</p>
<p>记号：</p>
<ul>
<li><p>每个格点的伯努利分布参数：$O\in [0,1]^{N\times N\times N}$，$o_n\in[0,1]$</p>
</li>
<li><p>每个格点的面顶点的偏移量：$X\in [0,1]^{N\times N\times N\times3}$，$x_n\in [0,1]^3$</p>
</li>
</ul>
<p>计算：</p>
<ul>
<li>那么格点$n$处于状态$t\in \{0,1\}$（1代表被占据（红色表示），0代表不被占据（绿色表示））时的概率为$p_n(t)=(o_n)^t(1-o_n)^{1-t}$</li>
<li>那么grid cell n处于状态$T$时的概率为$p_n(T)=\prod_{m\in \{0,1\}^3}(o_{n+m})^{t_m}(1-o_{n+m})^{1-t_m}$。其中$2^8$种顶点的排列方式中只有140种是有效的拓扑形式。</li>
<li>那么entire grid的概率分布就是$p(\{T_n\})=\prod_{n\in\mathcal{T}}p_n(T_n)$。其中$\mathcal{T}=\{1,…,N-1\}^{3}$有$(N-1)^3$个cell</li>
</ul>
<p>网络结构图：（输入点云，然后预测出$O$和$X$，从而预测出mesh）</p>
<p><img src="12_3.png" alt></p>
<p>Loss：（包含4项）</p>
<ul>
<li>Point to Mesh Loss：最小化observed 3D points到mesh平面的几何距离之和。作用：directly<br>measures the geometric error of the inferred mesh</li>
<li>Occupancy Loss：鼓励边界上不被占据，鼓励内部被占据</li>
<li>Smoothness Loss：鼓励相邻格点的占据状态不变</li>
<li>Curvature Loss：鼓励相邻cell的normal orientation的smooth transitions</li>
</ul>
</li>
</ol>
<h2 id="DeepSDF（2019-1）👍"><a href="#DeepSDF（2019-1）👍" class="headerlink" title="DeepSDF（2019.1）👍"></a>DeepSDF（2019.1）👍</h2><blockquote>
<p>CVPR2019《DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation》</p>
<p>paper下载：<a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Park_DeepSDF_Learning_Continuous_Signed_Distance_Functions_for_Shape_Representation_CVPR_2019_paper.html" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_CVPR_2019/html/Park_DeepSDF_Learning_Continuous_Signed_Distance_Functions_for_Shape_Representation_CVPR_2019_paper.html</a></p>
</blockquote>
<h3 id="概述-10"><a href="#概述-10" class="headerlink" title="概述"></a>概述</h3><p>DeepSDF能生成具有复杂拓扑结构的、高质量的、闭合的连续表面、可用于形状插值、形状补全。一个DeepSDF能够表示一个类别的形状（一类物体训练一个模型，而传统的SDF方法一个模型只能表示一个物体的形状）。取得了SOTA。而且模型的大小缩小了。</p>
<p><img src="13_3.png" alt></p>
<p>本质：使用“coordinate + shape latent vector”作为输入，通过MLP后输出SDF值。训练时同时优化“shape vector”和“MLP parameters”（梯度下降优化，最后得到的shape vector就是训练集的shape vector）。推断“latent vector”时使用梯度下降不断迭代优化“latent vector”的初始值，从而得到符合observation的latent vector的值。</p>
<p>注：</p>
<ul>
<li>DeepSDF最有用的用途是Shape Completion（inference + feed-forward + marchingcubes）、Shape Interpolation（inference + interpolate + feed-forward + marchingcubes）</li>
<li>SDF：有向距离函数：内部负数、外部正数、表面为零。</li>
<li>“auto-decoder”可用于降噪、补全、错误检测</li>
<li>补全：只输入部分数据，然后生成latent vector（形状信息），再生成完整的shape prediction</li>
<li>最后还是要用marching cubes来生成等值面</li>
<li>feed-forward耗时短；inference耗时长（对latent vector的优化速度慢，或许可以考虑替换掉ADAM优化器）</li>
</ul>
<p>TODO IDEA：</p>
<ul>
<li>或许可以用于生成动态的3D物体、或者是带有纹理材质的物体。</li>
<li>不同物体形状在latent vector分布中的位置不能指定。或许latent vector可以是类似one-hot的，一个变量代表一个物体种类，每个变量都服从高斯，然后只需要从高斯采样就能得到物体形状了。可能类似于Variational Auto-Decoder？</li>
</ul>
<h3 id="实现-12"><a href="#实现-12" class="headerlink" title="实现"></a>实现</h3><p>通过学习一个“shape-conditioned classifier”，决策边界（最后一层是tanh）就是物体表面。</p>
<p>使用“auto-decoder”，接受latent vector和point coordinate作为输入，输出SDF值。</p>
<ul>
<li>training：已知observation（点坐标&amp;SDF）来优化latent vector $z$和model parameters $\theta$</li>
<li>inference：已知observation（点坐标&amp;SDF）求latent vector $z$</li>
<li>feed-forward：已知latent vector $z$和点坐标，求SDF</li>
</ul>
<p><img src="13_1.png" alt></p>
<p>记号：</p>
<ul>
<li>coordinate $x$丢到模型里得到的SDF估计（latent vector为$z$）：$f_\theta(z,x)$</li>
<li>第i个形状的coordinate $x$处真实的SDF值：${SDF^i}(x)$</li>
<li>第i个形状的数据集（包含点坐标、真实的SDF值）：$X_i := \{(x_j,s_j):s_j=SDF^i(x_j)\}$</li>
</ul>
<p>第一种方式是使用上图（a）所示的方法。但是一个模型只能对应一个物体。故我们不采用。</p>
<p>第二种方式是使用上图（b）所示的方法。一个模型可以对应一整类物体（例如各种小车）。我们采样这种方法。但是code未知，我们使用“auto-decoder”的学习方式来自动学习code的分布（注意这里的code不是人为指定的分布，例如不是one-hot）</p>
<ul>
<li><p>训练（training）：我们首先初始化code为一个很小的值（例如$\mathcal{N}(0,0.01^2)$），然后根据已知的coordinate和SDF值来训练网络，反向传播时根据$\frac{\partial Loss}{\partial z_i}$和$\frac{\partial Loss}{\partial \theta}$分别更新Code $z_i$和Parameter $\theta$。</p>
<p><img src="13_5.png" alt></p>
</li>
<li><p>推断（inference）：根据已知的coordinate和SDF值，利用反向传播的$\frac{\partial Loss}{\partial z}$，更新优化Code $z$</p>
<p><img src="13_4.png" alt></p>
</li>
<li><p>前向传播（暂且称作feed-forward吧）：根据latent vector $z$和点坐标$x$得到预测的SDF值</p>
</li>
</ul>
<p>注：</p>
<ul>
<li><p>还可以计算得到surface normals：方法一和二（采用SDF的方法）都能通过计算$\frac{\partial f_\theta(x)}{\partial x}$来计算估计得到的表面法向（因为沿着法线方向SDF变化最快）</p>
</li>
<li><p>对于一个点的损失函数：$\mathcal{L}(f_\theta(x),s)=|clamp(f_\theta(x),\delta)-clamp(s,\delta)|$，以$f_\theta(x)$、$s$为x、y轴画图得（$\delta$越小越能描述表面的细节）：</p>
<p><img src="13_2.png" alt></p>
</li>
<li><p>“auto-decoder”是对“coded shape deepsdf”的训练方式。</p>
</li>
<li><p>为什么不用“encoder-decoder”结构：</p>
<p>传统的“encoder-decoder”结构的使用方法：</p>
<ul>
<li>训练：“encoder-decoder”两部分一起训练</li>
<li>测试：丢掉训练好的encoder。在形状补全任务中，首先准备待补全的训练数据，然后重新训练另一个encoder。</li>
</ul>
<p>作者认为训练“encoder-decoder”结构时，最后使用时只是用到了decoder，而没有用到训练完成的encoder，极大浪费了计算力。并且使用“encoder-decoder”进行形状补全时，训练新的encoder时的训练数据也要是“待补全”的数据集（这样才能从输入推断出latent vector）。</p>
</li>
<li><p>DeepSDF可以只输入部分的observation（例如depth map中点，只是部分的表面的点），然后推断出latent vector $z$的值，所以可以用于形状补全。</p>
</li>
<li><p>整个实现都是基于在物体对齐的条件下进行的（in a canonical pose）。否则同一个coordinate会因为不对齐（例如旋转）而会有不同的SDF值</p>
</li>
</ul>
<h3 id="实现-13"><a href="#实现-13" class="headerlink" title="实现"></a>实现</h3><p>实施细节见前面的部分。下面是实验部分（使用ShapeNet）：</p>
<ol>
<li><p>Representing Known 3D Shapes（represent training data）</p>
<p>用于测试latent vector和MLP是否能充分表示精细的结构。所使用的物体是在训练集内的。</p>
<p>实现过程：利用GT（在训练集中）的$X$（包含point coordinate和SDF）来inference得到这个物体所对应的latent vector的值；然后输入latent vector和密集网格采样的point coordinate，通过正向传播得到预测的SDF值，再用marching cubes来重建表面。</p>
</li>
<li><p>Representing Test 3D Shapes（use learned feature representation to reconstruct unseen shapes）</p>
<p>用于测试“得到latent vector的方法”是否能用于全新未见过的物体</p>
<p>实现过程：利用GT（不在训练集中）通过inference的方法得到latent vector；再正向传播、marching cubes得到表面。</p>
</li>
<li><p>Shape Completion（apply shape priors to complete partial shapes）</p>
<p>已知物体表面的一些部分（例如depth map只涉及一些表面），求整个物体的形状</p>
<p>实现过程：根据depth map可以知道位于surface上的点，然后估计surface normal。沿着surface normal的方向偏移$\pm \eta$的距离得到两个点（位于surface两侧且距离surface$\eta$的两个点），这样就得到SDF的样本数据。利用SDF样本数据先inference得到latent vector，再正向传播、marching cube就能得到mesh。</p>
<p>注：depth map的视角需要与物体对齐。作者还额外从“介于surface和camera之间的free-space”中随机采样一些点作为empty space points（或者称作free-space observations，且SDF值大于零），使得样本的点数增加，且不会在free-space中生成奇怪的形状。而且添加了“freespace loss”。</p>
</li>
<li><p>Latent Space Shape Interpolation（learn smooth and complete shape embedding space from which we can sample new shapes）</p>
<p>过程：利用training最后时得到的latent vector（或inference得到的都可以），将两个物体的latent vector线性插值再feed-forword得到中间状态的物体</p>
</li>
</ol>
<h2 id="GEOMetrics（2019-1）"><a href="#GEOMetrics（2019-1）" class="headerlink" title="GEOMetrics（2019.1）"></a>GEOMetrics（2019.1）</h2><blockquote>
<p>PMLR《GEOMetrics: Exploiting Geometric Structure for Graph-Encoded Objects》</p>
<p>paper下载：<a href="http://proceedings.mlr.press/v97/smith19a/smith19a.pdf" target="_blank" rel="noopener">http://proceedings.mlr.press/v97/smith19a/smith19a.pdf</a></p>
</blockquote>
<h3 id="概述-11"><a href="#概述-11" class="headerlink" title="概述"></a>概述</h3><p>能够生成Adaptive mesh（顶点密度不均匀的mesh），作者是第一个。提出了0N-GCN layers，避免了顶点信息的平滑；提出了Adaptive Face Splitting，在高曲率的地方自动分割三角面片（改变拓扑），使得可以生成细节；改进了Chamfer Loss得到Point-to-point loss和Point-to-surface loss。提出了global mesh loss用于比对形状的全局信息。并最后用单图重建mesh的任务来说明其性能。达到了SOTA，视觉效果好，所需的顶点数较少。</p>
<p>基本流程（主要基于deformation）：对初始mesh（球）附带特征（例如通过project投射到CNN的feature map上），然后用0N-GCN进行mesh的形变和特征变换，最后用face splitting策略将曲率大的三角面片添加顶点继续细分。不断重复得到最终输出mesh。</p>
<p><img src="14_3.png" alt></p>
<p>注：</p>
<ul>
<li><p>GEOMetrics == Geometrically Exploited Object Metrics。利用了graph-encoded objects的优势和几何结构</p>
</li>
<li><p>将triangle mesh视作graph便可以使用图卷积的方法</p>
</li>
<li><p>传统mesh重建的效果：顶点和面的分布比较均匀。没有充分利用mesh表示的独有优势。应当在细节处放置更多顶点、在平坦处放置顶点较少。这样才能with smaller space requirements and higher precision</p>
</li>
<li><p>普通的GCN可能会使得顶点的信息平滑（是由于取neighborhood mean造成的），作者提出对于一些特征维度不聚合邻居信息而是保持原样，以避免特征的平滑（over-smoothing）。</p>
</li>
<li><p>普通的Chamfer Distance只是约束vertex position，并没有约束三角面片。</p>
</li>
<li><p>face splitting技术似乎生成了许多不想要的细节，例如：</p>
<p><img src="14_2.png" alt></p>
</li>
</ul>
<h3 id="原理-7"><a href="#原理-7" class="headerlink" title="原理"></a>原理</h3><p><img src="14_1.png" alt></p>
<p>上图是mesh reconstruction module的构成（主要功能：细化输入的mesh）。整个系统由m=3个这样的module串联组成。系统的输入是球状mesh和一张图片。不同module的cnn feature map其实都是同一个。</p>
<p>首先将mesh投射到feature map上使得vertices带上feature map上的feature vector。然后使用作者发明的GCN（Zero-Neighbor Graph Convolutional Networks）进行graph的特征提取和deformation。最后使用face splitting技术分割曲率大的面（以描述细节部分）。最后得到输出的mesh。再重复m-1次module的流程，最后输出的mesh就是最终结果。</p>
<h3 id="实现-14"><a href="#实现-14" class="headerlink" title="实现"></a>实现</h3><ol>
<li><p>feature extraction</p>
<p>得到的feature vector应该是以下三者/两者的cancat：</p>
<ul>
<li>feature map对应位置上的feature vector</li>
<li>3D point的空间坐标</li>
<li>前一个module输出的mesh的feature vector（如果当前module不是第一个的话）</li>
</ul>
</li>
<li><p>mesh deformation</p>
<p>使用多个“0N-GCN layers”进行特征变换，输出residual prediction（位置的偏移量），那么3D点的最终位置就是original position + residual prediction</p>
<p>“0N-GCN layers”的目的：为了解决顶点的特征被平滑掉的问题</p>
<p>传统GCN：$H’=\sigma(AHW+b)$，其中A是NxN的邻接矩阵，H是NxF的输入特征向量矩阵，H’是NxF’的输出特征向量矩阵，W是权重矩阵，b是偏置向量，$\sigma(\cdot)$是激活函数</p>
<p>方法：一部分特征维度取邻接矩阵的零次幂（identity matrix）以不聚合特征，其余维度正常聚合特征</p>
<p>即现如今变成：$H’=HW,\;H’’=\sigma([AH’_{0:i}||A^0 H’_{i:}]+b)$</p>
<p>其中$[\cdot||\cdot]$是concat算符（cancat到矩阵的右边），且$i\in[0,F’-1]$</p>
<p>但是其中$i$是怎么选取的不知道。</p>
</li>
<li><p>face splitting</p>
<p>对曲率大的曲面，在center位置添加顶点，并添加3条连向它的边。</p>
<p><img src="14_4.png" alt></p>
<p>曲率的计算方法（似乎是定性表示）：</p>
<ul>
<li>首先根据三角面片的3个顶点，可以计算得到其法向$N_f$</li>
<li>然后对于三角面片$f$，计算其邻居面$i\in \mathcal{H}_f$的法向量$N_i$，并计算$N_i $与$N_f$的夹角$\theta_{f,i}$</li>
<li>然后计算$\theta_{f,i}$的平均值$\frac{1}{|\mathcal{H}_f|}\sum_{i\in \mathcal{H}_f} \theta_{f,i}$便得到在面$f$处的曲率$C_f$</li>
</ul>
<p>根据设定的曲率阈值$\alpha$来判断曲率大不大，大的话就进行face splitting</p>
</li>
<li><p>Differentiable Surface Sampling Losses（Chamfer Loss的改版）</p>
<p>用于描述local topology，即三角面片的约束</p>
<p><img src="14_7.png" alt></p>
<p>过程：</p>
<ul>
<li><p>首先需要从GT mesh和pred mesh的surface中采样，分别得到点集$S$和$\hat{S} $</p>
</li>
<li><p>如果是Point-to-point的形式：（点到点的距离）</p>
<p><img src="14_5.png" alt></p>
</li>
<li><p>如果是Point-to-surface的形式：（点到面的距离）</p>
<p><img src="14_6.png" alt></p>
</li>
</ul>
</li>
<li><p>Global Encoding of Graphs（用于描述全局信息）</p>
<p>global mesh loss，或称作latent loss，见下图：</p>
<p><img src="14_8.png" alt></p>
<p>是两个latent vector之差的二范数的平方</p>
<p>encoder：stacking 0N-GCN layers，followed by a max pooling</p>
<p>decoder：3D CNN</p>
<p>做成encoder-decoder的结构，其中后面voxel的形式用于监督学习，目标仅仅是得到为了encoder</p>
</li>
<li><p>Optimization Details</p>
<p>还有an edge length minimizing regularizer、Laplacian-maintaining regularizer，最后的Loss是上述4者的加权和。</p>
</li>
</ol>
<h2 id="Skeleton-bridged-Approach（2019-3，Our-lab）"><a href="#Skeleton-bridged-Approach（2019-3，Our-lab）" class="headerlink" title="Skeleton-bridged Approach（2019.3，Our lab）"></a>Skeleton-bridged Approach（2019.3，Our lab）</h2><blockquote>
<p>CVPR2019《A Skeleton-Bridged Deep Learning Approach for Generating Meshes of Complex Topologies From Single RGB Images》</p>
<p>paper下载：<a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Tang_A_Skeleton-Bridged_Deep_Learning_Approach_for_Generating_Meshes_of_Complex_CVPR_2019_paper.html" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_CVPR_2019/html/Tang_A_Skeleton-Bridged_Deep_Learning_Approach_for_Generating_Meshes_of_Complex_CVPR_2019_paper.html</a></p>
</blockquote>
<h3 id="概述-12"><a href="#概述-12" class="headerlink" title="概述"></a>概述</h3><p>从单视图重建3Dmesh。“Approach”主要由3部分组成（即3个stages），第一部分得到curve- and surface-like skeleton points，第二部分将skeleton转换为volume然后提升分辨率，第三部分将volume转换为mesh然后deform。每个stages都有独立的input image的CNN来修正每一stage积累的预测偏差。因而将point、voxel、mesh表示都用了一遍。能够生成具有复杂拓扑、细节精细的mesh，优于现有算法。并公开ShapeNet-Skeleton数据集。</p>
<p><img src="15_4.png" alt></p>
<p>启发：</p>
<ul>
<li>将原始输入重新以DNN的方式融入或许能增加性能（例如skip-connection、images information的再次输入等等）</li>
<li>其他：网络的不同深度放置多个loss有助于梯度信息的回传和优化（虽然本文没涉及）</li>
</ul>
<p>注：</p>
<ul>
<li>通俗地说就是多种方法的串联使用（大致是：AtlasNet + 3DCNN + Pixel2mesh）</li>
<li>使用skeleton具有好性质：topology preservation、lower complexity to learn。但实质上其skeleton是点集。</li>
<li>skeleton的GT是meso-skeleton</li>
</ul>
<h3 id="原理-8"><a href="#原理-8" class="headerlink" title="原理"></a>原理</h3><p>整体流程见下图</p>
<ul>
<li>将image转skeleton points，仿照AtlasNet：将1D、2D映射到3D得到curve- and surface-like skeleton points</li>
<li>利用voxelization转换为体素表示</li>
<li>利用3DCNN提升体素分辨率得到高分辨率的体素表示</li>
<li>利用Marching Cubes将体素转换为mesh，利用QEM算法减少顶点数</li>
<li>将mesh进行deformation，模仿Pixel2mesh：使用GCN来deformation</li>
</ul>
<h3 id="实现-15"><a href="#实现-15" class="headerlink" title="实现"></a>实现</h3><p><img src="15_1.png" alt></p>
<ol>
<li><p>Learning of Meso-Skeleton（stage 1）</p>
<p>形状的meso-skeleton表示为medial axis。3D模型的medial axis由curve skeletons（由点组成的线）、median sheets（由点组成的面）组成。</p>
<p>数据集的制作：将3D模型转换为点云。用Deep points consolidation算法提取meso-skeleton points。根据其邻居用PCA将meso-skeleton points划分为curve-like或surface-like。</p>
<p>实现：</p>
<ul>
<li>用ResNet-18将图像编码得到latent vector</li>
<li>CurSkeNet （上面那条分支）：将$[0,1]$的1D实数线段利用MLP映射到3D空间中得到3D曲线。一共有20个这类MLP。（MLP的前面几层是ReLU、最后一层是tanh）</li>
<li>SurSkeNet（下面那条分支）：将$[0,1]^2$的2D实数正方形利用MLP映射到3D空间中得到3D曲面。一共有20个这类MLP。（MLP的前面几层是ReLU、最后一层是tanh）</li>
<li>训练：使用Chamfer Distance作为Loss、使用Laplacian smoothness作为正则项（使得得到的surface光滑）</li>
</ul>
</li>
<li><p>From Skeleton to Base Mesh（stage 2）</p>
<p><img src="15_2.png" alt></p>
<p>如上图，使用了两者不同分辨率的体素表示。上面的绿色部分作为全局指导，下面的蓝色部分作为局部的细节生成。图像encoder为ResNet-18，图像信息的再次使用提升了细节。</p>
<p>既能保持high-resolution，也能preserves global structure。</p>
<p>使用Marching Cubes将其转换为mesh，使用QEM algorithm简化得到的mesh。</p>
</li>
<li><p>Mesh Refinement（stage 3）</p>
<p><img src="15_3.png" alt></p>
<p>使用VGG-16作为图像的encoder。将vertices投影到feature map上使其作为附加特征concat到顶点坐标coordinate上，然后使用Graph CNNs来进行deformation。使用CD loss、edge regularization、normal loss，并将其加权起来。</p>
</li>
</ol>
<h2 id="Topology-Modification-Approach（2019-9，Our-lab）"><a href="#Topology-Modification-Approach（2019-9，Our-lab）" class="headerlink" title="Topology Modification Approach（2019.9，Our lab）"></a>Topology Modification Approach（2019.9，Our lab）</h2><blockquote>
<p>ICCV2019《Deep Mesh Reconstruction From Single RGB Images via Topology Modification Networks》</p>
<p>paper下载： <a href="http://openaccess.thecvf.com/content_ICCV_2019/html/Pan_Deep_Mesh_Reconstruction_From_Single_RGB_Images_via_Topology_Modification_ICCV_2019_paper.html" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_ICCV_2019/html/Pan_Deep_Mesh_Reconstruction_From_Single_RGB_Images_via_Topology_Modification_ICCV_2019_paper.html</a> </p>
</blockquote>
<h3 id="概述-13"><a href="#概述-13" class="headerlink" title="概述"></a>概述</h3><p>单视图mesh重建。最大特点是提出了修改mesh的拓扑结构的方法，通过使用在三角面片中采点、然后输入MLP进行与GT的距离的估计，对误差大的三角面片的使用阈值进行裁剪。但是生成的mesh并不watertight</p>
<p>基本流程：deformation + modification + boundaryRefinement</p>
<p><img src="16_5.png" alt></p>
<p>注：</p>
<ul>
<li>创新点：Topology Modification、Boundary Refinement</li>
<li>是end-to-end的，可以生成复杂拓扑的shape、并且达到了SOTA</li>
<li>因为输出mesh不闭合。也许可以使用后处理步骤，即dense points sampling + normals估计，然后使用Poisson surface reconstruction来获得闭合的mesh</li>
<li>各个subnet之间先分开训练，最后合在一起fine-tune</li>
<li>AtlasNet也可以看作是square的deform获得</li>
</ul>
<h3 id="原理-9"><a href="#原理-9" class="headerlink" title="原理"></a>原理</h3><p><img src="16_1.png" alt></p>
<p>使用ResNet-18提取图像1024维特征$x$。对初始mesh（球）进行deformation再modification，进行两次，最后再Boundary Refinement，最后输出。</p>
<p>deformation使用MLP预测顶点坐标的offset；modification使用MLP预测误差然后阈值剔除；refinement使用boundary regularizer进行正则消除zigzags</p>
<h3 id="实现-16"><a href="#实现-16" class="headerlink" title="实现"></a>实现</h3><ol>
<li><p>Mesh DeformNet</p>
<p>将图像特征$x$和点坐标进行concat输入到MLP，输出坐标的offset，前三层使用ReLU，最后一层使用tanh。</p>
</li>
<li><p>Topology Modification</p>
<p><img src="16_3.png" alt></p>
<p>注意：此操作会使得mesh不闭合（not watertight）</p>
<ul>
<li>Error Estimation：从face上采样一些点，然后与图像特征$x$进行cancat，输入到MLP中然后输出per-point errors（distances to the ground truth），最后取errors的平均值就能得到三角面片的errors了。</li>
<li>Face Pruning：使用固定的阈值进行剔除（高于此阈值则剔除这个面片）。第一个subnet与第二个subnet的剔除阈值不一样（第一个阈值0.1，第二个0.05）</li>
</ul>
</li>
<li><p>Boundary Refinement</p>
<p>不进行refinement的话，剔除后的边缘会出现锯齿状。（左：no refinement，右：refinement）</p>
<p><img src="16_4.png" alt></p>
<p>refinement的目的是使得边缘平滑、消除锯齿。使用MLP，输入boundary vertex输出位移量（使得边缘点可以位置移动）。使用boundary regularizer来penalizes the zigzags（enforcing the boundary curves to stay smooth and consistent）：</p>
<p><img src="16_2.png" alt></p>
</li>
<li><p>Training Objectives</p>
<p>在deformation和refinement中使用CD Loss</p>
<p>在error estimation（Modification）中使用quadratic loss（输出的GT值是corresponding ground truth error）</p>
<p>在deformation中使用normal loss、smoothness loss、edge loss</p>
</li>
</ol>
<h2 id="Mesh-R-CNN（2019-6）"><a href="#Mesh-R-CNN（2019-6）" class="headerlink" title="Mesh R-CNN（2019.6）"></a>Mesh R-CNN（2019.6）</h2><blockquote>
<p>ICCV2019《Mesh R-CNN》</p>
<p>paper下载：<a href="http://openaccess.thecvf.com/content_ICCV_2019/html/Gkioxari_Mesh_R-CNN_ICCV_2019_paper.html" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_ICCV_2019/html/Gkioxari_Mesh_R-CNN_ICCV_2019_paper.html</a> </p>
</blockquote>
<h3 id="概述-14"><a href="#概述-14" class="headerlink" title="概述"></a>概述</h3><p>输入真实现实场景中的单张图片，推断其中出现的物体的3D mesh（相当于2Dsegmentation与3Dreconstruction的结合，即Mask R-CNN与3Dreconstruction（基于deformation）的结合）</p>
<p><img src="17_1.png" alt></p>
<p>流程简述：使用RPN提出proposal，然后根据区域得到coarse voxel representations，再转换成mesh使用GCN进行deformation得到最后的mesh。先用coarse voxel是因为voxel可以近似任意的拓扑结构，避免了deformation不能修改拓扑的缺点。</p>
<p>在ShapeNet数据集上检验“mesh prediction branch”的性能；在Pix3D数据集上检验“full Mesh R-CNN system”的性能。输入：单张图片。输出：2D bounding box、label、2D mask、3D mesh。</p>
<p>注：</p>
<ul>
<li>网络trainable end-to-end</li>
<li>输出拓扑可以任意（因为是从voxel representation转换得到initial mesh）</li>
<li>作者提出了Cubify过程将voxel转化为mesh（但是得到的mesh比较粗糙），同时对比了Marching Cubes（不高效、实现复杂）</li>
</ul>
<h3 id="原理-10"><a href="#原理-10" class="headerlink" title="原理"></a>原理</h3><p><img src="17_2.png" alt></p>
<p>输入的图片通过backbone network（例如ResNet-50-FPN），再通过RPN得到proposals。proposals可以用于bounding box的回归、categories的分类、mask的预测、mesh的VertAlign、voxel branch的输入。</p>
<p>其中mesh predictor由voxel branch、mesh refinement branch组成。即mesh predictor根据proposals区域使用3D卷积得到voxel occupancy probabilities，再转换为mesh，最后根据VertAlign（其实就是将3Dvertex投影到feature map上得到该顶点特征的concat值）得到的特征使用GCN得到顶点的位移量，再在“Refine”步骤中更新坐标。多次使用该过程得到输出。</p>
<h3 id="实现-17"><a href="#实现-17" class="headerlink" title="实现"></a>实现</h3><ol>
<li><p>Voxel Branch</p>
<p>输出voxel occupancy probabilities。又可以看怍是3D版本的mask。</p>
<p>对RoIAlign得到的feature map使用small fully-convolutional network以保持特征间的对应关系。</p>
<p>Voxel Loss：minimize the binary cross-entropy</p>
</li>
<li><p>Cubify: Voxel to Mesh</p>
<p>将得到的voxel转换成triangle mesh。先用阈值分割probabilities得到确定的voxel结构。然后将每一个voxel立方体转换成mesh立方体结构（在立方体的每个面上加上斜对角线）。靠在一起的edges、vertices、faces共享。</p>
<p>由此得到了watertight的mesh，且拓扑结构与voxel相同（亏格可以不是零，相当于做了拓扑上恰当的初始化）。</p>
<p>Marching Cubes通过提取等值面来获取surface，但是更为复杂，不高效。</p>
</li>
<li><p>Mesh Refinement Branch</p>
<ul>
<li><p>Vertex Alignment（VertAlign）：将vertex投影到image plane再用双线性插值。对于第一个stage则作为初始的feature vector，往后的stage则concat到已有的feature vector上</p>
</li>
<li><p>Graph Convolution：使用GCN来变换特征，一个stage级联several graph convolution layers</p>
</li>
<li><p>Vertex Refinement：更新顶点位置$v_i’=v_i+tanh(W_{vert}[f_i;v_i])$，其中$f_i$、$v_i$分别是顶点的特征向量和空间坐标。This updates the mesh geometry, keeping its topology fixed.</p>
</li>
<li><p>Mesh Losses（用differentiable mesh sampling operation得到点集）：</p>
<p>其中的：</p>
<p><img src="17_5.png" alt></p>
<ul>
<li><p>chamfer distance：最小化距离</p>
<p><img src="17_3.png" alt></p>
</li>
<li><p>(absolute) normal distance：最小化法向夹角</p>
<p><img src="17_4.png" alt></p>
</li>
<li><p>edge loss：使得edge不能太长</p>
</li>
<li><p>Laplacian loss：使得光滑</p>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="Occupancy-Networks（2018-12）👍"><a href="#Occupancy-Networks（2018-12）👍" class="headerlink" title="Occupancy Networks（2018.12）👍"></a>Occupancy Networks（2018.12）👍</h2><blockquote>
<p>CVPR2019《Occupancy Networks: Learning 3D Reconstruction in Function Space》</p>
<p>paper下载： <a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Mescheder_Occupancy_Networks_Learning_3D_Reconstruction_in_Function_Space_CVPR_2019_paper.html" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_CVPR_2019/html/Mescheder_Occupancy_Networks_Learning_3D_Reconstruction_in_Function_Space_CVPR_2019_paper.html</a> </p>
</blockquote>
<h3 id="概述-15"><a href="#概述-15" class="headerlink" title="概述"></a>概述</h3><p>将物体实体表示为函数二分类问题（物体内部被占据的概率），提出Multiresolution IsoSurface Extraction（MISE）算法用于提取等值面（虽然最后也使用了MarchingCubes），使用Fast-Quadric-Mesh-Simplification算法简化得到的mesh，使用一、二阶信息来进行mesh的refine。其中提出的MISE比传统的MarchingCubes更加高效和内存节省，提取的mesh更加精细。取得了SOTA的效果。</p>
<p>注：</p>
<ul>
<li>函数表达的使用，使得可以表达任意的拓扑结构、并且分辨率高、又不消耗过多的内存（与voxel相比）</li>
<li>可以支持多种类型的3D数据输入（其实只要将3D数据转变为shape vector输入即可）</li>
<li>优点：high resolution closed surfaces without self intersections and does not require template meshes</li>
<li>还可以用于学习得到probabilistic latent variable models（未详述）</li>
<li>但是MISE算法需要选取超参数，即initial resolution，作者选取了$32^3$</li>
<li>也可以用于提取法向（朝着$\frac{\partial Probability}{\partial Location}$方向移动使得Probability增加，即法向）</li>
<li>In total, our inference algorithm requires 3s per mesh</li>
<li>可以使用KD-Tree来快速计算Chamfer-L1 distance</li>
<li>可以将大量的shape编码入MLPs中，而MLPs的参数量不多</li>
<li>ONet == Occupancy Network</li>
</ul>
<h3 id="原理-11"><a href="#原理-11" class="headerlink" title="原理"></a>原理</h3><p>使用MLP，输入$(x=形状特征,p=点坐标)$，输出$o=占据概率\in[0,1]$</p>
<p>使用由粗到细的多分辨率的MarchingCubes获取等值面（思想与Octree Generating Networks类似），然后简化mesh，最后refine mesh。</p>
<h3 id="实现-18"><a href="#实现-18" class="headerlink" title="实现"></a>实现</h3><ol>
<li><p>Occupancy Networks</p>
<ul>
<li><p>定义</p>
<p>已知对物体的observation $x\in\mathcal{X}$（例如图片的特征向量），以及空间中的点$p\in\mathbb{R}^3$</p>
<p>那么构造一个MLP $f_\theta$映射，输入是pair $(p,x)$，输出是点$p$被占据的概率值，即：</p>
<script type="math/tex; mode=display">
f_\theta:\mathbb{R}\times\mathcal{X}\rightarrow[0,1]</script><p><img src="18_12.png" alt></p>
</li>
<li><p>训练</p>
<p>一个batch有$|\mathcal{B}|$个训练物体，一个物体采样$K$个点。那么训练时所用的损失函数为：</p>
<p><img src="18_1.png" alt></p>
<p>求出$\frac{\partial \mathcal{L_B}}{\partial x}$和$\frac{\partial \mathcal{L_B}}{\partial \theta}$即可梯度回传更新参数。</p>
</li>
<li><p>采样</p>
<p>作者发现在bounding volume of the ground truth mesh中均匀采样points效果最佳。</p>
</li>
<li><p>细节</p>
<p>实际上MLP由5 ResNet blocks组成。并且使用conditional batch normalization</p>
<ul>
<li>对于单视图输入（Single Image 3D Reconstruction），使用ResNet18编码</li>
<li>对于点云输入（Point Cloud Completion），使用PointNet编码</li>
<li>对于体素输入（Voxel Super-Resolution），使用3D CNN编码</li>
</ul>
</li>
</ul>
</li>
<li><p>Inference</p>
<p><img src="18_11.png" alt></p>
<ul>
<li><p>Multiresolution IsoSurface Extraction（MISE）</p>
<p>基本原理：incrementally building an octree</p>
<p>效果：extract high resolution meshes without densely evaluating all points</p>
<p><img src="18_2.png" alt></p>
<p>既有occupied和unoccupied的grid被认定为active（light red部分），即需要细分。不断重复此过程。最后使用MarchingCubes获得等值面。initial resolution = $32^3$ = 32768 points</p>
</li>
<li><p>Fast-Quadric-Mesh-Simplification algorithm</p>
<p>使用这个算法进行简化得到的mesh（合并过多的edges，删除过多的triangles）</p>
<p>此算法不是作者提出的</p>
</li>
<li><p>Refine the output mesh</p>
<p>效果：removes the discretization artifacts of the Marching Cubes</p>
<p>使用一、二阶信息来refinement。从输出的mesh的表面上采点$p_k$，然后最小化：</p>
<p><img src="18_3.png" alt></p>
<p>其中$n(p_k)$表示$p_k$处的法向。$\lambda=0.01$。第二项使用了梯度信息，可以使用Double-Backpropagation算法快速实现。</p>
</li>
</ul>
</li>
</ol>
<h3 id="实验-6"><a href="#实验-6" class="headerlink" title="实验"></a>实验</h3><p>使用的指标：volumetric IoU、Chamfer-L1 distance、normal consistency score</p>
<ol>
<li><p>无shape vector输入时，检验representation power</p>
<p>This gives us an upper bound on the results we can achieve when conditioning our representation on additional input</p>
<p>做法：将物体编码到512维的向量。然后训练Occupancy Networks从这512维向量中恢复mesh。</p>
<p><img src="18_4.png" alt></p>
</li>
<li><p>有shape vector输入时（单视图、点云、体素），重建mesh</p>
<p>即单视图重建mesh、少量点云重建出高分辨率的mesh、低分辨率体素重建出高分辨率的mesh</p>
<ul>
<li><p>单视图重建：使用3D-R2N2、Point Set Generating Networks（PSGN）、Pixel2Mesh、AtlasNet作为比对对象</p>
<p>在ShapeNet数据集上的效果：</p>
<p><img src="18_5.png" alt></p>
<p>在KITTI和OnlineProducts数据集上的效果（真实数据集）：</p>
<p><img src="18_6.png" alt></p>
</li>
<li><p>噪声点云输入：将3D-R2N2、PSGN改为点云输入，以及Deep Marching Cubes（DMC）作为比对对象</p>
<p><img src="18_7.png" alt></p>
</li>
<li><p>体素输入：将相应的输入改变为体素输入即可</p>
<p><img src="18_8.png" alt></p>
</li>
</ul>
</li>
<li><p>使用probabilistic latent variable models生成模型（无监督）</p>
<p>We examine the generative capabilities of occupancy networks by adding an encoder to our model and generating unconditional samples from this model</p>
<p><img src="18_9.png" alt></p>
<p><img src="18_10.png" alt></p>
</li>
<li><p>Ablation Study</p>
<ul>
<li><p>Effect of sampling strategy（第一种uniform sampling效果最好）：</p>
<ul>
<li><p>sampling 2048 points uniformly in the bounding volume of the ground truth mesh（uniform sampling）</p>
</li>
<li><p>sampling 1024 points inside and 1024 points outside mesh（equal sampling）</p>
</li>
<li><p>sampling 1024 points uniformly and 1024 points on the surface of the mesh plus some Gaussian noise with standard deviation 0.1（surface sampling）</p>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<ul>
<li>Effect of architecture（探究了两种结构的影响）：<ul>
<li>remove the conditional batch normalization and replace it with a linear layer</li>
<li>remove all ResNet blocks in the decoder and replace them with linear blocks</li>
</ul>
</li>
</ul>
<h2 id="NOCS-maps（2019-7）"><a href="#NOCS-maps（2019-7）" class="headerlink" title="NOCS maps（2019.7）"></a>NOCS maps（2019.7）</h2><blockquote>
<p>NIPS2019《Multiview Aggregation for Learning Category-Specific Shape Reconstruction》</p>
<p>paper下载：<a href="https://arxiv.org/abs/1907.01085" target="_blank" rel="noopener">https://arxiv.org/abs/1907.01085</a> </p>
</blockquote>
<h3 id="概述-16"><a href="#概述-16" class="headerlink" title="概述"></a>概述</h3><p>输入多张RGB图，输出surface上的点云，甚至还能预测camera pose。提出使用normalized object coordinate space maps（NOCS maps）来进行multiview的处理。其本质类似于深度图。NOCS maps内的每个点都对应到NOCS中的一个3D点，NOCS就是unit cube的3D点云空间。multiview的信息利用通过3D点的union来实现，通过permutation equivariant layers实现在feature层级的信息结合。</p>
<p>注：</p>
<ul>
<li>NOCS shape level：点集取并集；feature level：permutation equivariant layers</li>
<li>与depth map的区别：<ul>
<li>depth map中的每个点是深度值。需要（x，y）坐标、camera pose、depth value才能推断得到3D空间中的点</li>
<li>NOCS maps中的每个点就是3D空间（此处是NOCS空间）中的空间坐标点。据此甚至还能推断出camera pose</li>
</ul>
</li>
</ul>
<h3 id="原理-12"><a href="#原理-12" class="headerlink" title="原理"></a>原理</h3><p><img src="19_1.png" alt></p>
<p>输入图（a），利用CNN输出预测（c）、（d）以及相对应的mask。其中（c）、（d）图中的每个像素对应了NOCS中的一个3D点。（c）是从这个视角看的前视NOCS maps，（d）是从背后看的NOCS maps，两者合称NOX maps。这些3D点的并集就是surface上的points了。</p>
<p><img src="19_2.png" alt></p>
<p>NOCS是单位立方体空间。将NOCS的物体投影得到NOCS maps。</p>
<h3 id="实现-19"><a href="#实现-19" class="headerlink" title="实现"></a>实现</h3><p><img src="19_3.png" alt></p>
<p>利用类似于SegNet的结构来输出图片。输入3通道RGB图，输出8通道图像（3+3+1+1=front NOCS map+back NOCS maps+front mask+back mask）、</p>
<p>那么输出的点云就是所有3D坐标的并集。</p>
<p>推断的同时推断mask可以减少计算loss时的计算量。总的loss函数是mask的binary cross entropy loss与NOCS pixels的L2范数损失的和。</p>
<p>使用permutation equivariant layers（图中橙色部分）来在特征层级融合信息。相对于将特征减去均值（即图中右上角的pooling为avg pooling）再进行非线性操作（即图中右下角嵌套的函数$f(\cdot)$）。</p>
<p>网络的输出还可以包括texture of the occluded surface of the object，那么就可以预测物体的纹理了。</p>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p><img src="19_4.png" alt></p>
<p><img src="19_5.png" alt></p>
<h2 id="MVSNet（2018-4）👍"><a href="#MVSNet（2018-4）👍" class="headerlink" title="MVSNet（2018.4）👍"></a>MVSNet（2018.4）👍</h2><blockquote>
<p>ECCV2018《MVSNet: Depth Inference for Unstructured Multi-view Stereo》</p>
<p>paper下载： <a href="http://openaccess.thecvf.com/content_ECCV_2018/html/Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper.html" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_ECCV_2018/html/Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper.html</a> </p>
</blockquote>
<h3 id="概述-17"><a href="#概述-17" class="headerlink" title="概述"></a>概述</h3><p>端到端的多视图的depth map推断。先用CNN提取特征，然后构造cost volume，然后用3D CNN来regularize and regress得到初始depth map，然后refine得到最终输出。超越了SOTA，而且更快，泛化能力强。</p>
<p>注：</p>
<ul>
<li><p>输入one reference image，以及several source images，那么MVSNet infers the depth map for the reference image</p>
</li>
<li><p>使用differentiable homography warping operation来将相机的几何关系编码进入MVSNet中，从2D features中得到3D cost volume，使得end-to-end训练成为可能。</p>
</li>
<li><p>为了可以接受任意数量的视图输入，使用variance-based metric，来将多个features变成one cost feature in the volume。然后此cost volume经过3DCNN便得到initial depth map。然后再次利用reference image来refine depth map（改善边界区域的准确率）。</p>
</li>
<li><p>推断的depth map是per-view depth map estimation</p>
</li>
</ul>
<p><img src="20_6.png" alt></p>
<h3 id="原理-13"><a href="#原理-13" class="headerlink" title="原理"></a>原理</h3><p><img src="20_1.png" alt></p>
<p>利用CNN提取feature maps，使用homography将feature maps变为feature volumes，再通过计算Cost Metric将多个feature volumes转变为只有一个的cost volume（使得可以多视图输入），再</p>
<h3 id="实现-20"><a href="#实现-20" class="headerlink" title="实现"></a>实现</h3><ol>
<li><p>Image Features</p>
<p>设有$N$张图像$\{I_i\}_{i=1}^N$，丢到CNN里得到了$N$个特征图$\{F_i\}_{i=1}^N$。每个特征图都是32通道（记F=32）的，尺寸是$\frac{W}{4}\times\frac{H}{4}$（丢到CNN里尺寸缩小了4倍）</p>
</li>
<li><p>Cost Volume</p>
<p>将reference image记作$I_1$，将source images记作$\{I_i\}_{i=2}^N$</p>
<ul>
<li><p>Differentiable Homography</p>
<p>将source images feature map利用Homography matrix转变为reference image的视角（又称wrap），根据reference camera的不同深度值（D个）由此形成了D个fronto-parallel planes。由于一共有N个images，所以有N个features volume$\{V_i\}_{i=1}^N$，其尺寸是$\frac{W}{4}\times\frac{H}{4}\times D\times F$（其中F是feature map的通道数32）。即$\frac{W}{4}\times\frac{H}{4}\times D$内的一个元素是一个F维的vector。利用了双线性插值。</p>
<p>通过这步就将相机的几何关系嵌入到volume中去了。</p>
</li>
<li><p>Cost Metric</p>
<p>这步将multiple feature volumes$\{V_i\}_{i=1}^N$转变为只有一个的cost volume$C$，本质是计算方差</p>
<p><img src="20_2.png" alt></p>
<p>our variance-based cost metric explicitly measures the multi-view feature difference</p>
</li>
<li><p>Cost Volume Regularization</p>
<p>作者说可能会由于non-Lambertian surfaces、occlusions造成得到的cost volume被noise-contaminated，而且需要加上smoothness constraints以推断depth map。</p>
<p>使用multi-scale 3D CNN（就像是3D版本的UNet）来做正则。3DCNN的最后一层输出1通道的volume，然后沿着depth direction做softmax operation得到probability volume。此probability volume可以用于per-pixel depth estimation或measure the estimation condence。注意：probability volume尺寸是$\frac{W}{4}\times\frac{H}{4}\times D \times 1$</p>
</li>
</ul>
</li>
<li><p>Depth Map</p>
<ul>
<li><p>Initial Estimation</p>
<p>沿着depth direction计算probability volume在每个像素上的深度的期望值，作为初始的深度估计。</p>
<p><img src="20_3.png" alt></p>
<p>那么输出depth map的尺寸就是$\frac{W}{4}\times\frac{H}{4} \times 1$</p>
</li>
<li><p>Probability Map</p>
<p><img src="20_4.png" alt></p>
<p>For those falsely matched pixels, their probability distributions are scattered and cannot be concentrated to one peak，因此可以作为quality of a depth estimation的衡量。</p>
</li>
<li><p>Depth Map Refinement</p>
<p>depth map的边界可能会平滑掉，因此使用reference image帮助获得边缘。</p>
<p>将initial depth map（缩放到0~1的数值范围先）和resized reference image进行concat得到4通道的输入，输入到2DCNN中，输出depth residual。与原initial depth map相加就得到refined depth map</p>
</li>
</ul>
</li>
<li><p>Loss</p>
<p><img src="20_5.png" alt></p>
<p>分别计算initial depth estimation、refined depth estimation与GT depth map的L1损失，求和。其中$d(p)$是GT，$\hat d_i(p)$是initial depth estimation，$\hat d_r(p)$是refined depth estimation。$p_{valid}$指只计算GT有有效数据的部分。</p>
</li>
</ol>

            </div>
            <hr/>

            
            <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.88rem;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-large waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fa fa-close"></i></a>
            <h4 class="reward-title">给小编加个鸡腿🍗呗</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>
            

            <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">

<div id="article-share">
    
    <div class="social-share" data-disabled="qzone" data-wechat-qrcode-helper="<p>微信里点“发现”->“扫一扫”二维码便可查看分享。</p>"></div>
    
</div>

<script src="/libs/share/js/social-share.min.js"></script>

            

    <div class="reprint" id="reprint-statement">
        <p class="reprint-tip">
            <i class="fa fa-exclamation-triangle"></i>&nbsp;&nbsp;
            <span>转载规则</span>
        </p>
        
            <div class="center-align">
                <a rel="license" href="https://creativecommons.org/licenses/by/4.0/deed.zh">
                    <img alt=""
                         style="border-width:0"
                         src="https://i.creativecommons.org/l/by/4.0/88x31.png"/>
                </a>
            </div>
            <br/>
            <span xmlns:dct="http://purl.org/dc/terms/" href="http://purl.org/dc/dcmitype/Text"
                  property="dct:title" rel="dct:type">
                    《3D结构重建》
                </span> 由
            <a xmlns:cc="http://creativecommons.org/ns#" href="/3d/3dreconstruction/" property="cc:attributionName"
               rel="cc:attributionURL">
                Karbo
            </a> 采用
            <a rel="license" href="https://creativecommons.org/licenses/by/4.0/deed.zh">
                知识共享署名 4.0 国际许可协议
            </a>进行许可。
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>


        </div>
    </div>

    

    

    

    

    
        <style>
    .valine-card {
        margin: 1.5rem auto;
    }

    .valine-card .card-content {
        padding: 20px 20px 5px 20px;
    }

    #vcomments input[type=text],
    #vcomments input[type=email],
    #vcomments input[type=url],
    #vcomments textarea {
        box-sizing: border-box;
    }

    #vcomments p {
        margin: 2px 2px 10px;
        font-size: 1.05rem;
        line-height: 1.78rem;
    }

    #vcomments blockquote p {
        text-indent: 0.2rem;
    }

    #vcomments a {
        padding: 0 2px;
        color: #42b983;
        font-weight: 500;
        text-decoration: underline;
    }

    #vcomments img {
        max-width: 100%;
        height: auto;
        cursor: pointer;
    }

    #vcomments ol li {
        list-style-type: decimal;
    }

    #vcomments ol,
    ul {
        display: block;
        padding-left: 2em;
        word-spacing: 0.05rem;
    }

    #vcomments ul li,
    ol li {
        display: list-item;
        line-height: 1.8rem;
        font-size: 1rem;
    }

    #vcomments ul li {
        list-style-type: disc;
    }

    #vcomments ul ul li {
        list-style-type: circle;
    }

    #vcomments table, th, td {
        padding: 12px 13px;
        border: 1px solid #dfe2e5;
    }

    #vcomments table, th, td {
        border: 0;
    }

    table tr:nth-child(2n), thead {
        background-color: #fafafa;
    }

    #vcomments table th {
        background-color: #f2f2f2;
        min-width: 80px;
    }

    #vcomments table td {
        min-width: 80px;
    }

    #vcomments h1 {
        font-size: 1.85rem;
        font-weight: bold;
        line-height: 2.2rem;
    }

    #vcomments h2 {
        font-size: 1.65rem;
        font-weight: bold;
        line-height: 1.9rem;
    }

    #vcomments h3 {
        font-size: 1.45rem;
        font-weight: bold;
        line-height: 1.7rem;
    }

    #vcomments h4 {
        font-size: 1.25rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    #vcomments h5 {
        font-size: 1.1rem;
        font-weight: bold;
        line-height: 1.4rem;
    }

    #vcomments h6 {
        font-size: 1rem;
        line-height: 1.3rem;
    }

    #vcomments p {
        font-size: 1rem;
        line-height: 1.5rem;
    }

    #vcomments hr {
        margin: 12px 0;
        border: 0;
        border-top: 1px solid #ccc;
    }

    #vcomments blockquote {
        margin: 15px 0;
        border-left: 5px solid #42b983;
        padding: 1rem 0.8rem 0.3rem 0.8rem;
        color: #666;
        background-color: rgba(66, 185, 131, .1);
    }

    #vcomments pre {
        font-family: monospace, monospace;
        padding: 1.2em;
        margin: .5em 0;
        background: #272822;
        overflow: auto;
        border-radius: 0.3em;
        tab-size: 4;
    }

    #vcomments code {
        font-family: monospace, monospace;
        padding: 1px 3px;
        font-size: 0.92rem;
        color: #e96900;
        background-color: #f8f8f8;
        border-radius: 2px;
    }

    #vcomments pre code {
        font-family: monospace, monospace;
        padding: 0;
        color: #e8eaf6;
        background-color: #272822;
    }

    #vcomments pre[class*="language-"] {
        padding: 1.2em;
        margin: .5em 0;
    }

    #vcomments code[class*="language-"],
    pre[class*="language-"] {
        color: #e8eaf6;
    }

    #vcomments [type="checkbox"]:not(:checked), [type="checkbox"]:checked {
        position: inherit;
        margin-left: -1.3rem;
        margin-right: 0.4rem;
        margin-top: -1px;
        vertical-align: middle;
        left: unset;
        visibility: visible;
    }

    #vcomments b,
    strong {
        font-weight: bold;
    }

    #vcomments dfn {
        font-style: italic;
    }

    #vcomments small {
        font-size: 85%;
    }

    #vcomments cite {
        font-style: normal;
    }

    #vcomments mark {
        background-color: #fcf8e3;
        padding: .2em;
    }

    #vcomments table, th, td {
        padding: 12px 13px;
        border: 1px solid #dfe2e5;
    }

    table tr:nth-child(2n), thead {
        background-color: #fafafa;
    }

    #vcomments table th {
        background-color: #f2f2f2;
        min-width: 80px;
    }

    #vcomments table td {
        min-width: 80px;
    }

    #vcomments [type="checkbox"]:not(:checked), [type="checkbox"]:checked {
        position: inherit;
        margin-left: -1.3rem;
        margin-right: 0.4rem;
        margin-top: -1px;
        vertical-align: middle;
        left: unset;
        visibility: visible;
    }
</style>

<div class="card valine-card" data-aos="fade-up">
    <div id="vcomments" class="card-content"></div>
</div>

<script src="/libs/valine/av-min.js"></script>
<script src="/libs/valine/Valine.min.js"></script>
<script>
    new Valine({
        el: '#vcomments',
        appId: 'WwrKLalwiFrnKlAcLCjpeyK7-gzGzoHsz',
        appKey: 'zF7jn8hejoyxVPESEeCAxCzY',
        notify: 'false' === 'true',
        verify: 'false' === 'true',
        visitor: 'true' === 'true',
        avatar: 'mm',
        pageSize: '10',
        lang: 'zh-cn',
        placeholder: 'ヾﾉ≧∀≦)o来评论啊，快活啊!'
    });
</script>
    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6 overflow-policy" data-aos="fade-up" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fa fa-dot-circle-o"></i>&nbsp;本篇
            </div>
            <div class="card">
                <a href="/3d/3dreconstruction/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/17.jpg" class="responsive-img" alt="3D结构重建">
                        
                        <span class="card-title">3D结构重建</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            本文是3D重建的学习笔记
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="fa fa-clock-o fa-fw icon-date"></i>2019-10-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fa fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/3d/" class="post-category" target="_blank">
                                    3d
                                </a>
                            
                            
                        </span>
                    </div>
                </div>

                
                <div class="card-action article-tags">
                    
                    <a href="/tags/3d/" target="_blank">
                        <span class="chip bg-color">3d</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6 overflow-policy" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fa fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/english/englishgrammar/">
                    <div class="card-image">
                        
                        <img src="/english/englishgrammar/1.jpg" class="responsive-img" alt="英语语法基础">
                        
                        <span class="card-title">英语语法基础</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            简单介绍了英语的基本语法规则
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="fa fa-clock-o fa-fw icon-date"></i>2019-10-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fa fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/english/" class="post-category" target="_blank">
                                    english
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/english/" target="_blank">
                        <span class="chip bg-color">english</span>
                    </a>
                    
                    <a href="/tags/grammar/" target="_blank">
                        <span class="chip bg-color">grammar</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>
</div>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title"><i class="fa fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fa fa-list"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            // headingsOffset: -205,
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).slideUp(500);
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).slideDown(500);
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>


<script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$'], ['\(', '\)']]}
    });
</script>

<footer class="page-footer bg-color">
    <div class="container row center-align">
        <div class="col s12 m8 l8 copy-right">
            本站由
            <a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">hexo-theme-matery</a>主题搭建.

            
            &nbsp;<i class="fa fa-area-chart"></i>&nbsp;站点总字数:&nbsp;
            <span class="white-color">95.2k</span>
            

            
            
            <br>
            
            <span id="busuanzi_container_site_pv">
                <i class="fa fa-heart-o"></i>
                本站总访问量 <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
            <span id="busuanzi_container_site_uv">
                <i class="fa fa-users"></i>
                次,&nbsp;访客数 <span id="busuanzi_value_site_uv" class="white-color"></span> 人.
            </span>
            
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
<a href="https://github.com/Karbo123" class="tooltipped" target="_blank" data-tooltip="我的GitHub" data-position="top"
    data-delay="50">
    <i class="fa fa-github"></i>
</a>



<a href="https://www.zhihu.com/people/karbo-50/activities" class="tooltipped" target="_blank" data-tooltip="我的知乎" data-position="top"
    data-delay="50">
    <i class="fa fa-user"></i>
</a>



<a href="mailto:lei@karbo.online" class="tooltipped" target="_blank" data-tooltip="邮件联系"
    data-position="top" data-delay="50">
    <i class="fa fa-envelope-open"></i>
</a>



<a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top"
    data-delay="50">
    <i class="fa fa-rss"></i>
</a>
</div>
    </div>
</footer>

<div class="progress-bar"></div>

<!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fa fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="/js/search.js"></script>
<script type="text/javascript">
$(function () {
    searchFunc("/" + "search.xml", 'searchInput', 'searchResult');
});
</script>
<!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fa fa-angle-up"></i>
    </a>
</div>


<script src="/libs/materialize/materialize.min.js"></script>
<script src="/libs/masonry/masonry.pkgd.min.js"></script>
<script src="/libs/aos/aos.js"></script>
<script src="/libs/scrollprogress/scrollProgress.min.js"></script>
<script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
<script src="/js/matery.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->



    <script src="/libs/others/clicklove.js"></script>


    <script async src="/libs/others/busuanzi.pure.mini.js"></script>


</body>
</html>